"""
CORTEX 2.0 - Ambient Context Capture Daemon

Purpose: Automatically capture workspace context without user intervention.
Target: Achieve 60% → 85% "continue" command success rate.

Author: Asif Hussain
Copyright: © 2024-2025 Asif Hussain. All rights reserved.
License: Proprietary - See LICENSE file for terms

Components:
- File System Watcher: Monitor workspace file changes
- VS Code Monitor: Capture open files and editor state
- Terminal Monitor: Track meaningful commands
- Git Monitor: Capture git operations
- Debouncer: Batch events to prevent excessive captures
"""

import os
import sys
import time
import json
import signal
import threading
import subprocess
import logging
import re
from pathlib import Path
from datetime import datetime
from logging.handlers import RotatingFileHandler
from typing import Dict, Any, List, Callable, Optional, Set

# Add src to path for imports
CORTEX_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(CORTEX_ROOT))

try:
    from watchdog.observers import Observer
    from watchdog.events import PatternMatchingEventHandler
except ImportError:
    print("[CORTEX] ERROR: watchdog library not installed")
    print("[CORTEX] Install with: pip install watchdog")
    sys.exit(1)


# ============================================================================
# Configuration
# ============================================================================

IGNORE_PATTERNS = [
    "**/__pycache__/**",
    "**/.venv/**",
    "**/.git/**",
    "**/node_modules/**",
    "**/*.pyc",
    "**/.DS_Store",
    "**/bin/**",
    "**/obj/**"
]

WATCH_PATTERNS = [
    "**/*.py",
    "**/*.md",
    "**/*.json",
    "**/*.yaml",
    "**/*.yml",
    "**/*.tsx",
    "**/*.ts",
    "**/*.cs",
    "**/*.sql",
    "**/*.sh",
    "**/*.ps1"
]

MEANINGFUL_COMMANDS = [
    "pytest", "npm test", "dotnet test",
    "npm run build", "dotnet build",
    "git commit", "git push", "git pull", "git merge",
    "python", "node", "dotnet run"
]

# Security limits
MAX_FILE_SIZE = 1024 * 1024  # 1MB limit for JSON files
MAX_COMMAND_LENGTH = 1000  # Limit command length
MAX_HISTORY_SIZE = 10 * 1024 * 1024  # 10MB limit for history files

# Dangerous command patterns to block
DANGEROUS_PATTERNS = [
    'rm -rf /',
    'rm -rf *',
    'sudo rm',
    'dd if=',
    'mkfs.',
    '>()',  # Fork bomb
    ':|:',  # Fork bomb pattern
    'wget | sh',
    'wget | bash',
    'curl | sh',
    'curl | bash',
    'eval',
    '__import__',
    'os.system',
    'subprocess.call'
]

# Noise file patterns for smart filtering
NOISE_FILE_PATTERNS = [
    # Build artifacts
    '__pycache__', '.pyc', 'bin/', 'obj/', 'build/', 'dist/',
    'node_modules/', 'package-lock.json', '.next/', '.nuxt/',
    # Temporary files
    '.tmp', '.temp', '.swp', '~', '.DS_Store', 'Thumbs.db',
    # Auto-generated
    'generated/', '.generated.',
    # Version control
    '.git/', '.svn/', '.hg/', '.git/objects/', '.git/refs/',
    # IDE files
    '.vscode/', '.idea/', '.suo', '.user',
    # Lock files
    'yarn.lock', 'Pipfile.lock', 'poetry.lock',
    # Coverage and reports
    'coverage/', '.coverage', 'htmlcov/', '.pytest_cache/'
]

# Generated file markers
GENERATED_MARKERS = [
    '@generated',
    'auto-generated',
    'autogenerated',
    'do not edit',
    'generated by',
    'code generated'
]


# ============================================================================
# Secure Logging Setup
# ============================================================================

def setup_secure_logging():
    """Set up secure logging that doesn't expose sensitive info."""
    log_dir = CORTEX_ROOT / "logs"
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / "ambient_capture.log"
    
    # Rotating file handler (10MB max, 5 backups)
    handler = RotatingFileHandler(
        log_file,
        maxBytes=10*1024*1024,
        backupCount=5
    )
    
    # Format without exposing sensitive paths
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    handler.setFormatter(formatter)
    
    logger = logging.getLogger('cortex.ambient')
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
    
    return logger

# Initialize secure logger
logger = setup_secure_logging()


# ============================================================================
# Smart File Filter - Intelligent noise reduction
# ============================================================================

class SmartFileFilter:
    """Intelligently filter out noise files from ambient capture."""
    
    def __init__(self, max_file_size_mb: float = 1.0):
        """
        Initialize smart filter.
        
        Args:
            max_file_size_mb: Maximum file size to consider (MB)
        """
        self.max_file_size = int(max_file_size_mb * 1024 * 1024)
        self._cache = {}  # Cache for generated marker checks
        
    def is_noise_file(self, file_path: Path) -> bool:
        """
        Determine if file is noise using smart heuristics.
        
        Args:
            file_path: Path to file to check
            
        Returns:
            True if file is noise and should be filtered
        """
        try:
            # Check if file exists (may have been deleted)
            if not file_path.exists():
                return True
            
            # Check filename directly (case-sensitive)
            filename = file_path.name
            if filename in {'.DS_Store', 'Thumbs.db', '.gitignore'}:
                return True
                
            # 1. Check path patterns (case-insensitive)
            path_str = str(file_path).lower()
            for pattern in NOISE_FILE_PATTERNS:
                if pattern in path_str:
                    return True
            
            # 2. Check file size (skip large files)
            try:
                file_size = file_path.stat().st_size
                if file_size > self.max_file_size:
                    logger.debug(f"Filtered large file: {file_path.name} ({file_size} bytes)")
                    return True
                    
                # Skip empty files
                if file_size == 0:
                    return True
            except (OSError, FileNotFoundError):
                return True
            
            # 3. Check for auto-generated marker (cache results)
            if self._has_generated_marker(file_path):
                logger.debug(f"Filtered generated file: {file_path.name}")
                return True
            
            # 4. Check if binary file (non-text)
            if self._is_binary_file(file_path):
                logger.debug(f"Filtered binary file: {file_path.name}")
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"Error checking file noise: {type(e).__name__}")
            # When in doubt, filter it out (conservative)
            return True
    
    def _has_generated_marker(self, file_path: Path) -> bool:
        """Check if file has auto-generated marker in first 1000 bytes."""
        
        # Check cache first
        cache_key = str(file_path)
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        try:
            # Read first 1000 bytes only
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                header = f.read(1000).lower()
                
            # Check for generated markers
            has_marker = any(marker.lower() in header for marker in GENERATED_MARKERS)
            
            # Cache result
            self._cache[cache_key] = has_marker
            
            # Limit cache size
            if len(self._cache) > 1000:
                # Remove oldest half
                items = list(self._cache.items())
                self._cache = dict(items[500:])
            
            return has_marker
            
        except (UnicodeDecodeError, OSError):
            # If can't read as text, assume not generated (will be caught by binary check)
            return False
    
    def _is_binary_file(self, file_path: Path) -> bool:
        """Check if file is binary (non-text)."""
        try:
            # Read first 8192 bytes
            with open(file_path, 'rb') as f:
                chunk = f.read(8192)
            
            # Check for null bytes (common in binaries)
            if b'\x00' in chunk:
                return True
            
            # Try to decode as UTF-8
            try:
                chunk.decode('utf-8')
                return False
            except UnicodeDecodeError:
                return True
                
        except (OSError, FileNotFoundError):
            return True
    
    def get_filter_stats(self) -> Dict[str, int]:
        """Get filtering statistics."""
        return {
            'cache_size': len(self._cache),
            'max_cache_size': 1000
        }


# ============================================================================
# Change Pattern Detector - Classify change types
# ============================================================================

class ChangePatternDetector:
    """Detect and classify change patterns in file modifications."""
    
    # Change pattern types
    PATTERN_REFACTOR = "REFACTOR"
    PATTERN_FEATURE = "FEATURE"
    PATTERN_BUGFIX = "BUGFIX"
    PATTERN_DOCS = "DOCS"
    PATTERN_CONFIG = "CONFIG"
    PATTERN_UNKNOWN = "UNKNOWN"
    
    def __init__(self, workspace_path: str):
        """
        Initialize pattern detector.
        
        Args:
            workspace_path: Root workspace path for git operations
        """
        self.workspace_path = Path(workspace_path).resolve()
        self._diff_cache = {}  # Cache git diffs (5 min TTL)
        self._cache_timestamps = {}
    
    def detect_pattern(self, event: Dict[str, Any]) -> str:
        """
        Classify change pattern for a file event.
        
        Args:
            event: File change event dictionary
            
        Returns:
            Pattern type string (REFACTOR, FEATURE, etc.)
        """
        try:
            file_path = Path(event['file'])
            event_type = event['event']
            
            # Quick classification by file extension
            pattern = self._classify_by_extension(file_path)
            if pattern != self.PATTERN_UNKNOWN:
                return pattern
            
            # Classification by event type
            if event_type == 'created':
                return self.PATTERN_FEATURE
            
            if event_type == 'deleted':
                return self.PATTERN_REFACTOR
            
            # Modified files require deeper analysis
            if event_type == 'modified':
                return self._analyze_modification(file_path)
            
            return self.PATTERN_UNKNOWN
            
        except Exception as e:
            logger.error(f"Error detecting pattern: {type(e).__name__}")
            return self.PATTERN_UNKNOWN
    
    def _classify_by_extension(self, file_path: Path) -> str:
        """Classify by file extension."""
        suffix = file_path.suffix.lower()
        filename = file_path.name.lower()
        
        # Check for dotfiles without extension (like .env)
        if filename.startswith('.') and not suffix:
            if filename in {'.env', '.gitignore', '.dockerignore'}:
                return self.PATTERN_CONFIG
        
        # Documentation files
        if suffix in {'.md', '.rst', '.txt'}:
            return self.PATTERN_DOCS
        
        # Configuration files
        if suffix in {'.json', '.yaml', '.yml', '.toml', '.ini', '.env', '.config'}:
            return self.PATTERN_CONFIG
        
        return self.PATTERN_UNKNOWN
    
    def _analyze_modification(self, file_path: Path) -> str:
        """
        Analyze modified file for pattern using git diff.
        
        Args:
            file_path: Path to modified file (relative)
            
        Returns:
            Detected pattern type
        """
        try:
            # Get git diff
            diff_lines = self._get_git_diff(file_path)
            
            if not diff_lines:
                return self.PATTERN_UNKNOWN
            
            # Count additions and deletions
            additions = sum(1 for line in diff_lines if line.startswith('+') and not line.startswith('+++'))
            deletions = sum(1 for line in diff_lines if line.startswith('-') and not line.startswith('---'))
            
            total_changes = additions + deletions
            
            # Small targeted change → likely bug fix
            if total_changes < 20:
                return self.PATTERN_BUGFIX
            
            # Similar adds/deletes → likely refactor
            if total_changes > 0:
                ratio = min(additions, deletions) / max(additions, deletions)
                if ratio > 0.7:  # 70% similarity
                    return self.PATTERN_REFACTOR
            
            # More additions than deletions → likely new feature
            if additions > deletions * 1.5 and additions > 50:
                return self.PATTERN_FEATURE
            
            # Default for moderate changes
            return self.PATTERN_UNKNOWN
            
        except Exception as e:
            logger.error(f"Error analyzing modification: {type(e).__name__}")
            return self.PATTERN_UNKNOWN
    
    def _get_git_diff(self, file_path: Path) -> List[str]:
        """
        Get git diff for file with caching.
        
        Args:
            file_path: Path to file (relative)
            
        Returns:
            List of diff lines
        """
        import subprocess
        import time
        
        cache_key = str(file_path)
        
        # Check cache (5 min TTL)
        if cache_key in self._diff_cache:
            timestamp = self._cache_timestamps.get(cache_key, 0)
            if time.time() - timestamp < 300:  # 5 minutes
                return self._diff_cache[cache_key]
        
        try:
            # Run git diff
            abs_path = self.workspace_path / file_path
            result = subprocess.run(
                ['git', 'diff', 'HEAD', str(abs_path)],
                cwd=str(self.workspace_path),
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0:
                diff_lines = result.stdout.splitlines()
                
                # Cache result
                self._diff_cache[cache_key] = diff_lines
                self._cache_timestamps[cache_key] = time.time()
                
                # Limit cache size
                if len(self._diff_cache) > 100:
                    # Remove oldest 50
                    oldest = sorted(self._cache_timestamps.items(), key=lambda x: x[1])[:50]
                    for key, _ in oldest:
                        self._diff_cache.pop(key, None)
                        self._cache_timestamps.pop(key, None)
                
                return diff_lines
            
            return []
            
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            logger.debug(f"Git diff failed: {type(e).__name__}")
            return []
    
    def clear_cache(self):
        """Clear diff cache."""
        self._diff_cache.clear()
        self._cache_timestamps.clear()


# ============================================================================
# Activity Scorer - Prioritize changes by importance
# ============================================================================

class ActivityScorer:
    """Score file change activity by importance."""
    
    # File type weights (0-40 points)
    FILE_TYPE_WEIGHTS = {
        '.py': 40, '.ts': 40, '.tsx': 40, '.cs': 40, '.java': 40,
        '.js': 38, '.jsx': 38, '.go': 40, '.rs': 40,
        '.json': 30, '.yaml': 30, '.yml': 30, '.toml': 28,
        '.md': 20, '.rst': 18, '.txt': 15,
        '.sh': 25, '.ps1': 25, '.bat': 20,
        'default': 10
    }
    
    # Pattern weights (0-20 points)
    PATTERN_WEIGHTS = {
        'FEATURE': 20,
        'BUGFIX': 15,
        'REFACTOR': 10,
        'CONFIG': 8,
        'DOCS': 5,
        'UNKNOWN': 5
    }
    
    def __init__(self):
        """Initialize activity scorer."""
        pass
    
    def score_activity(self, event: Dict[str, Any], pattern: str) -> int:
        """
        Calculate activity score (0-100).
        
        Args:
            event: File change event
            pattern: Detected change pattern
            
        Returns:
            Score from 0-100
        """
        try:
            file_path = Path(event['file'])
            
            # 1. File type weight (0-40)
            file_type_score = self._get_file_type_score(file_path)
            
            # 2. Change magnitude estimate (0-30)
            magnitude_score = self._estimate_magnitude(event)
            
            # 3. Pattern weight (0-20)
            pattern_score = self.PATTERN_WEIGHTS.get(pattern, 5)
            
            # 4. File importance (0-10)
            importance_score = self._calculate_importance(file_path)
            
            # Total score (capped at 100)
            total = min(100, file_type_score + magnitude_score + pattern_score + importance_score)
            
            return total
            
        except Exception as e:
            logger.error(f"Error scoring activity: {type(e).__name__}")
            return 50  # Default middle score
    
    def _get_file_type_score(self, file_path: Path) -> int:
        """Get score based on file type."""
        ext = file_path.suffix.lower()
        return self.FILE_TYPE_WEIGHTS.get(ext, self.FILE_TYPE_WEIGHTS['default'])
    
    def _estimate_magnitude(self, event: Dict[str, Any]) -> int:
        """
        Estimate change magnitude from event type.
        
        Returns score from 0-30.
        """
        event_type = event.get('event', 'modified')
        
        if event_type == 'created':
            return 30  # New file = large change
        elif event_type == 'deleted':
            return 25  # Deletion = significant
        elif event_type == 'modified':
            # Can't determine size without diff, use moderate score
            return 20
        else:
            return 10
    
    def _calculate_importance(self, file_path: Path) -> int:
        """
        Calculate file importance based on path.
        
        Returns score from 0-10.
        """
        path_str = str(file_path).lower()
        
        # Core source code
        if any(p in path_str for p in ['/src/', '/lib/', '/core/']):
            return 10
        
        # Tests
        if any(p in path_str for p in ['/tests/', '/test/', '/__tests__/']):
            return 8
        
        # Scripts and tools
        if any(p in path_str for p in ['/scripts/', '/tools/', '/utils/']):
            return 6
        
        # Documentation
        if any(p in path_str for p in ['/docs/', '/documentation/']):
            return 4
        
        # Configuration at root
        if '/' not in path_str or path_str.count('/') == 1:
            return 7
        
        # Other files
        return 3


# ============================================================================
# Auto-Summarizer - Generate natural language summaries
# ============================================================================

class AutoSummarizer:
    """Generate natural language summaries of changes."""
    
    def __init__(self):
        """Initialize summarizer."""
        pass
    
    def summarize_event(self, event: Dict[str, Any], pattern: str, score: int) -> str:
        """
        Create event-level summary.
        
        Args:
            event: File change event
            pattern: Detected pattern
            score: Activity score
            
        Returns:
            Human-readable summary string
        """
        try:
            file_path = event['file']
            event_type = event['event']
            
            # Choose template based on event type and pattern
            if event_type == 'created':
                summary = f"Created {file_path}"
            elif event_type == 'deleted':
                summary = f"Deleted {file_path}"
            elif event_type == 'modified':
                summary = f"Modified {file_path}"
            else:
                summary = f"Changed {file_path}"
            
            # Add pattern context
            if pattern == 'FEATURE':
                summary += " (new feature)"
            elif pattern == 'BUGFIX':
                summary += " (bug fix)"
            elif pattern == 'REFACTOR':
                summary += " (refactor)"
            elif pattern == 'CONFIG':
                summary += " (configuration)"
            elif pattern == 'DOCS':
                summary += " (documentation)"
            
            # Add score indicator for high-priority changes
            if score >= 80:
                summary += " [HIGH PRIORITY]"
            
            return summary
            
        except Exception as e:
            logger.error(f"Error summarizing event: {type(e).__name__}")
            return f"File change: {event.get('file', 'unknown')}"
    
    def summarize_batch(self, events: List[Dict[str, Any]]) -> str:
        """
        Create batch-level summary.
        
        Args:
            events: List of file change events
            
        Returns:
            Batch summary string
        """
        try:
            if not events:
                return "No changes"
            
            # Group by pattern
            by_pattern = {}
            for event in events:
                pattern = event.get('pattern', 'UNKNOWN')
                by_pattern.setdefault(pattern, []).append(event)
            
            # Count files
            file_count = len(events)
            
            # Calculate average score
            total_score = sum(e.get('score', 0) for e in events)
            avg_score = total_score // len(events) if events else 0
            
            # Build summary
            parts = []
            for pattern, pattern_events in by_pattern.items():
                count = len(pattern_events)
                pattern_name = pattern.lower()
                parts.append(f"{count} {pattern_name}")
            
            summary = f"Batch: {', '.join(parts)} changes. "
            summary += f"Files: {file_count}. "
            summary += f"Avg score: {avg_score}/100"
            
            return summary
            
        except Exception as e:
            logger.error(f"Error summarizing batch: {type(e).__name__}")
            return f"Batch: {len(events)} changes"
    
    def summarize_session(self, start_time: datetime, end_time: datetime, 
                          all_events: List[Dict[str, Any]]) -> str:
        """
        Create session-level summary.
        
        Args:
            start_time: Session start time
            end_time: Session end time
            all_events: All events in session
            
        Returns:
            Session summary string
        """
        try:
            duration_hours = (end_time - start_time).total_seconds() / 3600
            
            # Extract high-priority events (score > 70)
            high_priority = [e for e in all_events if e.get('score', 0) > 70]
            
            # Count changes per file
            file_changes = {}
            for event in all_events:
                file_path = event.get('file', '')
                file_changes[file_path] = file_changes.get(file_path, 0) + 1
            
            # Get top 3 files
            top_files = sorted(file_changes.items(), key=lambda x: x[1], reverse=True)[:3]
            
            # Build summary
            summary = f"Work session {start_time.strftime('%Y-%m-%d %H:%M')} - "
            summary += f"{end_time.strftime('%H:%M')} ({duration_hours:.1f}h): "
            summary += f"{len(all_events)} total changes. "
            
            if high_priority:
                summary += f"{len(high_priority)} high-priority. "
            
            if top_files:
                top_file_names = [Path(f[0]).name for f in top_files]
                summary += f"Most active: {', '.join(top_file_names)}."
            
            return summary
            
        except Exception as e:
            logger.error(f"Error summarizing session: {type(e).__name__}")
            return f"Session: {len(all_events)} changes"


# ============================================================================
# Debouncer - Batch events to prevent excessive captures
# ============================================================================

class Debouncer:
    """Debounces context capture events to prevent excessive writes."""
    
    def __init__(self, delay_seconds: int = 5, summarizer: Optional['AutoSummarizer'] = None):
        self.delay = delay_seconds
        self.buffer = []
        self.lock = threading.Lock()
        self.timer = None
        self.summarizer = summarizer or AutoSummarizer()  # NEW Phase 4.4
        
    def add_event(self, context: Dict[str, Any]):
        """Add event to buffer."""
        with self.lock:
            self.buffer.append(context)
            
            # Reset timer
            if self.timer:
                self.timer.cancel()
                
            self.timer = threading.Timer(self.delay, self._flush)
            self.timer.start()
            
    def _flush(self):
        """Flush buffered events to Tier 1 with enhanced summarization (Phase 4.4)."""
        with self.lock:
            if not self.buffer:
                return
                
            # Merge similar events
            merged = self._merge_events(self.buffer)
            
            # NEW Phase 4.4: Generate batch summary
            batch_summary = self.summarizer.summarize_batch(merged)
            
            # Write to Tier 1 with summary
            self._write_to_tier1(merged, batch_summary)
            
            # Clear buffer
            self.buffer.clear()
            
    def _merge_events(self, events: List[Dict]) -> List[Dict]:
        """Merge similar events to reduce duplicates."""
        merged = {}
        
        for event in events:
            key = f"{event['type']}:{event.get('file', '')}"
            
            if key not in merged:
                merged[key] = event
            else:
                # Update timestamp to latest
                merged[key]["timestamp"] = event["timestamp"]
                
        return list(merged.values())
        
    def _write_to_tier1(self, events: List[Dict], batch_summary: str = ""):
        """Write events to Tier 1 with enhanced context (Phase 4.4)."""
        try:
            from src.tier1.working_memory import WorkingMemory
            
            brain_path = os.environ.get("CORTEX_BRAIN_PATH", str(CORTEX_ROOT / "cortex-brain"))
            db_path = Path(brain_path) / "tier1" / "conversations.db"
            
            if not db_path.exists():
                print(f"[CORTEX] WARNING: Tier 1 database not found: {db_path}")
                return
            
            wm = WorkingMemory(str(db_path))
            
            # Get or create ambient session
            session_id = self._get_ambient_session(wm)
            
            # NEW Phase 4.4: Store batch summary first
            if batch_summary:
                wm.store_message(
                    conversation_id=session_id,
                    message={
                        "role": "system",
                        "content": f"[Ambient Batch Summary] {batch_summary}",
                        "timestamp": datetime.now().isoformat()
                    }
                )
            
            # Store individual events with enriched context
            for event in events:
                # Include summary, pattern, and score in content
                content = f"[Ambient Capture] {event.get('summary', event['type'])}"
                content += f" | Pattern: {event.get('pattern', 'UNKNOWN')}"
                content += f" | Score: {event.get('score', 0)}/100"
                
                wm.store_message(
                    conversation_id=session_id,
                    message={
                        "role": "system",
                        "content": content,
                        "timestamp": event["timestamp"],
                        "metadata": {
                            "file": event.get('file'),
                            "pattern": event.get('pattern'),
                            "score": event.get('score')
                        }
                    }
                )
                
            print(f"[CORTEX] Captured {len(events)} enriched events to Tier 1")
            logger.info(f"Batch summary: {batch_summary}")
            
        except Exception as e:
            print(f"[CORTEX] ERROR writing to Tier 1: {e}")
            logger.error(f"Tier 1 write error: {type(e).__name__}: {e}")
            
    def _get_ambient_session(self, wm) -> str:
        """Get or create ambient capture session."""
        # Check for today's ambient session
        from datetime import date
        today = date.today().isoformat()
        
        # Create new ambient session for today
        session_id = wm.start_conversation(
            user_id="ambient_daemon",
            metadata={
                "type": "ambient",
                "date": today,
                "description": "Automatic background context capture"
            }
        )
        
        return session_id


# ============================================================================
# File System Watcher - Monitor workspace file changes
# ============================================================================

class FileSystemWatcher:
    """Monitors workspace file changes in real-time with security hardening."""
    
    # Whitelist of allowed extensions
    ALLOWED_EXTENSIONS = {'.py', '.md', '.json', '.yaml', '.yml', '.tsx', '.ts', '.cs', '.sql', '.sh', '.ps1'}
    
    def __init__(self, workspace_path: str, callback: Callable):
        # SECURITY: Resolve to absolute path and validate
        self.workspace_path = Path(workspace_path).resolve(strict=True)
        
        # SECURITY: Validate workspace is a directory
        if not self.workspace_path.is_dir():
            raise ValueError("Workspace path must be a directory")
            
        self.callback = callback
        self.observer = Observer()
        
        # NEW: Initialize smart filter
        self.smart_filter = SmartFileFilter()
        
    def start(self):
        """Start monitoring file system."""
        handler = PatternMatchingEventHandler(
            patterns=WATCH_PATTERNS,
            ignore_patterns=IGNORE_PATTERNS,
            ignore_directories=True,
            case_sensitive=False
        )
        
        handler.on_modified = self._on_file_changed
        handler.on_created = self._on_file_changed
        handler.on_deleted = self._on_file_changed
        
        self.observer.schedule(handler, str(self.workspace_path), recursive=True)
        self.observer.start()
        
        print(f"[CORTEX] File system watcher started: {self.workspace_path}")
        
    def _on_file_changed(self, event):
        """Handle file change event with security validation."""
        if event.is_directory:
            return
            
        try:
            # SECURITY: Get absolute path and resolve symlinks
            file_path = Path(event.src_path).resolve(strict=False)
            
            # SECURITY: Ensure file is within workspace (prevent path traversal)
            if not self._is_safe_path(file_path):
                logger.warning(f"SECURITY: Blocked access to file outside workspace: {file_path}")
                return
            
            # Get relative path safely
            relative_path = file_path.relative_to(self.workspace_path)
            
            # SECURITY: Validate file extension is in whitelist
            if not self._is_allowed_extension(file_path):
                return
            
            # NEW: Smart filtering - ignore noise files
            if self.smart_filter.is_noise_file(file_path):
                return
                
            # Create sanitized context
            context = {
                "type": "file_change",
                "file": str(relative_path),
                "event": event.event_type,
                "timestamp": datetime.now().isoformat()
            }
            
            self.callback(context)
            
        except Exception as e:
            # Don't expose detailed errors to console
            logger.error(f"Error processing file event: {type(e).__name__}")
            print(f"[CORTEX] Error processing file event (see logs)")
            
    def _is_safe_path(self, file_path: Path) -> bool:
        """Check if path is within workspace (prevent path traversal)."""
        try:
            # Resolve both paths
            file_resolved = file_path.resolve()
            workspace_resolved = self.workspace_path.resolve()
            
            # Check if file is under workspace
            return workspace_resolved in file_resolved.parents or file_resolved == workspace_resolved
            
        except (ValueError, RuntimeError):
            return False
            
    def _is_allowed_extension(self, file_path: Path) -> bool:
        """Check if file extension is in whitelist."""
        ext = file_path.suffix.lower()
        return ext in self.ALLOWED_EXTENSIONS


# ============================================================================
# VS Code Monitor - Capture editor state
# ============================================================================

class VSCodeMonitor:
    """Monitors VS Code editor state with security hardening."""
    
    def __init__(self, workspace_path: str):
        # SECURITY: Resolve to absolute path
        self.workspace_path = Path(workspace_path).resolve(strict=True)
        self.vscode_path = self.workspace_path / ".vscode"
        
    def get_open_files(self) -> List[str]:
        """Get list of currently open files with security validation."""
        workspace_file = self.vscode_path / "workspace.json"
        
        if not workspace_file.exists():
            return []
            
        try:
            # SECURITY: Check file size before reading
            if workspace_file.stat().st_size > MAX_FILE_SIZE:
                logger.warning("SECURITY: Workspace file too large, skipping")
                return []
            
            # Read JSON safely
            with open(workspace_file, 'r', encoding='utf-8') as f:
                workspace_data = json.load(f)
            
            # Extract files safely (limit to 100)
            open_files = []
            for folder in workspace_data.get("folders", [])[:100]:
                if "path" in folder and isinstance(folder["path"], str):
                    sanitized = self._sanitize_path(folder["path"])
                    if sanitized:
                        open_files.append(sanitized)
                        
            return open_files[:100]
            
        except (json.JSONDecodeError, OSError) as e:
            logger.error(f"Error reading workspace JSON: {type(e).__name__}")
            return []
            
    def _sanitize_path(self, path_str: str) -> Optional[str]:
        """Sanitize and validate file path."""
        try:
            # Convert to Path and resolve
            path = Path(path_str).resolve()
            
            # SECURITY: Ensure within workspace
            if self.workspace_path not in path.parents and path != self.workspace_path:
                return None
                
            return str(path.relative_to(self.workspace_path))
            
        except (ValueError, RuntimeError):
            return None
        
    def get_active_file(self) -> Optional[str]:
        """Get currently active file in editor."""
        # Note: This is a simplified implementation
        # Full implementation would require VS Code extension
        return None


# ============================================================================
# Terminal Monitor - Track meaningful commands
# ============================================================================

class TerminalMonitor:
    """Monitors terminal command execution with security hardening."""
    
    def __init__(self, callback: Callable):
        self.callback = callback
        self.monitoring = False
        self._validated_history_path = None
        
    def start(self):
        """Start monitoring terminal with security validation."""
        # SECURITY: Validate history file path on startup
        if not self._validate_history_path():
            logger.warning("SECURITY: Could not validate terminal history path")
            print("[CORTEX] Terminal monitor: history path not accessible")
            return
            
        self.monitoring = True
        thread = threading.Thread(target=self._monitor_history_safe, daemon=True)
        thread.start()
        print("[CORTEX] Terminal monitor started")
        
    def _validate_history_path(self) -> bool:
        """Validate terminal history file path."""
        try:
            # Get history path (platform-specific)
            if sys.platform == "win32":
                history_file = Path.home() / "AppData" / "Roaming" / "Microsoft" / "Windows" / "PowerShell" / "PSReadLine" / "ConsoleHost_history.txt"
            else:
                history_file = Path.home() / ".bash_history"
                
            # SECURITY: Resolve and validate path
            history_file = history_file.resolve(strict=True)
            
            # SECURITY: Ensure it's a file and readable
            if not history_file.is_file():
                return False
                
            # SECURITY: Check file size
            if history_file.stat().st_size > MAX_HISTORY_SIZE:
                logger.warning("SECURITY: History file too large")
                return False
                
            self._validated_history_path = history_file
            return True
            
        except (FileNotFoundError, PermissionError, RuntimeError):
            return False
        
    def _monitor_history_safe(self):
        """Monitor history file with security checks."""
        try:
            with open(self._validated_history_path, 'r', encoding='utf-8', errors='ignore') as f:
                f.seek(0, os.SEEK_END)
                
                while self.monitoring:
                    line = f.readline()
                    if line:
                        self._process_command_safe(line.strip())
                    else:
                        time.sleep(1)
                        
        except Exception as e:
            logger.error(f"Terminal monitoring stopped: {type(e).__name__}")
            print(f"[CORTEX] Terminal monitoring stopped (see logs)")
            
    def _process_command_safe(self, command: str):
        """Process terminal command with security validation."""
        # SECURITY: Validate command length
        if len(command) > MAX_COMMAND_LENGTH:
            return
            
        # SECURITY: Check for malicious patterns
        if self._is_malicious_command(command):
            logger.warning(f"SECURITY: Blocked malicious command pattern")
            return
            
        # Check if meaningful
        is_meaningful = any(
            cmd in command.lower()
            for cmd in MEANINGFUL_COMMANDS
        )
        
        if not is_meaningful:
            return
            
        # SECURITY: Sanitize command before logging
        sanitized_command = self._sanitize_command(command)
        command_type = self._identify_command_type(sanitized_command)
        
        context = {
            "type": "terminal_command",
            "command": sanitized_command,
            "command_type": command_type,
            "timestamp": datetime.now().isoformat()
        }
        
        self.callback(context)
        
    def _is_malicious_command(self, command: str) -> bool:
        """Check for obviously malicious commands."""
        command_lower = command.lower()
        return any(pattern in command_lower for pattern in DANGEROUS_PATTERNS)
        
    def _sanitize_command(self, command: str) -> str:
        """Sanitize command for logging."""
        # Redact passwords after -p, --password flags
        command = re.sub(r'(-p|--password)[\s=]+\S+', r'\1 [REDACTED]', command, flags=re.IGNORECASE)
        command = re.sub(r'(password|passwd|pwd)[\s=]+\S+', r'\1=[REDACTED]', command, flags=re.IGNORECASE)
        
        # Redact GitHub tokens (ghp_, ghs_)
        command = re.sub(r'gh[ps]_[a-zA-Z0-9]{36,}', '[REDACTED]', command)
        
        # Redact tokens and API keys
        command = re.sub(r'(token|api_key|secret)[\s=:]+\S+', r'\1=[REDACTED]', command, flags=re.IGNORECASE)
        
        # Redact credentials in URLs
        command = re.sub(r'(https?://)[^:]+:[^@]+@', r'\1[REDACTED]:[REDACTED]@', command)
        
        return command
        
    def _identify_command_type(self, command: str) -> str:
        """Identify command type."""
        command_lower = command.lower()
        
        if "pytest" in command_lower or "test" in command_lower:
            return "test_execution"
        elif "build" in command_lower:
            return "build"
        elif "git commit" in command_lower:
            return "git_commit"
        elif "git push" in command_lower:
            return "git_push"
        elif "git pull" in command_lower:
            return "git_pull"
        elif "python" in command_lower or "node" in command_lower:
            return "code_execution"
        else:
            return "other"


# ============================================================================
# Git Monitor - Capture git operations
# ============================================================================

class GitMonitor:
    """Monitors git operations with security hardening."""
    
    # Whitelist of allowed hook types
    ALLOWED_HOOKS: Set[str] = {"post-commit", "post-merge", "post-checkout"}
    
    def __init__(self, repo_path: str, callback: Callable):
        # SECURITY: Validate repo_path is absolute and exists
        self.repo_path = Path(repo_path).resolve(strict=True)
        
        # SECURITY: Validate it's a git repository
        self.git_dir = self.repo_path / ".git"
        if not self.git_dir.is_dir():
            logger.info("Not a git repository, git monitoring disabled")
            self.git_dir = None
            
        self.callback = callback
        
    def install_hooks(self):
        """Install git hooks with security validation."""
        if self.git_dir is None:
            print("[CORTEX] Not a git repository, skipping git hooks")
            return
            
        hooks_dir = self.git_dir / "hooks"
        hooks_dir.mkdir(exist_ok=True)
        
        for hook_type in self.ALLOWED_HOOKS:
            self._install_hook_securely(hook_type)
            
        print("[CORTEX] Git hooks installed securely")
        
    def _install_hook_securely(self, hook_type: str):
        """Install specific git hook with security checks."""
        # SECURITY: Validate hook type
        if hook_type not in self.ALLOWED_HOOKS:
            logger.error(f"SECURITY: Invalid hook type rejected: {hook_type}")
            return
            
        hook_file = self.git_dir / "hooks" / hook_type
        
        # SECURITY: Backup existing hook if present
        if hook_file.exists():
            backup = hook_file.with_suffix('.cortex-backup')
            try:
                hook_file.rename(backup)
                print(f"[CORTEX] Backed up existing {hook_type} hook")
            except OSError as e:
                logger.error(f"Failed to backup hook: {type(e).__name__}")
                return
        
        # SECURITY: Use absolute paths (prevent path injection)
        capture_script = (self.repo_path / "scripts" / "cortex" / "capture_git_event.py").resolve()
        
        # Validate capture script exists
        if not capture_script.exists():
            logger.error("Capture script not found, skipping hook install")
            return
        
        # SECURITY: Create hook script with no shell injection possible
        # Using sh instead of bash for better compatibility
        script = f'''#!/bin/sh
# CORTEX Ambient Capture Git Hook
# Generated by CORTEX 2.0 - DO NOT EDIT

PYTHON=python3
CAPTURE_SCRIPT="{capture_script}"
HOOK_TYPE="{hook_type}"

"$PYTHON" "$CAPTURE_SCRIPT" "$HOOK_TYPE"
'''
        
        try:
            hook_file.write_text(script, encoding='utf-8')
            
            # SECURITY: Set safe permissions (owner read/write/execute only)
            if sys.platform != "win32":
                hook_file.chmod(0o700)  # rwx------
                
        except OSError as e:
            logger.error(f"Failed to install hook: {type(e).__name__}")


# ============================================================================
# Main Daemon
# ============================================================================

class AmbientCaptureDaemon:
    """Main ambient capture daemon with enhanced intelligence (Phase 4.4)."""
    
    def __init__(self, workspace_path: str):
        self.workspace_path = Path(workspace_path)
        self.running = False
        
        # NEW Phase 4.4: Enhanced intelligence components
        self.pattern_detector = ChangePatternDetector(workspace_path)
        self.activity_scorer = ActivityScorer()
        self.auto_summarizer = AutoSummarizer()
        
        # Initialize components (pass summarizer to debouncer)
        self.debouncer = Debouncer(delay_seconds=5, summarizer=self.auto_summarizer)
        self.file_watcher = FileSystemWatcher(workspace_path, self._handle_file_change)
        self.vscode_monitor = VSCodeMonitor(workspace_path)
        self.terminal_monitor = TerminalMonitor(self.debouncer.add_event)
        self.git_monitor = GitMonitor(workspace_path, self.debouncer.add_event)
        
        logger.info("Ambient Capture Daemon initialized with Phase 4.4 enhancements")
    
    def _handle_file_change(self, context: Dict[str, Any]):
        """
        Enhanced file change handler with pattern detection, scoring, and summarization.
        
        This is the NEW Phase 4.4 pipeline:
        1. Smart filtering (already done by FileSystemWatcher)
        2. Pattern detection
        3. Activity scoring
        4. Summarization
        5. Enriched context to debouncer
        """
        try:
            # Detect change pattern
            pattern = self.pattern_detector.detect_pattern(context)
            context['pattern'] = pattern
            
            # Score activity
            score = self.activity_scorer.score_activity(context, pattern)
            context['score'] = score
            
            # Generate summary
            summary = self.auto_summarizer.summarize_event(context, pattern, score)
            context['summary'] = summary
            
            # Pass enriched context to debouncer
            self.debouncer.add_event(context)
            
            logger.debug(f"Captured: {summary} (score: {score})")
            
        except Exception as e:
            logger.error(f"Error in enhanced file handler: {type(e).__name__}")
            # Fallback: pass original context
            self.debouncer.add_event(context)
        
    def start(self):
        """Start ambient capture daemon."""
        print("=" * 60)
        print("[CORTEX] Starting Ambient Capture Daemon...")
        print("=" * 60)
        
        # Install git hooks
        self.git_monitor.install_hooks()
        
        # Start monitoring components
        self.file_watcher.start()
        self.terminal_monitor.start()
        
        self.running = True
        
        # Periodic VS Code state capture
        threading.Thread(target=self._periodic_vscode_capture, daemon=True).start()
        
        print("[CORTEX] Ambient Capture Daemon started successfully")
        print("[CORTEX] Monitoring workspace:", self.workspace_path)
        print("[CORTEX] Press Ctrl+C to stop")
        print("=" * 60)
        
        # Keep alive
        self._keep_alive()
        
    def _periodic_vscode_capture(self):
        """Periodically capture VS Code state."""
        while self.running:
            try:
                open_files = self.vscode_monitor.get_open_files()
                active_file = self.vscode_monitor.get_active_file()
                
                if open_files or active_file:
                    context = {
                        "type": "vscode_state",
                        "open_files": open_files,
                        "active_file": active_file,
                        "timestamp": datetime.now().isoformat()
                    }
                    self.debouncer.add_event(context)
                    
            except Exception as e:
                print(f"[CORTEX] Error capturing VS Code state: {e}")
                
            time.sleep(60)  # Every minute
            
    def _keep_alive(self):
        """Keep daemon running."""
        signal.signal(signal.SIGINT, self._handle_shutdown)
        signal.signal(signal.SIGTERM, self._handle_shutdown)
        
        try:
            while self.running:
                time.sleep(1)
        except KeyboardInterrupt:
            self._handle_shutdown(None, None)
            
    def _handle_shutdown(self, signum, frame):
        """Handle shutdown gracefully."""
        print("\n" + "=" * 60)
        print("[CORTEX] Shutting down Ambient Capture Daemon...")
        print("=" * 60)
        
        self.running = False
        
        # Stop file watcher
        if hasattr(self, 'file_watcher'):
            self.file_watcher.observer.stop()
            self.file_watcher.observer.join(timeout=2)
            
        # Stop terminal monitor
        if hasattr(self, 'terminal_monitor'):
            self.terminal_monitor.monitoring = False
            
        print("[CORTEX] Daemon stopped")
        sys.exit(0)


# ============================================================================
# Entry Point
# ============================================================================

def main():
    """Main entry point."""
    print("\n[CORTEX] Ambient Context Capture Daemon v1.0")
    print("[CORTEX] Author: Asif Hussain")
    print("[CORTEX] Copyright © 2024-2025 Asif Hussain. All rights reserved.\n")
    
    # Get workspace path
    workspace_path = os.environ.get("CORTEX_ROOT")
    
    if not workspace_path:
        workspace_path = os.getcwd()
        print(f"[CORTEX] CORTEX_ROOT not set, using current directory: {workspace_path}")
    
    # Validate workspace
    workspace = Path(workspace_path)
    if not workspace.exists():
        print(f"[CORTEX] ERROR: Workspace not found: {workspace_path}")
        sys.exit(1)
        
    # Start daemon
    daemon = AmbientCaptureDaemon(workspace_path)
    daemon.start()


if __name__ == "__main__":
    main()
