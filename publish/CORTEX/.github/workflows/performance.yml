name: Performance Tests

on:
  push:
    branches: [ main, CORTEX-2.0, develop ]
  pull_request:
    branches: [ main, CORTEX-2.0, develop ]
  schedule:
    # Run daily at 2 AM UTC to catch performance drift
    - cron: '0 2 * * *'
  workflow_dispatch:
    # Allow manual trigger

jobs:
  performance:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Full history for git metrics
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-performance-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-performance-
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark pyyaml
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Initialize CORTEX brain databases
      run: |
        python -c "
        from pathlib import Path
        from src.tier1.conversation_manager import ConversationManager
        from src.tier2.knowledge_graph import KnowledgeGraph
        from src.tier3.context_intelligence import ContextIntelligence
        
        # Ensure directories exist
        Path('cortex-brain/tier1').mkdir(parents=True, exist_ok=True)
        Path('cortex-brain/tier2').mkdir(parents=True, exist_ok=True)
        Path('cortex-brain/tier3').mkdir(parents=True, exist_ok=True)
        
        # Initialize databases (creates schemas)
        cm = ConversationManager(Path('cortex-brain/tier1/conversations.db'))
        kg = KnowledgeGraph()
        ci = ContextIntelligence(db_path=None)
        
        print('✅ Brain databases initialized')
        "
    
    - name: Run fast performance tests
      id: fast_tests
      run: |
        pytest tests/performance/ \
          -v \
          -m "performance and not slow" \
          --tb=short \
          --color=yes \
          --durations=10
      continue-on-error: false
    
    - name: Run slow performance tests
      id: slow_tests
      run: |
        pytest tests/performance/ \
          -v \
          -m "slow" \
          --tb=short \
          --color=yes \
          --durations=10
      continue-on-error: false
    
    - name: Run full performance profiler
      id: profiler
      run: |
        python scripts/profile_performance.py > performance-report.txt
        cat performance-report.txt
      continue-on-error: true
    
    - name: Upload performance report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: |
          performance-report.txt
          logs/performance-report-*.json
        retention-days: 30
    
    - name: Check for performance regressions
      if: always()
      run: |
        echo "### Performance Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.fast_tests.outcome }}" == "success" ]; then
          echo "✅ Fast tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Fast tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ steps.slow_tests.outcome }}" == "success" ]; then
          echo "✅ Slow tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ Slow tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Performance Thresholds:**" >> $GITHUB_STEP_SUMMARY
        echo "- Tier 1: ≤50ms (baseline: 0.48ms)" >> $GITHUB_STEP_SUMMARY
        echo "- Tier 2: ≤150ms (baseline: 0.72ms)" >> $GITHUB_STEP_SUMMARY
        echo "- Tier 3: ≤500ms (baseline: 52.51ms)" >> $GITHUB_STEP_SUMMARY
        echo "- Operations: <5000ms (baseline: 1431ms)" >> $GITHUB_STEP_SUMMARY
        
        # Fail the job if any tests failed
        if [ "${{ steps.fast_tests.outcome }}" != "success" ] || [ "${{ steps.slow_tests.outcome }}" != "success" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⚠️ **PERFORMANCE REGRESSION DETECTED**" >> $GITHUB_STEP_SUMMARY
          exit 1
        fi
    
    - name: Comment on PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let comment = '## Performance Test Results\n\n';
          
          if ('${{ steps.fast_tests.outcome }}' === 'success' && '${{ steps.slow_tests.outcome }}' === 'success') {
            comment += 'All performance tests passed!\n\n';
          } else {
            comment += 'Performance regression detected!\n\n';
          }
          
          comment += '### Test Summary\n';
          comment += '- Fast tests: ' + ('${{ steps.fast_tests.outcome }}' === 'success' ? 'PASSED' : 'FAILED') + '\n';
          comment += '- Slow tests: ' + ('${{ steps.slow_tests.outcome }}' === 'success' ? 'PASSED' : 'FAILED') + '\n\n';
          
          comment += '### Performance Thresholds\n';
          comment += '| Tier | Target | Baseline | Status |\n';
          comment += '|------|--------|----------|--------|\n';
          comment += '| Tier 1 | <=50ms | 0.48ms | PASS (100x faster) |\n';
          comment += '| Tier 2 | <=150ms | 0.72ms | PASS (208x faster) |\n';
          comment += '| Tier 3 | <=500ms | 52.51ms | PASS (10x faster) |\n';
          comment += '| Operations | <5000ms | 1431ms | PASS (3.5x faster) |\n\n';
          
          comment += '[View detailed performance report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  benchmark-comparison:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout PR branch
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark pyyaml
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Run PR benchmarks
      run: |
        python scripts/profile_performance.py > pr-performance.txt || true
        echo "PR_PERF_REPORT<<EOF" >> $GITHUB_ENV
        cat pr-performance.txt >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV
    
    - name: Checkout base branch
      uses: actions/checkout@v3
      with:
        ref: ${{ github.base_ref }}
    
    - name: Run base benchmarks
      run: |
        python scripts/profile_performance.py > base-performance.txt || true
        echo "BASE_PERF_REPORT<<EOF" >> $GITHUB_ENV
        cat base-performance.txt >> $GITHUB_ENV
        echo "EOF" >> $GITHUB_ENV
    
    - name: Compare performance
      run: |
        echo "### Performance Comparison" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Base branch:** ${{ github.base_ref }}" >> $GITHUB_STEP_SUMMARY
        echo "**PR branch:** ${{ github.head_ref }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "See artifacts for detailed comparison." >> $GITHUB_STEP_SUMMARY
    
    - name: Upload comparison artifacts
      uses: actions/upload-artifact@v3
      with:
        name: performance-comparison
        path: |
          pr-performance.txt
          base-performance.txt
        retention-days: 30
