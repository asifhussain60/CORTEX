# CORTEX Unified Crawler System - Quick Reference

## What We Built

âœ… **Complete crawler architecture** with:
- Base crawler class for extensibility
- Orchestrator with dependency resolution
- Tooling crawler (databases, APIs, tools)
- UI crawler (components, element IDs, routes)
- Ready for database crawler integration

## File Structure

```
src/crawlers/
â”œâ”€â”€ __init__.py                 # Package exports
â”œâ”€â”€ base_crawler.py             # BaseCrawler (345 lines) âœ…
â”œâ”€â”€ orchestrator.py             # Orchestrator (427 lines) âœ…
â”œâ”€â”€ tooling_crawler.py          # Tooling discovery (733 lines) âœ…
â”œâ”€â”€ ui_crawler.py               # UI discovery (490 lines) âœ…
â””â”€â”€ README.md                   # Documentation (215 lines) âœ…

src/tier2/
â””â”€â”€ oracle_crawler.py           # Existing (584 lines) - needs adapter

cortex-brain/cortex-2.0-design/
â””â”€â”€ CRAWLER-SYSTEM-COMPLETE.md  # Implementation summary
```

## How It Works

```
1. Orchestrator starts
   â†“
2. Tooling Crawler runs (CRITICAL priority)
   - Discovers: Databases, APIs, Build Tools, Frameworks
   - Determines: Which other crawlers to run
   â†“
3. UI Crawler runs (if UI framework detected)
   - Discovers: Components, Element IDs, Routes
   â†“
4. API Crawler runs
   - Discovers: REST endpoints, GraphQL schemas
   â†“
5. Database Crawlers run (conditional)
   - Oracle (if Oracle connections found)
   - SQL Server (if SQL Server connections found)
   - PostgreSQL (if Postgres connections found)
   â†“
6. Results stored in Knowledge Graph (Tier 2)
```

## Usage Example

```python
from pathlib import Path
from src.crawlers.orchestrator import CrawlerOrchestrator
from src.crawlers.tooling_crawler import ToolingCrawler
from src.crawlers.ui_crawler import UICrawler
from src.tier2.knowledge_graph import KnowledgeGraph

# Initialize
kg = KnowledgeGraph()
orchestrator = CrawlerOrchestrator(
    workspace_path=Path.cwd(),
    knowledge_graph=kg,
    parallel=True
)

# Register crawlers
orchestrator.register(ToolingCrawler)
orchestrator.register(UICrawler)
# Add more as they're implemented

# Run all
result = orchestrator.run_all()

print(f"Completed: {result.completed}/{result.total_crawlers}")
print(f"Items discovered: {result.total_items_discovered}")
print(f"Patterns created: {result.total_patterns_created}")
```

## Key Features

### 1. Extensibility
- Add new crawlers by inheriting from `BaseCrawler`
- Implement 4 methods: `get_crawler_info()`, `validate()`, `crawl()`, `store_results()`
- Register with orchestrator

### 2. Smart Execution
- **Dependency Resolution**: Runs crawlers in correct order
- **Conditional Execution**: Skips DB crawlers if no connections
- **Parallel Execution**: Independent crawlers run concurrently
- **Error Handling**: Individual failures don't stop others

### 3. Knowledge Graph Integration
- All results stored as patterns in Tier 2
- FTS5 search enabled
- Namespace boundaries enforced
- Confidence scoring

## What Tooling Crawler Discovers

### Databases
- âœ… Oracle tnsnames.ora
- âœ… Environment variables (ORACLE_CONNECTION_STRING, etc.)
- âœ… Connection strings in code
- âœ… Configuration files (appsettings.json, .env)

### APIs
- âœ… OpenAPI/Swagger specs
- âœ… Environment variables (API_BASE_URL)
- âœ… REST endpoints in code

### Build Tools
- âœ… npm/yarn (package.json)
- âœ… Maven (pom.xml)
- âœ… Gradle (build.gradle)
- âœ… .NET (*.csproj)
- âœ… Python (requirements.txt, Pipfile, pyproject.toml)
- âœ… Go (go.mod)
- âœ… Rust (Cargo.toml)

### Frameworks
- âœ… React, Angular, Vue (frontend)
- âœ… Flask, Django, FastAPI (Python)
- âœ… Express, Next.js (Node.js)

## What UI Crawler Discovers

### React Components
- âœ… Component names and files
- âœ… Element IDs (`id="..."`)
- âœ… Routes (`<Route path="...">`)
- âœ… Props
- âœ… Dependencies

### Angular Components
- âœ… Component classes
- âœ… Template element IDs
- âœ… Route configurations
- âœ… @Input decorators

### Vue Components
- âœ… Component files
- âœ… Template element IDs
- âœ… Props
- âœ… Routes

## Next Steps

### Phase 1: Complete Core Crawlers
1. â¬œ API crawler (REST, GraphQL)
2. â¬œ Adapt Oracle crawler to BaseCrawler
3. â¬œ SQL Server crawler
4. â¬œ PostgreSQL crawler

### Phase 2: Plugin Integration
5. â¬œ Create crawler plugin
6. â¬œ Add commands (`cortex crawlers:run`, `crawlers:list`, etc.)
7. â¬œ Progress reporting
8. â¬œ Result formatting

### Phase 3: Testing
9. â¬œ Unit tests for all crawlers
10. â¬œ Orchestrator tests
11. â¬œ Integration tests
12. â¬œ Mock database tests

### Phase 4: Documentation
13. â¬œ User guide
14. â¬œ Developer guide
15. â¬œ API documentation

## Adding a New Crawler

```python
from src.crawlers.base_crawler import BaseCrawler, CrawlerPriority

class MyCrawler(BaseCrawler):
    def get_crawler_info(self):
        return {
            'crawler_id': 'my_crawler',
            'name': 'My Custom Crawler',
            'version': '1.0.0',
            'priority': CrawlerPriority.MEDIUM,
            'dependencies': ['tooling_crawler'],  # Optional
            'description': 'Discovers X, Y, Z'
        }
    
    def validate(self):
        # Check if crawler can run
        return True
    
    def crawl(self):
        # Discovery logic
        return {'items': [...]}
    
    def store_results(self, data):
        # Store in knowledge graph
        for item in data['items']:
            self.knowledge_graph.add_pattern(...)
        return len(data['items'])

# Register
orchestrator.register(MyCrawler)
```

## Performance

| Crawler | Expected Time |
|---------|---------------|
| Tooling | ~3 seconds |
| UI | ~7 seconds |
| API | ~5 seconds |
| Oracle | ~15 seconds |
| SQL Server | ~15 seconds |
| **Total (parallel)** | **~20 seconds** |

## Design Principles

1. **Extensible**: Easy to add new crawlers
2. **Conditional**: Smart execution based on detection
3. **Resilient**: Errors don't cascade
4. **Integrated**: Deep knowledge graph integration
5. **Fast**: Parallel execution where possible
6. **User-Friendly**: Simple API and CLI

## Testing the System

When ready, you can test with:

```bash
# After plugin implementation
cortex crawlers:run

# Or manually
python -c "
from pathlib import Path
from src.crawlers.orchestrator import CrawlerOrchestrator
from src.crawlers.tooling_crawler import ToolingCrawler
from src.tier2.knowledge_graph import KnowledgeGraph

kg = KnowledgeGraph()
orch = CrawlerOrchestrator(Path.cwd(), kg)
orch.register(ToolingCrawler)
result = orch.run_all()
print(result.to_dict())
"
```

## Summary

âœ… **Architecture Complete**
- Extensible base class with lifecycle
- Orchestrator with dependencies and conditions
- Smart execution flow

âœ… **Core Crawlers Implemented**
- Tooling crawler (733 lines) - discovers everything
- UI crawler (490 lines) - finds components and IDs

âœ… **Ready for Integration**
- Knowledge graph storage
- Plugin system hooks
- Configuration support

ðŸš§ **Next: Implementation**
- API crawler
- Database adapters
- Plugin integration
- Testing suite

**Total Code**: ~2,236 lines (base + orchestrator + crawlers + docs)

The system is ready to test the crawlers individually and can be extended with additional crawler types as needed!
