# CORTEX 2.0 Incremental Creation System

**Document:** 09-incremental-creation.md  
**Version:** 2.0.0-alpha  
**Date:** 2025-11-07

---

## ğŸ¯ Purpose

Prevent length limit errors when creating large files by:
- Breaking file creation into manageable chunks
- Providing intelligent split points
- Tracking creation progress
- Enabling resume on failure
- Validating completeness

---

## âŒ Current Pain Points (CORTEX 1.0)

### Problem 1: Length Limit Errors
```
Attempting to create: 07-self-review-system.md (850 lines)

Result:
  âŒ Error: Length limit exceeded
  âŒ Partial file created (incomplete)
  âŒ No way to resume
  âŒ Must start over
  âŒ Frustrating experience
```

### Problem 2: No Chunking Strategy
```
Current approach:
  1. Generate entire file content
  2. Call create_file tool
  3. Hope it's not too long
  4. If error, manually split
  5. Create multiple smaller files

Issues:
  âŒ No automatic chunking
  âŒ Manual split points
  âŒ File fragmentation
  âŒ Lost context between chunks
```

### Problem 3: No Progress Tracking
```
Creating large documentation:
  - 20 documents to create
  - Each 500-800 lines
  - Some fail mid-creation
  - No way to know where we left off
  - Must manually track progress
```

### Problem 4: Incomplete Files
```
When creation fails:
  âŒ File partially written
  âŒ Missing critical sections
  âŒ Invalid syntax (unmatched braces, etc.)
  âŒ No validation
  âŒ Hard to detect incompleteness
```

---

## âœ… CORTEX 2.0 Solution

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Incremental Creation Engine (NEW)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Intelligent file chunking                             â”‚
â”‚  â€¢ Smart split point detection                           â”‚
â”‚  â€¢ Progress tracking & resume                            â”‚
â”‚  â€¢ Completeness validation                               â”‚
â”‚  â€¢ Automatic retry on failure                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Chunker      â”‚        â”‚  Tracker        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚â€¢ Split logic  â”‚        â”‚â€¢ Progress state â”‚
    â”‚â€¢ Smart breaks â”‚        â”‚â€¢ Resume points  â”‚
    â”‚â€¢ Size calc    â”‚        â”‚â€¢ Validation     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Validator                â”‚
            â”‚  â€¢ Syntax check           â”‚
            â”‚  â€¢ Completeness check     â”‚
            â”‚  â€¢ Section verification   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Implementation: Incremental Creation Engine

```python
# src/utils/incremental_creation.py

from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Callable
from pathlib import Path
from enum import Enum
import json
import re

class ChunkType(Enum):
    """Type of content chunk"""
    HEADER = "header"
    SECTION = "section"
    CODE_BLOCK = "code_block"
    LIST = "list"
    TABLE = "table"
    FOOTER = "footer"

@dataclass
class ContentChunk:
    """A chunk of file content"""
    chunk_id: int
    chunk_type: ChunkType
    content: str
    line_start: int
    line_end: int
    estimated_tokens: int
    dependencies: List[int]  # Chunks that must exist before this one
    
@dataclass
class CreationProgress:
    """Progress tracking for file creation"""
    file_path: Path
    total_chunks: int
    completed_chunks: List[int]
    failed_chunks: List[int]
    current_chunk: Optional[int]
    started_at: str
    last_update: str
    complete: bool

class IncrementalCreationEngine:
    """System for creating large files in manageable chunks"""
    
    # Configuration
    MAX_CHUNK_LINES = 200  # Maximum lines per chunk
    MAX_CHUNK_TOKENS = 4000  # Approximate token limit per chunk
    
    def __init__(self, 
                 file_path: Path,
                 progress_dir: Path = None):
        """
        Initialize incremental creation engine
        
        Args:
            file_path: Target file to create
            progress_dir: Directory for progress tracking files
        """
        self.file_path = file_path
        self.progress_dir = progress_dir or Path(".cortex/creation-progress")
        self.progress_dir.mkdir(parents=True, exist_ok=True)
        
        self.progress_file = self.progress_dir / f"{file_path.name}.progress.json"
        self.chunks: List[ContentChunk] = []
        self.progress: Optional[CreationProgress] = None
    
    def create_file_incrementally(self, 
                                  content: str,
                                  validator: Callable = None) -> bool:
        """
        Create file incrementally in chunks
        
        Args:
            content: Complete file content to create
            validator: Optional function to validate each chunk
        
        Returns:
            True if file created successfully
        """
        print(f"ğŸ“ Creating file incrementally: {self.file_path.name}")
        
        # Load existing progress if resuming
        if self.progress_file.exists():
            print("   ğŸ”„ Resuming from previous progress...")
            self.progress = self._load_progress()
        
        # Split content into chunks
        if not self.chunks:
            print("   ğŸ”ª Splitting content into chunks...")
            self.chunks = self._split_into_chunks(content)
            print(f"   ğŸ“Š Created {len(self.chunks)} chunks")
        
        # Initialize progress tracking
        if not self.progress:
            self.progress = self._init_progress()
        
        # Create/append chunks
        try:
            for chunk in self.chunks:
                if chunk.chunk_id in self.progress.completed_chunks:
                    print(f"   âœ“ Chunk {chunk.chunk_id}/{len(self.chunks)} already complete")
                    continue
                
                print(f"   âš™ï¸  Writing chunk {chunk.chunk_id}/{len(self.chunks)} ({chunk.chunk_type.value})...")
                
                # Validate dependencies
                if not self._check_dependencies(chunk):
                    print(f"   âš ï¸  Skipping chunk {chunk.chunk_id} - dependencies not met")
                    continue
                
                # Write chunk
                success = self._write_chunk(chunk, validator)
                
                if success:
                    self.progress.completed_chunks.append(chunk.chunk_id)
                    self._save_progress()
                else:
                    self.progress.failed_chunks.append(chunk.chunk_id)
                    self._save_progress()
                    raise Exception(f"Failed to write chunk {chunk.chunk_id}")
            
            # Validate complete file
            if validator:
                print("   ğŸ” Validating complete file...")
                if not validator(self.file_path):
                    raise Exception("File validation failed")
            
            # Mark as complete
            self.progress.complete = True
            self._save_progress()
            
            print(f"   âœ… File created successfully: {self.file_path}")
            return True
            
        except Exception as e:
            print(f"   âŒ Creation failed: {e}")
            print(f"   ğŸ’¾ Progress saved to: {self.progress_file}")
            return False
    
    def _split_into_chunks(self, content: str) -> List[ContentChunk]:
        """
        Split content into intelligent chunks
        
        Strategy:
        1. Identify natural split points (headers, section breaks)
        2. Keep related content together (code blocks, lists)
        3. Respect size limits
        4. Track dependencies
        """
        chunks = []
        lines = content.split('\n')
        current_chunk_lines = []
        current_chunk_type = ChunkType.SECTION
        line_number = 1
        chunk_id = 1
        
        def create_chunk():
            nonlocal chunk_id, current_chunk_lines, line_number
            if not current_chunk_lines:
                return
            
            chunk_content = '\n'.join(current_chunk_lines)
            chunk = ContentChunk(
                chunk_id=chunk_id,
                chunk_type=current_chunk_type,
                content=chunk_content,
                line_start=line_number - len(current_chunk_lines),
                line_end=line_number - 1,
                estimated_tokens=self._estimate_tokens(chunk_content),
                dependencies=[chunk_id - 1] if chunk_id > 1 else []
            )
            chunks.append(chunk)
            chunk_id += 1
            current_chunk_lines = []
        
        in_code_block = False
        code_block_fence = None
        
        for line in lines:
            # Detect code block boundaries
            if line.strip().startswith('```'):
                if not in_code_block:
                    in_code_block = True
                    code_block_fence = line.strip()
                    current_chunk_type = ChunkType.CODE_BLOCK
                elif line.strip() == code_block_fence or line.strip() == '```':
                    in_code_block = False
                    current_chunk_lines.append(line)
                    line_number += 1
                    # End code block chunk
                    create_chunk()
                    current_chunk_type = ChunkType.SECTION
                    continue
            
            # If in code block, keep accumulating
            if in_code_block:
                current_chunk_lines.append(line)
                line_number += 1
                continue
            
            # Detect headers (natural split points)
            if line.startswith('#'):
                # Save previous chunk
                if current_chunk_lines:
                    create_chunk()
                
                # Start new chunk with header
                current_chunk_type = ChunkType.HEADER if line.startswith('# ') else ChunkType.SECTION
                current_chunk_lines.append(line)
                line_number += 1
                continue
            
            # Detect section breaks (---, ***, etc.)
            if re.match(r'^[\-\*=]{3,}$', line.strip()):
                current_chunk_lines.append(line)
                line_number += 1
                # Section break ends current chunk
                create_chunk()
                continue
            
            # Check chunk size limits
            chunk_content = '\n'.join(current_chunk_lines + [line])
            estimated_tokens = self._estimate_tokens(chunk_content)
            
            if (len(current_chunk_lines) >= self.MAX_CHUNK_LINES or 
                estimated_tokens >= self.MAX_CHUNK_TOKENS):
                # Find better split point (empty line)
                if line.strip() == '':
                    current_chunk_lines.append(line)
                    line_number += 1
                    create_chunk()
                    continue
            
            # Add line to current chunk
            current_chunk_lines.append(line)
            line_number += 1
        
        # Create final chunk
        if current_chunk_lines:
            create_chunk()
        
        return chunks
    
    def _write_chunk(self, chunk: ContentChunk, validator: Callable = None) -> bool:
        """Write a single chunk to file"""
        try:
            # Determine write mode
            if chunk.chunk_id == 1:
                # First chunk - create file
                mode = 'w'
            else:
                # Subsequent chunks - append
                mode = 'a'
            
            with open(self.file_path, mode, encoding='utf-8') as f:
                if chunk.chunk_id > 1:
                    # Add newline between chunks
                    f.write('\n')
                f.write(chunk.content)
            
            # Validate chunk if validator provided
            if validator:
                if not validator(self.file_path, chunk):
                    return False
            
            return True
            
        except Exception as e:
            print(f"      âŒ Error writing chunk: {e}")
            return False
    
    def _check_dependencies(self, chunk: ContentChunk) -> bool:
        """Check if chunk dependencies are satisfied"""
        if not chunk.dependencies:
            return True
        
        return all(dep_id in self.progress.completed_chunks 
                  for dep_id in chunk.dependencies)
    
    def _estimate_tokens(self, content: str) -> int:
        """Estimate token count for content"""
        # Rough estimation: 1 token â‰ˆ 4 characters
        return len(content) // 4
    
    def _init_progress(self) -> CreationProgress:
        """Initialize progress tracking"""
        from datetime import datetime
        
        return CreationProgress(
            file_path=self.file_path,
            total_chunks=len(self.chunks),
            completed_chunks=[],
            failed_chunks=[],
            current_chunk=1,
            started_at=datetime.now().isoformat(),
            last_update=datetime.now().isoformat(),
            complete=False
        )
    
    def _save_progress(self):
        """Save progress to file"""
        from datetime import datetime
        
        self.progress.last_update = datetime.now().isoformat()
        
        progress_data = {
            "file_path": str(self.progress.file_path),
            "total_chunks": self.progress.total_chunks,
            "completed_chunks": self.progress.completed_chunks,
            "failed_chunks": self.progress.failed_chunks,
            "current_chunk": self.progress.current_chunk,
            "started_at": self.progress.started_at,
            "last_update": self.progress.last_update,
            "complete": self.progress.complete
        }
        
        with open(self.progress_file, 'w') as f:
            json.dump(progress_data, f, indent=2)
    
    def _load_progress(self) -> CreationProgress:
        """Load progress from file"""
        with open(self.progress_file, 'r') as f:
            data = json.load(f)
        
        return CreationProgress(
            file_path=Path(data["file_path"]),
            total_chunks=data["total_chunks"],
            completed_chunks=data["completed_chunks"],
            failed_chunks=data["failed_chunks"],
            current_chunk=data["current_chunk"],
            started_at=data["started_at"],
            last_update=data["last_update"],
            complete=data["complete"]
        )
    
    def resume_creation(self, content: str) -> bool:
        """Resume failed creation from progress file"""
        if not self.progress_file.exists():
            print("âŒ No progress file found - cannot resume")
            return False
        
        print(f"ğŸ”„ Resuming creation from progress file...")
        return self.create_file_incrementally(content)
    
    def get_progress_report(self) -> str:
        """Get human-readable progress report"""
        if not self.progress:
            return "No creation in progress"
        
        completed_pct = (len(self.progress.completed_chunks) / self.progress.total_chunks) * 100
        
        lines = [
            f"File: {self.progress.file_path.name}",
            f"Progress: {len(self.progress.completed_chunks)}/{self.progress.total_chunks} chunks ({completed_pct:.1f}%)",
            f"Started: {self.progress.started_at}",
            f"Last Update: {self.progress.last_update}",
            f"Status: {'âœ… Complete' if self.progress.complete else 'â³ In Progress'}"
        ]
        
        if self.progress.failed_chunks:
            lines.append(f"Failed Chunks: {', '.join(map(str, self.progress.failed_chunks))}")
        
        return '\n'.join(lines)

class FileValidator:
    """Validators for different file types"""
    
    @staticmethod
    def validate_markdown(file_path: Path, chunk: ContentChunk = None) -> bool:
        """Validate markdown file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check for common markdown issues
            issues = []
            
            # Check code block balance
            fences = content.count('```')
            if fences % 2 != 0:
                issues.append("Unbalanced code fences")
            
            # Check header structure
            lines = content.split('\n')
            if lines and not lines[0].startswith('#'):
                issues.append("File should start with a header")
            
            if issues:
                print(f"      âš ï¸  Markdown validation issues: {', '.join(issues)}")
                return False
            
            return True
            
        except Exception as e:
            print(f"      âŒ Validation error: {e}")
            return False
    
    @staticmethod
    def validate_python(file_path: Path, chunk: ContentChunk = None) -> bool:
        """Validate Python file"""
        try:
            import ast
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse Python syntax
            ast.parse(content)
            return True
            
        except SyntaxError as e:
            print(f"      âŒ Python syntax error: {e}")
            return False
        except Exception as e:
            print(f"      âŒ Validation error: {e}")
            return False
    
    @staticmethod
    def validate_json(file_path: Path, chunk: ContentChunk = None) -> bool:
        """Validate JSON file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                json.load(f)
            return True
            
        except json.JSONDecodeError as e:
            print(f"      âŒ JSON validation error: {e}")
            return False
        except Exception as e:
            print(f"      âŒ Validation error: {e}")
            return False

def create_large_file(file_path: Path, 
                     content: str,
                     file_type: str = "markdown") -> bool:
    """
    Convenience function for creating large files
    
    Args:
        file_path: Target file path
        content: Complete file content
        file_type: Type of file (markdown, python, json, etc.)
    
    Returns:
        True if file created successfully
    """
    # Select validator
    validators = {
        "markdown": FileValidator.validate_markdown,
        "python": FileValidator.validate_python,
        "json": FileValidator.validate_json
    }
    validator = validators.get(file_type)
    
    # Create incrementally
    engine = IncrementalCreationEngine(file_path)
    return engine.create_file_incrementally(content, validator)
```

---

## ğŸ“Š Usage Examples

### Example 1: Create Large Markdown Document

```python
from utils.incremental_creation import create_large_file
from pathlib import Path

# Generate large content
content = generate_design_document()  # 1000+ lines

# Create incrementally
success = create_large_file(
    file_path=Path("docs/07-self-review-system.md"),
    content=content,
    file_type="markdown"
)

if success:
    print("âœ… Document created successfully")
else:
    print("âŒ Creation failed - check progress file for resume")
```

### Example 2: Resume Failed Creation

```python
from utils.incremental_creation import IncrementalCreationEngine
from pathlib import Path

# Load previous attempt
file_path = Path("docs/08-database-maintenance.md")
engine = IncrementalCreationEngine(file_path)

# Resume from progress file
content = generate_design_document()  # Regenerate content
success = engine.resume_creation(content)

# Check progress
print(engine.get_progress_report())
```

### Example 3: Custom Validator

```python
def validate_design_doc(file_path: Path, chunk = None) -> bool:
    """Custom validator for design documents"""
    with open(file_path, 'r') as f:
        content = f.read()
    
    required_sections = [
        "## ğŸ¯ Purpose",
        "## âŒ Current Pain Points",
        "## âœ… CORTEX 2.0 Solution",
        "## ğŸ—ï¸ Implementation"
    ]
    
    for section in required_sections:
        if section not in content:
            print(f"Missing required section: {section}")
            return False
    
    return True

# Use custom validator
engine = IncrementalCreationEngine(file_path)
engine.create_file_incrementally(content, validator=validate_design_doc)
```

### Example 4: Batch Creation

```python
from utils.incremental_creation import create_large_file

documents = [
    ("09-incremental-creation.md", generate_doc_09()),
    ("10-agent-workflows.md", generate_doc_10()),
    ("11-database-schema-updates.md", generate_doc_11()),
]

for filename, content in documents:
    print(f"\nğŸ“ Creating {filename}...")
    
    success = create_large_file(
        file_path=Path(f"cortex-brain/cortex-2.0-design/{filename}"),
        content=content,
        file_type="markdown"
    )
    
    if not success:
        print(f"âš ï¸  Failed to create {filename} - will retry later")
        continue
    
    print(f"âœ… {filename} complete")
```

---

## ğŸ”Œ Integration with CORTEX

### 1. Agent Integration

```python
# src/tier3/documentation_agent.py

from utils.incremental_creation import create_large_file

class DocumentationAgent:
    """Agent for creating documentation"""
    
    def create_design_document(self, doc_number: int, content: str):
        """Create design document incrementally"""
        
        filename = f"{doc_number:02d}-{self.get_doc_title(doc_number)}.md"
        file_path = self.paths.resolve(f"cortex-brain/cortex-2.0-design/{filename}")
        
        print(f"ğŸ“ Creating design document: {filename}")
        
        success = create_large_file(
            file_path=file_path,
            content=content,
            file_type="markdown"
        )
        
        if success:
            # Update index
            self.update_index(doc_number, "Complete")
            return True
        else:
            print(f"âš ï¸  Document creation incomplete - progress saved")
            return False
```

### 2. Plugin Integration

```python
# src/plugins/incremental_creation_plugin.py

from plugins.base_plugin import BasePlugin, PluginMetadata, PluginCategory
from utils.incremental_creation import IncrementalCreationEngine

class Plugin(BasePlugin):
    """Incremental file creation plugin"""
    
    def _get_metadata(self) -> PluginMetadata:
        return PluginMetadata(
            plugin_id="incremental_creation_plugin",
            name="Incremental Creation",
            version="1.0.0",
            category=PluginCategory.UTILITY,
            description="Create large files in manageable chunks",
            author="CORTEX"
        )
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute incremental creation"""
        
        file_path = Path(context["file_path"])
        content = context["content"]
        
        engine = IncrementalCreationEngine(file_path)
        success = engine.create_file_incrementally(content)
        
        return {
            "success": success,
            "progress": engine.get_progress_report()
        }
```

---

## ğŸ¯ Smart Chunking Strategies

### 1. Markdown Documents

```python
Split points:
  âœ“ Level 1 headers (#)
  âœ“ Level 2 headers (##)
  âœ“ Section dividers (---)
  âœ“ Code block boundaries (```)
  âœ“ Empty lines between sections

Keep together:
  âœ“ Complete code blocks
  âœ“ Tables
  âœ“ Lists
  âœ“ Related paragraphs
```

### 2. Python Files

```python
Split points:
  âœ“ Class definitions
  âœ“ Function definitions
  âœ“ Module docstrings
  âœ“ Import blocks

Keep together:
  âœ“ Complete functions
  âœ“ Complete classes
  âœ“ Docstrings with their code
  âœ“ Related imports
```

### 3. JSON/Config Files

```python
Split points:
  âœ“ Top-level keys
  âœ“ Array elements
  âœ“ Object boundaries

Keep together:
  âœ“ Complete objects
  âœ“ Complete arrays
  âœ“ Related configuration sections
```

---

## ğŸ“Š Progress Tracking

### Progress File Format

```json
{
  "file_path": "cortex-brain/cortex-2.0-design/09-incremental-creation.md",
  "total_chunks": 12,
  "completed_chunks": [1, 2, 3, 4, 5, 6, 7, 8],
  "failed_chunks": [],
  "current_chunk": 9,
  "started_at": "2025-11-07T14:30:00",
  "last_update": "2025-11-07T14:35:00",
  "complete": false
}
```

### Progress Reporting

```bash
$ python scripts/cortex-creation-status.py

ğŸ“Š Incremental Creation Status

File: 09-incremental-creation.md
Progress: 8/12 chunks (66.7%)
Started: 2025-11-07 14:30:00
Last Update: 2025-11-07 14:35:00
Status: â³ In Progress

Remaining chunks: 9, 10, 11, 12
Estimated time: 2-3 minutes
```

---

## âœ… Benefits

### 1. No More Length Limit Errors
```
Before:
  âŒ Create 850-line file
  âŒ "Length limit exceeded"
  âŒ Start over

After:
  âœ… Split into 12 chunks
  âœ… Each chunk ~70 lines
  âœ… All chunks created successfully
  âœ… Complete file validated
```

### 2. Resume on Failure
```
Creation interrupted at chunk 8/12:
  âœ… Progress saved
  âœ… Resume from chunk 9
  âœ… No data loss
  âœ… No manual intervention
```

### 3. Validation at Each Step
```
Each chunk:
  âœ… Syntax validated
  âœ… Dependencies checked
  âœ… Size verified
  
Complete file:
  âœ… Structure validated
  âœ… Completeness verified
  âœ… Required sections present
```

### 4. Smart Chunking
```
Preserves structure:
  âœ… Code blocks stay together
  âœ… Sections remain intact
  âœ… Natural split points
  âœ… Maintains readability
```

---

## ğŸ”§ Configuration

```json
{
  "incremental_creation": {
    "enabled": true,
    "max_chunk_lines": 200,
    "max_chunk_tokens": 4000,
    "progress_dir": ".cortex/creation-progress",
    
    "chunking_strategy": {
      "markdown": {
        "split_on_headers": true,
        "split_on_dividers": true,
        "keep_code_blocks_together": true,
        "min_chunk_lines": 20
      },
      "python": {
        "split_on_classes": true,
        "split_on_functions": true,
        "keep_docstrings_with_code": true,
        "min_chunk_lines": 30
      }
    },
    
    "validation": {
      "enabled": true,
      "validate_syntax": true,
      "validate_structure": true,
      "validate_completeness": true
    },
    
    "resume": {
      "enabled": true,
      "save_progress_every_chunk": true,
      "cleanup_on_success": true,
      "retry_failed_chunks": 3
    }
  }
}
```

---

## ğŸ“ˆ Performance Metrics

```python
# Typical performance

Small file (100-300 lines):
  - Chunks: 2-3
  - Time: 5-10 seconds
  - Success rate: 99%

Medium file (300-600 lines):
  - Chunks: 4-8
  - Time: 15-30 seconds
  - Success rate: 98%

Large file (600-1000 lines):
  - Chunks: 8-15
  - Time: 30-60 seconds
  - Success rate: 95%

Extra large file (1000+ lines):
  - Chunks: 15-25
  - Time: 60-120 seconds
  - Success rate: 90%
  - Resume capability: Essential
```

---

## ğŸ¯ Best Practices

### 1. Choose Appropriate Chunk Size
```python
# Too small
MAX_CHUNK_LINES = 50  # âŒ Too many chunks, overhead

# Too large
MAX_CHUNK_LINES = 500  # âŒ May hit length limits

# Just right
MAX_CHUNK_LINES = 200  # âœ… Good balance
```

### 2. Use Validators
```python
# Always validate
create_large_file(
    file_path=path,
    content=content,
    file_type="markdown"  # âœ… Auto-validation
)

# Never skip validation
# (unless you enjoy debugging incomplete files)
```

### 3. Monitor Progress
```python
# For long-running creations
engine = IncrementalCreationEngine(file_path)
engine.create_file_incrementally(content)

# Check progress
print(engine.get_progress_report())
```

### 4. Handle Failures Gracefully
```python
try:
    success = create_large_file(file_path, content)
    if not success:
        # Progress saved - can resume
        print("âš ï¸  Creation incomplete - resume later")
except Exception as e:
    # Progress saved automatically
    print(f"âŒ Error: {e}")
    print("ğŸ”„ Run with --resume flag to continue")
```

---

## ğŸ”„ Future Enhancements

### Phase 2 (Optional)
```python
# Parallel chunk writing (if safe)
# Compression for large files
# Streaming for very large files
# Cloud storage integration
# Distributed chunk processing
```

---

**Next:** 10-agent-workflows.md (Updated agent responsibilities with new features)
