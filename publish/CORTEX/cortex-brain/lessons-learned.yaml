version: '1.0'
last_updated: '2025-11-16T21:30:00'
total_lessons: 18
lessons:
- id: wpf-tdd-001
  title: WPF implementation without TDD causes runtime crashes
  category: testing
  subcategory: wpf
  severity: high
  date: '2025-11-05'
  problem: "WPF application implemented without tests. Build succeeded but crashed\
    \ at runtime \nwith XamlParseException. User reported \"nothing happens\" - silent\
    \ failure mode.\n"
  root_cause: TDD instinct rule violated - code written before tests
  symptoms:
  - Build succeeds with 0 errors, 0 warnings
  - Application crashes on startup
  - No error dialog shown to user
  - Silent process exit
  solution: Retroactive TDD - created MaterialDesignIconTests.cs, found invalid icon
    names
  prevention_rules:
  - Always write tests FIRST for WPF/XAML components
  - Test XAML initialization before implementing views
  - Validate Material Design icon names with Enum.TryParse
  - Create smoke tests for exe existence and dependencies
  time_cost:
    debugging_time_minutes: 30
    retroactive_fix_minutes: 45
    tdd_would_have_taken_minutes: 10
  related_files:
  - MainWindow.xaml
  - ActivityView.xaml
  - MaterialDesignIconTests.cs
  tags:
  - wpf
  - tdd
  - xaml
  - material-design
  - runtime-crash
  pattern_id: wpf_icon_validation_pattern
  confidence: 0.95
- id: wpf-icons-001
  title: Material Design icon names must be validated
  category: validation
  subcategory: wpf
  severity: high
  date: '2025-11-05'
  problem: 'Used Kind="Lightning" in XAML. "Lightning" is not a valid PackIconKind
    enum value.

    Runtime error: System.FormatException: Lightning is not a valid value for PackIconKind

    '
  root_cause: Assumed icon names match common words without validation
  symptoms:
  - XamlParseException at runtime
  - String-to-enum conversion fails
  - Build succeeds but runtime fails
  solution: Use PackIconKind enum validation test, correct name is 'Flash' not 'Lightning'
  valid_alternatives:
    invalid: Lightning
    valid:
    - Flash
    - FlashAuto
    - Bolt
    - BoltOutline
  prevention_rules:
  - Create Theory test with InlineData for all icon names
  - Use Enum.TryParse<PackIconKind> to validate before XAML
  - Look up icon names in MaterialDesignInXaml documentation
  code_example: "[Theory]\n[InlineData(\"Lightning\")]\npublic void Icon_ShouldBeValidPackIconKind(string\
    \ iconName)\n{\n    bool isValid = Enum.TryParse<PackIconKind>(iconName, out _);\n\
    \    Assert.True(isValid); // FAILS - Lightning invalid\n}\n"
  related_files:
  - MaterialDesignIconTests.cs
  - MainWindow.xaml
  tags:
  - wpf
  - material-design
  - icons
  - validation
  - enum
  pattern_id: wpf_icon_validation_pattern
  confidence: 0.95
- id: wpf-silent-001
  title: WPF runtime errors are silent by default
  category: debugging
  subcategory: wpf
  severity: medium
  date: '2025-11-05'
  problem: 'User clicked .exe and reported "nothing happens". No error dialog, no
    console output.

    Application failed silently due to XAML parsing error during initialization.

    '
  root_cause: WPF apps don't show console by default, XAML errors occur before UI
    appears
  symptoms:
  - Process starts then immediately exits
  - No visible error to user
  - No console output
  - Unclear failure mode
  solution: Use dotnet run in terminal during development to see errors, implement
    smoke tests
  prevention_rules:
  - Test XAML initialization in unit tests
  - Use 'dotnet run' not 'Start-Process' during development
  - Create MainWindow_ShouldInitializeWithoutException test
  - Add error handling in App.xaml.cs startup
  code_example: "[Fact]\npublic void MainWindow_ShouldInitializeWithoutException()\n\
    {\n    var window = new MainWindow();\n    window.InitializeComponent(); // Will\
    \ throw if XAML invalid\n}\n"
  related_files:
  - MainWindow.xaml
  - App.xaml.cs
  tags:
  - wpf
  - debugging
  - silent-failure
  - xaml
  confidence: 0.92
- id: build-success-001
  title: Build success does not guarantee runtime success
  category: testing
  subcategory: general
  severity: high
  date: '2025-11-05'
  problem: 'dotnet build succeeded with 0 errors, 0 warnings, but application crashed
    at runtime.

    False sense of security from clean build.

    '
  root_cause: XAML string-to-enum conversion happens at RUNTIME, not compile time
  symptoms:
  - Clean build output
  - Runtime crash on startup
  - Compile-time validation insufficient
  solution: Always include runtime tests, especially for WPF/XAML components
  anti_pattern: Assuming clean build means working application
  prevention_rules:
  - Build success is necessary but NOT sufficient
  - XAML requires runtime validation tests
  - Test initialization of all UI components
  - Definition of DONE includes passing tests, not just build
  related_technologies:
  - WPF: XAML compiled to BAML, enum conversion at runtime
  - Blazor: Runtime DI and service resolution
  - React: Runtime prop validation
  tags:
  - testing
  - build
  - runtime
  - wpf
  - validation
  confidence: 0.95
- id: conversation-tracking-001
  title: Conversation tracking requires explicit database flush
  category: infrastructure
  subcategory: tier1
  severity: medium
  date: '2025-11-03'
  problem: 'Conversations logged to SQLite but not appearing in queries. Database
    connection

    held open, commits not flushed to disk.

    '
  root_cause: SQLite connection pooling prevents immediate disk writes
  symptoms:
  - INSERT statements succeed
  - Queries return empty results
  - Data appears after Python process exits
  - Race condition between write and read
  solution: Explicitly call conn.commit() and conn.close() after each write operation
  prevention_rules:
  - Always commit SQLite transactions explicitly
  - Close connections to flush to disk
  - Use context managers for automatic cleanup
  - Test database persistence, not just in-memory state
  code_example: '# Wrong - no commit

    cursor.execute("INSERT INTO conversations ...")


    # Right - explicit commit

    cursor.execute("INSERT INTO conversations ...")

    conn.commit()

    conn.close()

    '
  related_files:
  - src/entry_point/cortex_entry.py
  - tests/tier0/test_conversation_tracking_integration.py
  tags:
  - sqlite
  - database
  - tier1
  - conversation-tracking
  - commit
  confidence: 0.9
- id: cross-platform-001
  title: Hard-coded paths break cross-platform compatibility
  category: portability
  subcategory: configuration
  severity: high
  date: '2025-11-09'
  problem: 'Script used /Users/asifhussain/PROJECTS/CORTEX hard-coded path, preventing

    use on Windows or other machines.

    '
  root_cause: Absolute path instead of dynamic detection
  symptoms:
  - Script fails on different machines
  - Path not found errors
  - Manual configuration required per machine
  solution: Use dynamic path detection with dirname and BASH_SOURCE
  prevention_rules:
  - Never hard-code absolute paths in scripts
  - Use $(dirname) for Bash scripts
  - Use Path(__file__).parent for Python
  - Use environment variables or config files for machine-specific paths
  code_example: '# Wrong

    CORTEX_ROOT="/Users/asifhussain/PROJECTS/CORTEX"


    # Right

    CORTEX_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

    '
  related_files:
  - scripts/launchers/run-cortex.sh
  - src/config.py
  tags:
  - cross-platform
  - portability
  - paths
  - configuration
  confidence: 0.98
- id: documentation-bloat-001
  title: Implementation summaries accumulate as bloat over time
  category: documentation
  subcategory: maintenance
  severity: medium
  date: '2025-11-09'
  problem: '19 phase completion and implementation summary markdown files in cortex-brain

    totaling 178 KB. These are historical artifacts, not active knowledge.

    '
  root_cause: No archival strategy for completed phase documentation
  symptoms:
  - Large number of *-COMPLETE.md and *-SUMMARY.md files
  - Active brain directory cluttered
  - Slower file navigation
  - Reduced signal-to-noise ratio
  solution: Created archives/phase-completions/ directory, moved 18 files (178 KB)
  prevention_rules:
  - Archive phase completions after verification period
  - Keep only active working documentation in brain root
  - Use archives/ subdirectories for historical references
  - Periodically audit for documentation bloat (quarterly)
  files_archived:
    count: 18
    total_size_kb: 178
    categories:
    - PHASE-*.md
    - '*-SUMMARY.md'
    - '*-COMPLETE*.md'
    - '*-FIX*.md'
  related_files:
  - cortex-brain/archives/phase-completions/
  tags:
  - documentation
  - maintenance
  - archival
  - cleanup
  confidence: 0.85
- id: yaml-conversion-001
  title: Convert prose lessons to structured YAML for efficiency
  category: optimization
  subcategory: documentation
  severity: medium
  date: '2025-11-09'
  problem: 'Valuable lessons stored in 438-line markdown file (16.2 KB). Difficult
    to query

    programmatically, inefficient token usage.

    '
  root_cause: Initial documentation captured in prose format without structure
  symptoms:
  - Large markdown files with lessons
  - Manual parsing required
  - High token consumption
  - Difficult to pattern match
  solution: Convert to lessons-learned.yaml with structured format (70% token reduction)
  prevention_rules:
  - Capture lessons in structured format from start
  - Use YAML for machine-readable knowledge
  - Reserve markdown for human-facing narrative documentation
  - Convert prose to YAML when file exceeds 10 KB or 200 lines
  token_savings:
    before_kb: 16.2
    after_kb: 5.4
    reduction_percent: 70
  related_files:
  - cortex-brain/lessons-learned.yaml
  - cortex-brain/archives/phase-completions/lessons-learned-wpf-implementation.md
  tags:
  - optimization
  - yaml
  - documentation
  - tokens
  - efficiency
  confidence: 0.9
- id: dependencies-general-001
  title: Missing Python module dependency
  category: dependencies
  subcategory: general
  severity: medium
  date: '2025-11-11'
  problem: Missing Python module dependency
  symptoms:
  - 'ModuleNotFoundError: No module named ''fake_missing_module'''
  confidence: 0.9
  tags:
  - dependencies
  - python
  - modules
  solution: Install missing dependency or update requirements.txt
  prevention_rules:
  - Review similar code for same issue
  error_type: ModuleNotFoundError
- id: platform-windows-001
  title: Platform encoding prevents Unicode character display
  category: platform
  subcategory: windows
  severity: high
  date: '2025-11-11'
  problem: Windows PowerShell console cannot display Unicode characters (emoji, box
    drawing)
  symptoms:
  - '''charmap'' codec can''t encode character'
  - UnicodeEncodeError on Windows
  - Operations fail immediately on header print
  confidence: 0.95
  tags:
  - unicode
  - encoding
  - windows
  - powershell
  - emoji
  root_cause: PowerShell uses cp1252 encoding which lacks Unicode support
  solution: Implement _safe_print() with platform-aware fallback to ASCII
  prevention_rules:
  - Test on all supported platforms (Windows, macOS, Linux)
  - Use platform-aware code paths
  error_type: UnicodeEncodeError
- id: orchestration-general-001
  title: Operation 'Workspace Cleanup' had 1 failed modules
  category: orchestration
  subcategory: general
  severity: high
  date: '2025-11-12'
  problem: Operation 'Workspace Cleanup' had 1 failed modules
  symptoms:
  - 'cleanup_orchestrator: Cleanup failed: ''charmap'' codec can''t encode characters
    in position 2-81: character maps to <undefined>'
  confidence: 0.8
  tags:
  - operation-failure
  - workspace-cleanup
- id: orchestration-general-002
  title: Operation 'Refresh CORTEX Story' had 1 failed modules
  category: orchestration
  subcategory: general
  severity: high
  date: '2025-11-12'
  problem: Operation 'Refresh CORTEX Story' had 1 failed modules
  symptoms:
  - 'apply_narrator_voice: Story transformation failed: ''ApplyNarratorVoiceModule''
    object has no attribute ''_validate_read_time'''
  confidence: 0.8
  tags:
  - operation-failure
  - refresh-cortex-story
- id: orchestration-general-003
  title: Operation 'Test Optional Failure' had 1 failed modules
  category: orchestration
  subcategory: general
  severity: high
  date: '2025-11-12'
  problem: Operation 'Test Optional Failure' had 1 failed modules
  symptoms:
  - 'module_fail: Module module_fail failed intentionally'
  confidence: 0.8
  tags:
  - operation-failure
  - test-optional-failure
- id: orchestration-general-004
  title: Operation 'Test Failure Handling' had 1 failed modules
  category: orchestration
  subcategory: general
  severity: high
  date: '2025-11-12'
  problem: Operation 'Test Failure Handling' had 1 failed modules
  symptoms:
  - 'module_fail: Module module_fail failed intentionally'
  confidence: 0.8
  tags:
  - operation-failure
  - test-failure-handling
- id: orchestration-general-005
  title: Operation 'Update Documentation' had 2 failed modules
  category: orchestration
  subcategory: general
  severity: high
  date: '2025-11-13'
  problem: Operation 'Update Documentation' had 2 failed modules
  symptoms:
  - 'generate_api_docs: No docstring index found in context'
  - 'build_mkdocs_site: MkDocs build failed'
  confidence: 0.8
  tags:
  - operation-failure
  - update-documentation
- id: orchestration-general-006
  title: Operation 'CORTEX Maintenance' had 2 failed modules
  category: orchestration
  subcategory: general
  severity: high
  date: '2025-11-14'
  problem: Operation 'CORTEX Maintenance' had 2 failed modules
  symptoms:
  - 'cleanup_orchestrator: Cleanup failed: print_minimalist() got an unexpected keyword
    argument ''dry_run'''
  - 'optimize_cortex_orchestrator: SKULL tests failed - cannot proceed with optimization'
  confidence: 0.8
  tags:
  - operation-failure
  - cortex-maintenance
- id: response-format-001
  title: CORTEX responses violating mandatory format structure
  category: quality-control
  subcategory: response-format
  severity: critical
  date: '2025-11-16'
  problem: 'AList.md conversation responses violated CORTEX.prompt.md mandatory format:
    missing "Your Request" echo section, using forbidden separator lines, showing
    verbose tool calls, improper Smart Hint placement, duplicate headers.'
  root_cause: Response format rules not consistently enforced during conversation
    generation
  symptoms:
  - Missing "üìù **Your Request:**" echo between Response and Next Steps
  - Horizontal separator lines (---) breaking in GitHub Copilot Chat
  - Smart Hint appearing before "Your Request" instead of after Next Steps
  - Verbose tool narration ("Read...", "Searched text for...")
  - File links showing as empty []() brackets
  - Duplicate headers (appearing at start and near end)
  - Over-enthusiastic intermediate comments ("Perfect!", "Excellent!")
  violations_found:
  - count: 3
    missing_request_echo: All 3 responses
    separator_lines: 2 instances in first response
    verbose_tools: 6+ tool calls shown explicitly
    wrong_smart_hint_placement: 1 instance
    duplicate_headers: 1 instance
    enthusiastic_comments: 3 instances
  correct_structure: |
    üß† **CORTEX [Operation Type]**
    Author: Asif Hussain | ¬© 2024-2025 | github.com/asifhussain60/CORTEX
    
    üéØ **My Understanding Of Your Request:** 
       [State understanding]
    
    ‚ö†Ô∏è **Challenge:** ‚úì **Accept** or ‚ö° **Challenge**
       [Reasoning]
    
    üí¨ **Response:** [Natural language explanation, tools execute silently]
    
    üìù **Your Request:** [Echo user request concisely]
    
    üîç Next Steps:
       1. [First recommendation]
       2. [Second recommendation]
  solution: Created quality checklist, documented violations in lessons-learned.yaml,
    preparing CORTEX.prompt.md updates with explicit "DON'T" examples
  prevention_rules:
  - ALWAYS include "üìù **Your Request:**" section between Response and Next Steps
  - NEVER use horizontal separator lines (---, ===, ___, any repeated chars)
  - Execute tools SILENTLY - explain WHAT was done, not HOW
  - Place Smart Hint AFTER Next Steps (optional, only if quality ‚â• GOOD)
  - Use header ONCE at start only
  - Maintain professional tone throughout (no "Perfect!", "Excellent!")
  - Show file paths in prose, not as empty []() links
  anti_patterns:
  - '‚ùå Response ‚Üí Next Steps (missing echo)'
  - '‚ùå Using --- or === separator lines'
  - '‚ùå "Read [](file://...)" tool narration'
  - '‚ùå Response ‚Üí Smart Hint ‚Üí Your Request (wrong order)'
  - '‚ùå Header at start AND near end (duplicate)'
  - '‚ùå "Perfect! Now let me..." (overly enthusiastic)'
  time_cost:
    analysis_minutes: 15
    documentation_minutes: 20
    future_validation_tool_minutes: 30
    total_minutes: 65
  related_files:
  - .github/prompts/CORTEX.prompt.md
  - .github/CopilotChats/AList.md
  - cortex-brain/response-templates.yaml
  tags:
  - response-format
  - quality-control
  - github-copilot-chat
  - mandatory-structure
  - documentation
  pattern_id: response_format_validation_pattern
  confidence: 0.98
- id: roadmap-status-001
  title: Planning document status fields do not reflect actual implementation
  category: project-management
  subcategory: status-tracking
  severity: high
  date: '2025-11-16'
  problem: 'CORTEX-3.0-ROADMAP.yaml showed "Track 2: NOT STARTED, 0% implementation"
    but Track B was actually 100% complete with 97.5% token reduction achieved. Git
    history showed completion commit (e14a4a5) with 824 files changed.'
  root_cause: Roadmap YAML is a planning document with manually updated status fields,
    not a live progress tracker
  symptoms:
  - Status fields showing NOT STARTED for completed work
  - Planning document and git history out of sync
  - Completion reports exist but roadmap not updated
  - User awareness of completion but agent reports 0%
  solution: Always validate planning documents against git history and completion
    reports before providing status updates
  prevention_rules:
  - Check git log for completion commits FIRST
  - Review final reports (TRACK-B-FINAL-REPORT.md)
  - Cross-reference work session summaries
  - Compare planning YAML status LAST (often outdated)
  - Source of truth order - 1. Git commits, 2. Test results, 3. Completion reports,
    4. Planning YAML
  validation_workflow:
  - 'Step 1: git log --oneline --graph --all -20 (check for "Complete" commits)'
  - 'Step 2: git log <commit> --stat (verify scope with file changes)'
  - 'Step 3: Search for *FINAL-REPORT*.md or *COMPLETE*.md files'
  - 'Step 4: Read work session summaries (*WORK-SESSION*.md)'
  - 'Step 5: Compare to roadmap YAML status fields'
  - 'Step 6: Note discrepancies and report actual progress'
  time_cost:
    incorrect_summary_minutes: 5
    user_correction_minutes: 2
    investigation_minutes: 10
    corrected_summary_minutes: 5
    total_minutes: 22
  related_files:
  - cortex-brain/cortex-3.0-design/CORTEX-3.0-ROADMAP.yaml
  - cortex-brain/documents/reports/CORTEX-3.0-TRACK-B-FINAL-REPORT.md
  - cortex-brain/documents/reports/WORK-SESSION-SUMMARY-2025-11-16.md
  tags:
  - status-tracking
  - git-history
  - planning
  - validation
  - project-management
  pattern_id: status_validation_pattern
  confidence: 0.98
patterns:
- pattern_id: status_validation_pattern
  name: Status Validation Against Git History
  confidence: 0.98
  applies_to:
  - project-management
  - status-reporting
  - roadmap-tracking
  - dual-track-development
  validation_sequence:
  - 'Step 1: Check git history for completion indicators'
  - 'Step 2: Examine commit details (scope, files changed)'
  - 'Step 3: Locate completion reports and summaries'
  - 'Step 4: Cross-reference multiple sources'
  - 'Step 5: Compare to planning document status'
  - 'Step 6: Report actual progress with discrepancies noted'
  benefits:
  - Prevents incorrect status reports
  - Validates planning docs against implementation
  - Discovers completed work not reflected in roadmaps
  - Establishes source-of-truth hierarchy
  - Documents git investigation methodology
  related_lessons:
  - roadmap-status-001
  git_commands:
    search_completions: git log --oneline --grep="Complete" --grep="feat"
    view_commit_details: git log <commit> --oneline -1 --stat
    graph_history: git log --oneline --graph --all -20
  source_of_truth_hierarchy:
  - '1. Git commit history (primary - what was actually done)'
  - '2. Test results (validation - what works)'
  - '3. Completion reports (documentation - comprehensive summaries)'
  - '4. Planning YAML status (metadata - often outdated)'
patterns:
- pattern_id: wpf_icon_validation_pattern
  name: Material Design Icon Validation
  confidence: 0.95
  applies_to:
  - wpf
  - xaml
  - material-design
  test_template: MaterialDesignIconTests.cs
  steps:
  - Extract all icon names from XAML (grep Kind=)
  - Create Theory test with InlineData for each icon
  - Use Enum.TryParse<PackIconKind> to validate
  - "Test fails \xE2\u2020\u2019 Look up valid icon names"
  - "Test passes \xE2\u2020\u2019 Safe to use in XAML"
  benefits:
  - Catches invalid icons at test-time
  - Documents all icons used in project
  - Prevents runtime crashes
  - Future-proof validation
  related_lessons:
  - wpf-tdd-001
  - wpf-icons-001
- pattern_id: wpf_smoke_test_pattern
  name: WPF Application Deployment Verification
  confidence: 0.92
  applies_to:
  - wpf
  - deployment
  - build
  test_template: ApplicationSmokeTests.cs
  tests:
  - Executable_ShouldExist
  - MainDll_ShouldExist
  - MaterialDesignThemes_DllShouldExist
  - RuntimeConfig_ShouldExist
  - ApplicationVersion_ShouldBeNet8
  purpose:
  - Verify deployment is complete
  - Catch missing dependencies before user runs app
  - Validate .NET runtime version
  when_to_use:
  - After every build
  - Before committing WPF changes
  - In CI/CD pipeline
  related_lessons:
  - wpf-silent-001
  - build-success-001
- pattern_id: cross_platform_path_pattern
  name: Dynamic Path Resolution
  confidence: 0.98
  applies_to:
  - bash
  - python
  - scripts
  - configuration
  implementations:
    bash: SCRIPT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    python: script_root = Path(__file__).parent
    powershell: $ScriptRoot = Split-Path -Parent $MyInvocation.MyCommand.Path
  prevention:
  - Never hard-code /Users/, /home/, C:\, D:\
  - Use relative paths from script location
  - Use environment variables for machine-specific config
  - Use config.py for centralized path management
  related_lessons:
  - cross-platform-001
- pattern_id: response_format_validation_pattern
  name: CORTEX Response Format Validation
  confidence: 0.98
  applies_to:
  - response-format
  - quality-control
  - github-copilot-chat
  - documentation
  mandatory_sections:
  - 'üß† **CORTEX [Operation Type]** + Author/Copyright (once at start)'
  - 'üéØ **My Understanding Of Your Request:**'
  - '‚ö†Ô∏è **Challenge:** ‚úì Accept or ‚ö° Challenge'
  - 'üí¨ **Response:** (natural language, tools silent)'
  - 'üìù **Your Request:** (echo user request)'
  - 'üîç Next Steps: (numbered or phase-based)'
  forbidden_elements:
  - Horizontal separator lines (---, ===, ___, any repeated chars)
  - Verbose tool narration (Read..., Searched text for...)
  - Empty file links []()
  - Duplicate headers
  - Over-enthusiastic comments (Perfect!, Excellent!)
  - Smart Hint before Next Steps (should be after or omitted)
  validation_checklist:
  - '‚úÖ Header present once at start'
  - '‚úÖ Understanding section before Challenge'
  - '‚úÖ Challenge includes Accept OR Challenge'
  - '‚úÖ Response explains what was done (not how)'
  - '‚úÖ "Your Request" echo present between Response and Next Steps'
  - '‚úÖ No separator lines used'
  - '‚úÖ Professional tone maintained throughout'
  - '‚úÖ Next Steps appropriate for work type (phases vs tasks)'
  correction_workflow:
  - 'Step 1: Check for "Your Request" section presence'
  - 'Step 2: Remove all separator lines'
  - 'Step 3: Simplify tool narration (or remove)'
  - 'Step 4: Move Smart Hint after Next Steps'
  - 'Step 5: Remove duplicate headers'
  - 'Step 6: Tone down enthusiastic comments'
  - 'Step 7: Verify section order matches template'
  benefits:
  - Consistent user experience across all CORTEX responses
  - Clean rendering in GitHub Copilot Chat
  - Professional, measured tone
  - Proper information hierarchy
  - Easier to scan and understand
  related_lessons:
  - response-format-001
statistics:
  total_lessons: 20
  by_category:
    testing: 3
    validation: 1
    debugging: 1
    infrastructure: 1
    portability: 1
    documentation: 1
    optimization: 1
    quality-control: 1
    architecture: 2
  by_severity:
    critical: 2
    high: 5
    medium: 4
    low: 0
  by_confidence:
    0.90-1.00: 11
    0.70-0.89: 0
    below_0.70: 0
  total_patterns: 7
  avg_confidence: 0.952
  maintenance:
    review_frequency: quarterly
    last_reviewed: '2025-11-16'
    next_review: '2026-02-09'
    archival_policy: Archive lessons older than 6 months with confidence < 0.80
    update_policy: Increment confidence when lesson prevents an issue

# Additional Lessons (Recent)
additional_lessons:
- id: prompt-routing-001
  title: GitHub Copilot Chat requires explicit trigger detection instructions
  category: architecture
  subcategory: prompt-engineering
  severity: critical
  date: '2025-11-17'
  problem: 'Planning workflow not activating when user said "let''s plan Azure DevOps
    enhancements". CORTEX went straight to execution mode, creating todos instead
    of engaging interactive planning template. Investigation revealed documentation
    described "autonomous intent router" but no such mechanism existed.'
  root_cause: GitHub Copilot reads prompts as context, not executable code. Without
    explicit "BEFORE responding, check triggers" instruction, LLM makes natural language
    decisions without checking response-templates.yaml
  symptoms:
  - Planning template exists but never loads
  - User says "let's plan" but execution happens instead
  - Documentation describes auto-routing that doesn't occur
  - Templates are passive data, not executable logic
  - Missing trigger keywords in routing configuration
  solution: 'Implemented 4-part fix: (1) Added planning_triggers to response-templates.yaml,
    (2) Added explicit "BEFORE responding" instruction to CORTEX.prompt.md, (3) Added
    activation triggers section to help_plan_feature.md, (4) Updated condensed templates
    for consistency'
  architecture_insight: 'CORTEX''s "intent router" is not autonomous middleware -
    it''s prompt-instructed behavior. GitHub Copilot Chat has no separate routing
    layer. All routing happens through explicit instructions: "Check triggers FIRST,
    then respond"'
  prevention_rules:
  - Add explicit "BEFORE responding" instructions for all workflow triggers
  - Never assume LLM will automatically detect and route keywords
  - Prompts are context, not executable middleware
  - Document trigger detection mechanism explicitly
  - Test trigger activation before marking feature complete
  trigger_detection_pattern: |
    # CRITICAL: Template Trigger Detection
    
    BEFORE responding to ANY user request:
    1. Check for template triggers in response-templates.yaml
    2. If matched: Load corresponding template/module
    3. If no match: Proceed with natural language response
    
    Examples:
    - "let's plan" ‚Üí MATCH planning_triggers ‚Üí Load help_plan_feature.md
    - "help" ‚Üí MATCH help_triggers ‚Üí Return help_table template
    - "add button" ‚Üí NO MATCH ‚Üí Natural language execution
  files_modified:
  - .github/prompts/CORTEX.prompt.md
  - cortex-brain/response-templates.yaml
  - cortex-brain/response-templates-condensed.yaml
  - prompts/shared/help_plan_feature.md
  time_cost:
    investigation_minutes: 45
    design_minutes: 30
    implementation_minutes: 25
    testing_minutes: 15
    total_minutes: 115
  related_files:
  - .github/prompts/CORTEX.prompt.md
  - cortex-brain/response-templates.yaml
  - prompts/shared/help_plan_feature.md
  - cortex-brain/documents/investigations/PLANNING-TRIGGER-INVESTIGATION.md
  - cortex-brain/documents/reports/PLANNING-TRIGGER-IMPLEMENTATION-COMPLETE.md
  - cortex-brain/documents/conversation-captures/2025-11-17-planning-trigger-implementation.md
  tags:
  - prompt-engineering
  - github-copilot-chat
  - trigger-detection
  - architecture
  - intent-routing
  - workflow-activation
  pattern_id: prompt_based_trigger_detection_pattern
  confidence: 0.98
- id: trigger-strategy-001
  title: Context detection prevents trigger explosion
  category: architecture
  subcategory: trigger-design
  severity: medium
  date: '2025-11-17'
  problem: 'Considered creating separate triggers for each domain (ADO planning,
    AWS planning, K8s planning). Would have led to trigger explosion: 8 planning
    triggers √ó 10 domains = 80 triggers to maintain.'
  root_cause: Initial impulse to create explicit triggers for every variation instead
    of using context detection within workflows
  symptoms:
  - Temptation to create domain-specific triggers
  - Difficulty maintaining large trigger arrays
  - Redundant trigger definitions
  - Complex routing logic
  solution: 'Use single trigger array ("planning") with context detection inside
    workflow. Same "let''s plan" trigger activates for all domains. Questions adapt
    based on detected context (ADO, AWS, K8s keywords).'
  architecture_decision: 'TIER-based trigger strategy: TIER 1 (need triggers now):
    planning, help, status. TIER 2 (beneficial later): setup, docs, maintenance.
    TIER 3 (never need): code execution, questions (natural language handles these).'
  prevention_rules:
  - Create triggers for structured workflows with consistent patterns only
  - Use context detection within workflows, not separate triggers
  - Maximum 8-12 trigger arrays total (maintainability limit)
  - Let natural language handle variations and domain specifics
  - Start with TIER 1, add TIER 2 only after validation
  context_detection_pattern: |
    Trigger Match: "let's plan" ‚Üí planning_triggers
    Context Detection: "ADO" / "Azure DevOps" in request
    Flow:
      1. Same planning workflow activates
      2. Confidence assessment includes domain context
      3. Questions tailored to detected domain
      4. Phase structure adapted for domain needs
  benefits:
  - Maintainability (8-12 triggers vs 50+)
  - Flexibility (new domains don't need new triggers)
  - Natural language understanding handles variations
  - Context detection more powerful than trigger matching
  time_cost:
    design_discussion_minutes: 25
    documentation_minutes: 15
    total_minutes: 40
  related_files:
  - cortex-brain/response-templates.yaml
  - cortex-brain/documents/conversation-captures/2025-11-17-planning-trigger-implementation.md
  tags:
  - trigger-strategy
  - context-detection
  - maintainability
  - architecture
  - scalability
  pattern_id: context_over_triggers_pattern
  confidence: 0.95
patterns:
- pattern_id: prompt_based_trigger_detection_pattern
  name: Prompt-Based Trigger Detection for LLM Routing
  confidence: 0.98
  applies_to:
  - github-copilot-chat
  - prompt-engineering
  - workflow-activation
  - intent-routing
  problem: LLMs don't automatically detect and route keywords without explicit instructions
  solution_template: |
    # CRITICAL: Template Trigger Detection
    
    BEFORE responding to ANY user request:
    1. Check for template triggers in [config-file]
    2. If matched: Load corresponding [template/module]
    3. If no match: Proceed with natural language response
  implementation_steps:
  - 'Step 1: Define trigger keywords in YAML (planning_triggers, help_triggers, etc.)'
  - 'Step 2: Add "BEFORE responding" instruction to prompt file'
  - 'Step 3: Document triggers in feature module (activation section)'
  - 'Step 4: Update condensed versions for token efficiency'
  - 'Step 5: Test trigger activation with original request'
  key_insights:
  - GitHub Copilot reads prompts as context, not executable code
  - No middleware layer intercepts requests in Copilot Chat
  - Templates are passive data until explicitly loaded
  - Routing must be taught through explicit procedural instructions
  - '''Intent router'' is conceptual - actually prompt-instructed behavior'
  benefits:
  - Consistent workflow activation across sessions
  - Predictable routing behavior
  - No external infrastructure required
  - Works within GitHub Copilot Chat constraints
  - Maintains CORTEX local-first principle
  related_lessons:
  - prompt-routing-001
- pattern_id: context_over_triggers_pattern
  name: Context Detection Over Trigger Explosion
  confidence: 0.95
  applies_to:
  - trigger-strategy
  - scalability
  - maintainability
  - workflow-design
  problem: Creating separate triggers for every domain variation leads to unmaintainable
    trigger explosion
  solution_approach: Single trigger array with context detection inside workflow
  tier_based_strategy:
    tier_1_need_triggers:
    - Planning (interactive, structured workflow)
    - Help/Status (pre-formatted responses)
    tier_2_beneficial_later:
    - Setup/Configuration
    - Documentation operations
    - Maintenance
    tier_3_never_need:
    - Code execution (context-dependent, too many variations)
    - Questions (direct responses work fine)
  context_detection_flow: |
    User: "let's plan an ADO feature"
    ‚Üí Matches: planning_triggers
    ‚Üí Detects: "ADO" keyword in request
    ‚Üí Adapts: Questions specific to Azure DevOps
    ‚Üí Generates: ADO-specialized phase plan
  benefits:
  - Maintainability (8-12 triggers vs 50+ with domain explosion)
  - Flexibility (new domains auto-detected, no new triggers needed)
  - Natural language handles variations better than explicit triggers
  - Scalable without configuration bloat
  decision_criteria:
  - Structured workflow with consistent pattern? ‚Üí Add trigger
  - Many domain variations? ‚Üí Use context detection
  - Simple direct response? ‚Üí Natural language only
  - High value from consistency? ‚Üí Add trigger
  related_lessons:
  - trigger-strategy-001
