# GitHub Copilot Chat Integration - Automatic Screenshot Analysis

**Purpose:** Enable automatic Vision API engagement when screenshots are attached to Copilot Chat conversations.

**Version:** 1.0  
**Status:** ‚úÖ PRODUCTION READY  
**Last Updated:** 2025-11-09

---

## üöÄ How It Works

When you attach a screenshot in GitHub Copilot Chat, CORTEX automatically:

1. ‚úÖ **Detects image attachment** via IntentRouter
2. ‚úÖ **Routes to ScreenshotAnalyzer** agent
3. ‚úÖ **Engages Vision API** for image processing
4. ‚úÖ **Returns structured analysis** with UI elements, colors, layout

**No explicit command needed** - just attach the screenshot!

---

## üì∏ Usage Examples

### Example 1: Automatic Analysis

```
[Attach screenshot of login page]

User: "What elements are in this UI?"

CORTEX automatically:
- Detects image attachment
- Routes to Vision API
- Analyzes UI elements
- Returns: email input, password input, login button
```

### Example 2: Specific Query

```
[Attach screenshot of dashboard]

User: "Extract all button colors from this screenshot"

CORTEX:
- Detects image + specific request
- Focuses Vision API on color extraction
- Returns: #3B82F6 (primary), #10B981 (success), #EF4444 (danger)
```

### Example 3: Test ID Generation

```
[Attach screenshot of form]

User: "Generate test IDs for all inputs"

CORTEX:
- Analyzes form fields
- Suggests data-testid attributes
- Returns: input-email, input-password, input-remember, btn-submit
```

---

## üîß How to Trigger Automatic Analysis

### Method 1: Implicit (Recommended)

Just attach a screenshot and ask a question. CORTEX detects it automatically:

```
[Attach screenshot]
"What's in this image?"
```

### Method 2: Explicit Keywords

Use trigger words for specific analysis types:

- **"analyze this screenshot"** ‚Üí Full UI analysis
- **"what colors"** ‚Üí Color extraction focus
- **"test IDs"** ‚Üí Generate test selectors
- **"UI elements"** ‚Üí Element identification
- **"layout structure"** ‚Üí Layout analysis

### Method 3: Natural Language

CORTEX understands natural requests:

```
"Find all buttons in this screenshot"
"What font sizes are used here?"
"Is this design accessible?"
"Extract the color palette"
```

---

## ‚öôÔ∏è Technical Implementation

### Configuration

Vision API enabled in `cortex.config.json`:

```json
{
  "vision_api": {
    "enabled": true,
    "max_tokens_per_image": 500,
    "warn_threshold_tokens": 400,
    "max_image_size_bytes": 2000000,
    "downscale_threshold": 1920,
    "jpeg_quality": 85,
    "cache_analysis_results": true,
    "cache_ttl_hours": 24
  }
}
```

### Intent Detection

`IntentRouter._classify_intent()` automatically detects images:

```python
# Check if there's an image attachment in context
if request.context:
    has_image = (
        'image_base64' in request.context or
        'image_path' in request.context or
        'image_data' in request.context or
        'screenshot' in request.context or
        any(k.startswith('image') for k in request.context.keys())
    )
    if has_image:
        return IntentType.SCREENSHOT
```

### Routing Flow

```
User attaches screenshot
    ‚Üì
GitHub Copilot Chat creates context with image data
    ‚Üì
CORTEX IntentRouter detects image in context
    ‚Üì
Routes to IntentType.SCREENSHOT
    ‚Üì
ScreenshotAnalyzer agent receives request
    ‚Üì
Vision API processes image
    ‚Üì
Returns structured analysis
```

---

## üìä Vision API Features

### Image Preprocessing

- **Auto-downscale:** Images > 1920px width resized automatically
- **Compression:** JPEG quality 85% to reduce token cost
- **Format conversion:** PNG/WebP ‚Üí JPEG for efficiency
- **Size validation:** Max 2MB enforced

### Token Management

- **Budget enforcement:** 500 token hard limit per image
- **Warning threshold:** Alert at 400 tokens
- **Estimation:** Pre-calculates token cost before API call
- **Cost tracking:** Logs token usage for optimization

### Caching

- **Result caching:** 24-hour TTL on analysis results
- **Hash-based:** Same image + prompt = cache hit
- **Memory efficient:** 100 result limit with LRU eviction
- **Zero cost:** Cached results use 0 tokens

---

## üéØ Analysis Capabilities

### UI Element Identification

```json
{
  "elements": [
    {
      "type": "button",
      "label": "Login",
      "suggested_id": "btn-login",
      "selector": "button[type='submit']",
      "position": {"x": 100, "y": 250}
    },
    {
      "type": "input",
      "label": "Email",
      "suggested_id": "input-email",
      "selector": "input[type='email']",
      "position": {"x": 100, "y": 150}
    }
  ],
  "element_count": 2
}
```

### Color Extraction

```json
{
  "colors": {
    "primary": "#3B82F6",
    "text": "#FFFFFF",
    "border": "#2563EB"
  },
  "contrast_ratio": 4.8,
  "wcag_compliant": true
}
```

### Layout Analysis

```json
{
  "layout": {
    "type": "3-column",
    "columns": [
      {"name": "sidebar", "width": "240px"},
      {"name": "main", "width": "flexible"},
      {"name": "activity", "width": "320px"}
    ]
  },
  "components": ["header", "cards", "grid"]
}
```

---

## üîç Example Workflows

### Workflow 1: Design Review

```
1. Designer shares screenshot in Copilot Chat
2. Ask: "Is this accessible?"
3. CORTEX analyzes:
   - Color contrast ratios
   - Font sizes
   - WCAG compliance
   - ARIA label presence
4. Returns accessibility report
```

### Workflow 2: Test Automation

```
1. Attach screenshot of feature
2. Ask: "Generate Playwright selectors"
3. CORTEX identifies:
   - All interactive elements
   - Suggests data-testid attributes
   - Provides Playwright selectors
4. Copy selectors into test file
```

### Workflow 3: UI Documentation

```
1. Attach screenshot of component
2. Ask: "Document this component"
3. CORTEX extracts:
   - Element hierarchy
   - Props/attributes
   - Style specifications
4. Generates component documentation
```

---

## ‚ö†Ô∏è Current Limitations

### GitHub Copilot Chat Integration

**Note:** GitHub Copilot Chat doesn't automatically pass image data to Python scripts. The Vision API infrastructure is ready, but requires explicit prompting.

**Workaround:** When you attach a screenshot, explicitly mention it:
- "Analyze this screenshot"
- "What's in this image?"
- "Extract colors from this"

### Future Enhancement

We're working on deeper GitHub Copilot extension integration to make image detection fully automatic without explicit mention.

---

## üöÄ Quick Start

### 1. Verify Configuration

Check `cortex.config.json`:

```json
{
  "vision_api": {
    "enabled": true
  }
}
```

### 2. Test Vision API

Run integration test:

```bash
python test_vision_integration.py
```

Expected output:
```
‚úÖ PASS: Config Check
‚úÖ PASS: Intent Detection
‚úÖ PASS: Screenshot Analyzer
‚úÖ PASS: Vision API Integration

üéâ All tests passed!
```

### 3. Try It Out

In GitHub Copilot Chat:

```
[Attach screenshot]
"What UI elements are in this screenshot?"
```

---

## üìù Vision API Metrics

Track usage with:

```python
from src.tier1.vision_api import VisionAPI
import json

config = json.load(open('cortex.config.json'))
vision = VisionAPI(config)

# After some usage
metrics = vision.get_metrics()
print(metrics)
```

Output:
```json
{
  "total_requests": 15,
  "total_tokens_used": 3420,
  "average_tokens_per_request": 228.0,
  "cache_hits": 3,
  "cache_hit_rate_percent": 16.7,
  "estimated_cost_usd": 0.1026,
  "cache_size": 12
}
```

---

## üõ†Ô∏è Troubleshooting

### Vision API Not Engaging

**Symptom:** Screenshot attached but not analyzed

**Solutions:**
1. ‚úÖ Check `vision_api.enabled = true` in config
2. ‚úÖ Explicitly mention the screenshot ("analyze this")
3. ‚úÖ Verify image format (PNG, JPEG, WebP supported)
4. ‚úÖ Check image size (< 2MB)

### Low Quality Analysis

**Symptom:** Vision API returns generic results

**Solutions:**
1. ‚úÖ Use higher resolution screenshots
2. ‚úÖ Be specific in your request
3. ‚úÖ Ensure good contrast in the image
4. ‚úÖ Avoid blurry or low-quality captures

### Token Budget Exceeded

**Symptom:** Error "Image too large: estimated X tokens"

**Solutions:**
1. ‚úÖ Downscale image before attaching
2. ‚úÖ Crop to relevant area only
3. ‚úÖ Increase `max_tokens_per_image` in config (carefully)
4. ‚úÖ Use lower resolution screenshots

---

## üìö Related Documentation

- **Vision API Implementation:** `src/tier1/vision_api.py`
- **Screenshot Analyzer:** `src/cortex_agents/screenshot_analyzer.py`
- **Intent Router:** `src/cortex_agents/intent_router.py`
- **Configuration:** `cortex.config.json`
- **Tests:** `tests/tier1/test_vision_api.py`

---

## üéØ Roadmap

### Phase 1: ‚úÖ Complete
- Vision API core implementation
- Intent detection for images
- ScreenshotAnalyzer agent
- Configuration management

### Phase 2: üü° In Progress
- GitHub Copilot extension integration
- Automatic attachment detection
- Enhanced element recognition

### Phase 3: ‚è∏Ô∏è Planned
- OCR text extraction
- Responsive design analysis
- A/B test comparison
- Accessibility scoring

---

*Last Updated: 2025-11-09 | CORTEX 2.0 Vision Integration*
