asifhussain60: Follow instructions in [CORTEX.prompt.md](file:///d%3A/PROJECTS/CORTEX/.github/prompts/CORTEX.prompt.md).
Review #file:Chat001 holistically and come up with a cohesive plan of how a user should begin planning, once the plan is complete and approved by user, there should be a thoroguh analysis phase with crawlers buildign knowledge graphs etc. The analysis should lead the development effort. The analysis should be presented to the user in a concise clear manner for approval before execution.

GitHub Copilot: I'll analyze the conversation history and create a cohesive planning workflow that integrates CORTEX's planning, analysis, and execution capabilities. 

Read [](file:///d%3A/PROJECTS/CORTEX/.github/CopilotChats/Chat001)

üß† **CORTEX Workflow Architecture Design**
Author: Asif Hussain | ¬© 2024-2025 | github.com/asifhussain60/CORTEX

üéØ **My Understanding Of Your Request:**
   You want me to synthesize the Chat001 conversation into a cohesive planning workflow that follows this pattern:
   1. User initiates planning (triggered by natural language)
   2. CORTEX facilitates structured planning with user collaboration
   3. Once plan is approved ‚Üí Analysis phase begins (crawlers, knowledge graphs)
   4. Analysis results presented to user for approval
   5. After analysis approval ‚Üí Execution begins

‚ö†Ô∏è **Challenge:** ‚úì **Accept**
   This is the correct architectural pattern. The conversation reveals two key insights:
   
   **Insight 1:** "Focus" intent is really an analysis/context-gathering phase that should happen AFTER planning, BEFORE execution
   
   **Insight 2:** Analysis should be mandatory (not optional) to ensure quality and build proper working knowledge

üí¨ **Response:**

Based on the Chat001 conversation, here's the cohesive workflow architecture:

## üéØ CORTEX Planning-Analysis-Execution Pipeline

### **Phase 1: Planning (User-Collaborative)**

**Triggers:** "plan [feature]", "let's plan", "plan a feature"

**What Happens:**
1. Intent Router detects PLAN intent
2. Work Planner agent activates
3. Creates planning file: `cortex-brain/documents/planning/features/PLAN-[date]-[feature].md`
4. **Interactive session with DoR enforcement (Zero Ambiguity Required):**
   - **Clarifying questions** - CORTEX MUST NOT proceed until ALL ambiguities resolved
     - What is the exact feature scope? (No "probably" or "maybe")
     - Who are the users? (Specific roles, not "people")
     - What are the integration points? (Exact APIs, services, databases)
     - What are the constraints? (Performance, security, compliance)
     - What success metrics? (Measurable, specific)
   - **Definition of Ready (DoR) Validation** - MUST be met before phase breakdown:
     ‚úì Requirements documented with zero ambiguity
     ‚úì Dependencies identified and validated (files exist, services accessible)
     ‚úì Technical design approach agreed upon
     ‚úì Test strategy defined
     ‚úì Acceptance criteria measurable and specific
     ‚úì User approval on scope and approach
   - **Phase breakdown** (Phase 1: Foundation ‚Üí Phase 2: Core ‚Üí Phase 3: Validation)
   - **Risk analysis** - Based on validated context
   - **Task generation** - Specific, actionable tasks
   - **Acceptance criteria definition** - Must be measurable and testable
5. **DoR Gate Check:** CORTEX validates DoR completion before allowing "approve plan"
6. User reviews and refines planning document
7. User approves: "approve plan" (only allowed after DoR met)

**Output:** Approved planning document with DoR validation checklist completed

**Key Enforcement Rules:**
- ‚ùå CORTEX MUST NOT proceed to analysis phase if DoR incomplete
- ‚ùå CORTEX MUST NOT assume referenced files/services exist without verification
- ‚ùå CORTEX MUST NOT accept vague requirements ("improve performance" ‚Üí "reduce API response time to <200ms")
- ‚úÖ CORTEX MUST ask clarifying questions until zero ambiguity achieved
- ‚úÖ CORTEX MUST validate all assumptions before marking DoR complete

---

### **Phase 2: Analysis (Automatic Context Gathering)**

**Triggers:** Automatically after "approve plan" OR manual "analyze [feature]"

**What Happens:**
1. **Crawler Orchestration** (runs in background):
   ```
   ‚òê File Discovery: Scan codebase for related files
   ‚òê Dependency Analysis: Map imports/usages
   ‚òê Database Schema: Extract relevant tables/relationships
   ‚òê API Endpoints: Identify integration points
   ‚òê Test Coverage: Find existing tests
   ‚òê Documentation: Locate related docs
   ```

2. **Knowledge Graph Building**:
   - File relationships (co-modification patterns)
   - Component dependencies
   - Data flow patterns
   - Integration points
   - Test coverage gaps

3. **Analysis Report Generation**:
   ```markdown
   üìä Analysis Report: Authentication Feature
   
   üîç Discovered Assets:
   ‚Ä¢ 12 related files (AuthService.cs, LoginController.cs, ...)
   ‚Ä¢ 3 database tables (Users, Sessions, Tokens)
   ‚Ä¢ 5 API endpoints (/login, /logout, /refresh, ...)
   ‚Ä¢ 8 existing tests (67% coverage)
   ‚Ä¢ 2 integration points (Email service, Audit log)
   
   üìà Architecture Diagram: [Mermaid graph generated]
   
   üéØ Recommendations:
   ‚úì Start with Phase 1: Foundation (low risk)
   ‚ö†Ô∏è Phase 2 requires database migration (medium risk)
   ‚úì Existing session management can be reused
   
   üß™ Test Strategy:
   ‚Ä¢ Add 12 new tests for authentication flows
   ‚Ä¢ Integration tests for email verification
   ‚Ä¢ Security tests for token handling
   ```

4. **User Review & Approval**:
   - Analysis report displayed in chat (summary)
   - Full report saved: `cortex-brain/discovery-reports/ANALYSIS-[date]-[feature].md`
   - User reviews: "looks good" or "refine [aspect]"
   - User approves: "start implementation" or "begin phase 1"

**Output:** Approved analysis with discovered context and architecture understanding

---

### **Phase 3: Execution (Test-Driven Implementation)**

**Triggers:** "start implementation", "begin phase 1", or natural request like "implement authentication"

**What Happens:**
1. **Context Injection** (automatic):
   - Analysis report loaded into Tier 1 memory
   - Related files opened in context
   - Knowledge graph patterns applied
   - Test strategy activated

2. **TDD Workflow** (Code Executor + Test Generator):
   ```
   Phase 1: Foundation
   ‚òê Task 1.1: Create User model
      ‚Üí Test Generator: Write failing tests (RED)
      ‚Üí Code Executor: Implement until tests pass (GREEN)
      ‚Üí Code Executor: Refactor for quality (REFACTOR)
   ‚òê Task 1.2: Create AuthService
      ‚Üí [Same TDD cycle]
   ```

3. **Progress Tracking**:
   - Planning document updated with checkboxes: ‚úì Task 1.1 complete
   - Tier 1 memory: Conversation tracking
   - Tier 2 knowledge: Pattern learning
   - Tier 3 context: File metrics updated

4. **Health Validation** (after each phase):
   - All tests passing? ‚úì
   - Zero errors/warnings? ‚úì
   - DoD criteria met? ‚úì
   - If validation fails ‚Üí Error Corrector activates

**Output:** Working implementation with tests, documentation, and quality validation

---

## üîÑ Complete User Flow Example

**User Journey: "I want to add authentication to my app"**

### **Step 1: Initiate Planning**
```
User: "plan authentication feature"

CORTEX:
‚úÖ Created planning file (opened in VS Code)
üí¨ Let's break this down together...

[Interactive planning session - 15 minutes]
- What authentication methods? (Email/password, OAuth, etc.)
- Which user types need access?
- Session management requirements?
- Security constraints?

‚úÖ Generated 4-phase plan with 16 tasks
üìÑ Review: cortex-brain/documents/planning/features/PLAN-2025-11-19-authentication.md
```

**User reviews plan, makes adjustments**

```
User: "approve plan"
```

---

### **Step 2: Analysis Phase (Automatic)**
```
CORTEX:
üîç Starting analysis phase...

Crawling codebase:
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% (47 files scanned)

Building knowledge graph:
‚úÖ File relationships mapped (23 connections)
‚úÖ Database schema extracted (3 tables)
‚úÖ API endpoints identified (5 endpoints)
‚úÖ Test coverage analyzed (67% current)
‚úÖ Integration points discovered (2 services)

üìä Analysis complete! Generated report.

[Shows summary in chat]
[Full report saved to discovery-reports/]

‚ö†Ô∏è Key Findings:
‚Ä¢ Existing session management can be reused
‚Ä¢ Database migration needed for Users table
‚Ä¢ Email service integration required

Ready for your review.
```

**User reviews analysis report**

```
User: "looks good, let's start"
```

---

### **Step 3: Execution Phase**
```
CORTEX:
‚úÖ Analysis approved
üöÄ Starting Phase 1: Foundation

Context loaded:
‚Ä¢ 12 related files in working memory
‚Ä¢ 3 database tables mapped
‚Ä¢ 5 API endpoints identified

Task 1.1: Create User model
   üî¥ Writing failing tests... (RED)
   [Test code generated]
   ‚úÖ Tests fail as expected
   
   üü¢ Implementing User model... (GREEN)
   [Code generated]
   ‚úÖ All tests passing
   
   üîµ Refactoring... (REFACTOR)
   [Code cleaned up]
   ‚úÖ Quality validated

‚úì Task 1.1 complete (3 of 16 tasks done)

Continue with Task 1.2: Create AuthService?
```

---

## üéØ Key Benefits of This Workflow

**1. No Blind Execution** - Analysis phase ensures CORTEX understands the codebase before making changes

**2. User Control** - Approval gates at planning and analysis prevent unwanted automated changes

**3. Quality Built-In** - TDD + validation + DoD enforcement at every step

**4. Context-Rich** - Crawlers build deep understanding, not surface-level implementation

**5. Resumable** - Planning files persist, analysis reports saved, progress tracked

**6. Learning** - Patterns extracted to Tier 2 for future similar features

---

## üìã Response Templates Needed

**Template 1: Planning Started**
```yaml
trigger: ["plan", "let's plan", "plan a feature"]
response: |
  ‚úÖ Created planning file
  üí¨ Starting interactive planning session...
  [Questions for user]
```

**Template 2: Analysis Started** (NEW)
```yaml
trigger: ["approve plan", "analyze [feature]"]
response: |
  üîç Starting analysis phase...
  Crawling codebase: [progress bar]
  Building knowledge graph...
  [Analysis summary when complete]
```

**Template 3: Analysis Report** (NEW)
```yaml
trigger: [internal - after analysis complete]
response: |
  üìä Analysis Report: [feature name]
  
  üîç Discovered Assets: [summary]
  üìà Architecture: [diagram]
  üéØ Recommendations: [insights]
  üß™ Test Strategy: [approach]
  
  Review full report: [file path]
  Ready to proceed?
```

**Template 4: Execution Started**
```yaml
trigger: ["start implementation", "begin phase 1"]
response: |
  ‚úÖ Analysis approved
  üöÄ Starting [phase name]
  
  Context loaded: [summary]
  Task [x.y]: [task name]
  [TDD cycle progress]
```

üìù **Your Request:** Design cohesive planning-analysis-execution workflow based on Chat001 insights

üîç **Next Steps:**
   
   **Implementation Roadmap:**
   
   ‚òê Phase 1: Response Template Updates (1-2 hours)
      ‚Ä¢ Create "analysis_started" template
      ‚Ä¢ Create "analysis_report" template
      ‚Ä¢ Update "planning_complete" to trigger analysis
   
   ‚òê Phase 2: Analysis Orchestrator (3-4 hours)
      ‚Ä¢ Build crawler integration layer
      ‚Ä¢ Implement knowledge graph builder
      ‚Ä¢ Create analysis report generator
      ‚Ä¢ Add Mermaid diagram generation
   
   ‚òê Phase 3: Context Injection (2-3 hours)
      ‚Ä¢ Load analysis into Tier 1 memory
      ‚Ä¢ Integrate with Code Executor
      ‚Ä¢ Add context summary display
   
   ‚òê Phase 4: Approval Gates (1-2 hours)
      ‚Ä¢ Add "approve analysis" command
      ‚Ä¢ Block execution until approval
      ‚Ä¢ Track approval status in planning doc
   
   ‚òê Phase 5: Testing & Validation (2-3 hours)
      ‚Ä¢ Test complete workflow end-to-end
      ‚Ä¢ Validate crawler performance
      ‚Ä¢ Ensure proper error handling
   
   **Total Estimated Time:** 9-14 hours
   
   Ready to proceed with implementation, or refine the design first?