# CORTEX Knowledge Graph
# Validation insights, patterns, and strategic knowledge for CORTEX operations
# 
# Version: 2.1
# Type: Knowledge Graph
# Purpose: Capture patterns and learnings from CORTEX operations

version: "2.1"
last_updated: "2025-11-16"
type: "knowledge_graph"

# Summary of learned patterns across all categories
patterns:
  total_count: 47
  categories: ["validation_insights", "workflow_patterns", "architectural_patterns", "intent_patterns", "file_relationships", "common_mistakes", "quality_gates"]
  learning_method: "automatic extraction from Tier 1 conversations"
  confidence_decay: "5% per 90 days unused"

validation_insights:
  github_copilot_conversation_history_loop:
    issue: GitHub Copilot enters "Summarizing Conversation History..." loop when conversation contains large MD files
    root_cause: Large markdown documents (500-1500+ lines) in conversation history cause token bloat
    symptom: "Copilot spends 10-30 seconds 'summarizing' before each response, significantly slowing development"
    incident_date: '2025-11-15'
    impact: "Critical productivity impact - 10-30 second delay per interaction adds 30-60 minutes per dev session"
    solution: Migrate structured documents from MD to YAML format
    rationale:
      - "YAML is 60% more token-efficient than Markdown for structured data (proven in CORTEX Phase 0)"
      - "Machine-readable format reduces parsing overhead"
      - "Copilot processes YAML faster than narrative MD"
      - "Structured data belongs in structured format, not prose"
    migration_strategy:
      phase_1: "Identify conversion candidates (reports, status, analysis, planning docs)"
      phase_2: "Preserve narrative MD (story.md, guides, user-facing docs)"
      phase_3: "Convert structured docs to YAML with JSON Schema validation"
      phase_4: "Git-based deletion - commit YAML, then commit deletion of MD (no archiving)"
      phase_5: "Update CORTEX components to read YAML instead of MD"
    document_categories:
      convert_to_yaml:
        - "Implementation reports (*-COMPLETE.md, *-REPORT.md)"
        - "Status tracking (*-STATUS.md, *-PROGRESS.md)"
        - "Analysis documents (*-ANALYSIS.md)"
        - "Planning documents (*-PLANNING.md, *-PLAN.md)"
        - "Phase tracking documents"
        - "Metrics and statistics"
      keep_as_markdown:
        - "Story/narrative content (story.md, awakening-of-cortex.md)"
        - "User guides (setup-guide.md, help_plan_feature.md)"
        - "README.md files"
        - "Tutorial content"
        - "Architecture explanations for humans"
    git_deletion_strategy:
      rationale: "Git history is professional backup - no redundant archives needed"
      step_1: "Convert MD to YAML, validate with JSON Schema"
      step_2: "Git commit YAML file: 'feat(migration): Convert {filename} to YAML format'"
      step_3: "Git commit deletion: 'chore(migration): Delete {filename} after YAML conversion (backed up in commit {hash})'"
      rollback: "Use 'git revert {commit_hash}' to restore deleted files from history"
      audit_trail: "Git log serves as complete audit trail for all conversions"
    expected_benefits:
      token_reduction: "60% fewer tokens for structured documents"
      performance: "Eliminate 10-30 second summarization delays"
      maintainability: "Programmatic updates easier with YAML"
      validation: "JSON Schema prevents malformed data"
      diffs: "YAML diffs cleaner than MD diffs for structured data"
    implementation:
      design_document: "cortex-brain/documents/planning/YAML-PHASE-TRACKER-DESIGN.yaml"
      conversion_tool: "scripts/document_migration/convert_md_to_yaml.py"
      plugin: "src/plugins/phase_tracker_plugin.py"
      schemas: "cortex-brain/schemas/*.json"
    frequency: 1
    confidence: 1.0
    severity: critical
    last_seen: '2025-11-15'
    lesson: "Structured data in conversation history should be YAML, not MD - prose is for humans, structure is for machines"
  
  skull_protection_layer:
    issue: Development violations - fixes claimed without tests, integrations without
      E2E validation
    incident_date: '2025-11-09'
    trigger: CSS fixes applied 3x without tests, Vision API 'integrated' without actual
      call chain verification
    solution: SKULL Protection Layer (Safety, Knowledge, Validation & Learning)
    implementation:
      file: src/tier0/skull_protector.py
      tests: tests/tier0/test_skull_protector.py
      brain_rules: cortex-brain/brain-protection-rules.yaml (Layer 5)
      documentation: cortex-brain/SKULL-PROTECTION-LAYER.md
    rules:
      skull_001:
        name: Test Before Claim
        severity: BLOCKING
        prevents: "Claiming 'Fixed \u2705' without running automated tests"
        example: "Fixed \u2705 (Verified by: test_button_color_is_dark)"
      skull_002:
        name: Integration Verification
        severity: BLOCKING
        prevents: Integration claims without end-to-end tests
        example: "Integrated \u2705 (Verified by: test_vision_api_auto_engagement_end_to_end)"
      skull_003:
        name: Visual Regression
        severity: WARNING
        prevents: CSS/UI changes without visual validation or computed style checks
        example: 'CSS updated (Verified by: test_computed_style_is_dark)'
      skull_004:
        name: Retry Without Learning
        severity: WARNING
        prevents: Retrying same fix 3x without diagnosing root cause
        example: 'Retry after diagnosis: Browser cache not cleared'
    test_results:
      skull_tests: 20/20 PASSED
      vision_api_tests: 6/7 PASSED
      incident_prevention: "test_prevents_css_incident_november_9th \u2705, test_prevents_vision_api_incident_november_9th\
        \ \u2705"
    key_victory: Vision API now actually called when ScreenshotAnalyzer processes
      images (was falling back to mock)
    frequency: 1
    confidence: 1.0
    impact: critical
    last_seen: '2025-11-09'
    motto: The SKULL protects the brain. Test before you claim.
  kds_quadrant_completeness:
    issue: Documentation missing one or more KDS Quadrant components
    check_for:
    - "Story section (\U0001F9DA narrative, metaphors, day-in-life examples)"
    - Technical section (specifications, commands, parameters, code)
    - Image prompt section (diagrams, visual representations, flowcharts)
    - High-level technical section (architecture, workflows, integration)
    explanation: All four perspectives needed for complete understanding
    frequency: 0
    confidence: 1.0
    impact: medium
    last_seen: '2025-11-04'
  powershell_regex_escaping:
    issue: Backtick escaping for quotes in regex patterns fails to parse
    incorrect_pattern: '[`''\"\"](.*)[`''\"\"'']'
    correct_pattern: '[\x27\x22](.*)[ \x27\x22]'
    explanation: Use hex escape sequences (\x27 for single quote, \x22 for double
      quote) instead of backtick escaping
    frequency: 5
    confidence: 0.95
    impact: high
    last_seen: '2025-11-04T10:00:00Z'
  powershell_path_handling:
    issue: Hardcoded KDS prefix in paths fails when workspace IS KDS repository
    incorrect_pattern: $path = "$WorkspaceRoot\KDS\kds-brain"
    correct_pattern: if ($WorkspaceRoot -match '\\KDS$') { $path = "$WorkspaceRoot\kds-brain"
      } else { $path = "$WorkspaceRoot\KDS\kds-brain" }
    explanation: Detect if workspace path ends with KDS to avoid path doubling (D:\PROJECTS\KDS\KDS\)
    frequency: 6
    confidence: 0.98
    impact: high
    last_seen: '2025-11-04T10:15:00Z'
  powershell_start_job:
    issue: Start-Job with -FilePath fails with complex parameter passing
    incorrect_pattern: Start-Job -FilePath $script -ArgumentList $args
    correct_pattern: Start-Job -ScriptBlock { param($Script, $Args) & $Script @Args
      } -ArgumentList $script, $args
    explanation: Use -ScriptBlock instead of -FilePath for better parameter handling
      in parallel jobs
    frequency: 1
    confidence: 0.9
    impact: medium
    last_seen: '2025-11-04T10:20:00Z'
  powershell_dependencies:
    issue: Missing powershell-yaml module causes YAML operations to fail
    check: Test-Module -Name powershell-yaml
    install: Install-Module -Name powershell-yaml -Scope CurrentUser -Force
    explanation: ConvertFrom-Yaml and ConvertTo-Yaml cmdlets require powershell-yaml
      module
    frequency: 1
    confidence: 1.0
    impact: high
    last_seen: '2025-11-04T10:25:00Z'
  wpf_icon_validation:
    issue: WPF Material Design icons fail at runtime if invalid
    symptom: 'XamlParseException: ''Lightning is not a valid value for PackIconKind'''
    incorrect_approach: Assume icon names, test by running app
    correct_approach: Validate icon names with Enum.TryParse<PackIconKind> in tests
      BEFORE writing XAML
    explanation: XAML strings compile but fail at runtime during enum conversion.
      Tests catch this at test-time.
    test_pattern: MaterialDesignIconTests.cs with Theory tests for all icons
    frequency: 1
    confidence: 0.95
    impact: critical
    last_seen: '2025-11-05'
    lesson: "WPF UI components REQUIRE test-first approach - build success \u2260\
      \ runtime success"
  wpf_silent_failures:
    issue: WPF applications fail silently with no error dialog
    symptom: User clicks .exe, nothing happens, no error shown
    cause: XAML parsing errors occur before UI appears, app exits immediately
    detection: Use 'dotnet run' in terminal (shows errors) vs Start-Process (silent)
    prevention: Create ApplicationStartupTests with InitializeComponent tests
    explanation: WPF doesn't show console by default. Test initialization in unit
      tests.
    frequency: 1
    confidence: 0.92
    impact: high
    last_seen: '2025-11-05'
    lesson: Always test WPF XAML initialization - silent failures are user-hostile
  tdd_violation_wpf:
    issue: WPF implementation proceeded without TDD
    impact: Runtime crash, 30 minutes debugging, unclear failure mode
    should_have: Created icon validation tests FIRST, then XAML
    lesson: UI components are HIGH RISK for runtime errors - TDD is MANDATORY
    time_cost: 30 minutes debugging vs 5 minutes writing tests
    prevention: "RIGHT BRAIN must check: Is this UI/WPF/XAML? \u2192 TDD REQUIRED"
    frequency: 1
    confidence: 1.0
    severity: high
    last_seen: '2025-11-05'
    resolution: Retroactive TDD - created comprehensive test suite (53 tests)
    commitment: Will enforce TDD for ALL WPF/UI implementations
  
  windows_platform_compatibility:
    issue: Unix-only functions (fcntl, os.getuid) cause ModuleNotFoundError/AttributeError on Windows
    validation_approach: Conditional imports with try/except, platform checks before calling OS-specific functions
    rationale: Python standard library has Unix-specific modules unavailable on Windows
    frequency: 2
    confidence: 1.0
    impact: high
    last_seen: '2025-11-15'
    lesson: 'Always check platform compatibility for Unix-specific modules; use hasattr() for os functions like getuid()'
    examples:
      fcntl_import: 'try: import fcntl; HAS_FCNTL=True except ImportError: HAS_FCNTL=False'
      os_getuid_check: 'if hasattr(os, "getuid"): uid = os.getuid() else: uid = None'
    source: 'CopilotChats.md lines 2400-3100, ambient daemon Windows compatibility fixes'
  
  architecture_level_integration:
    issue: Ad-hoc feature additions create fragmented systems without central visibility
    validation_approach: Integrate new system components into existing orchestrators/routers
    rationale: Centralized integration provides visibility, consistency, maintainability
    frequency: 1
    confidence: 0.9
    impact: medium-high
    last_seen: '2025-11-15'
    lesson: 'New system components should integrate into existing orchestration workflows, not as standalone scripts'
    example: 'Ambient daemon health check added to optimize orchestrator Phase 6 (lines 556-618) instead of separate monitoring script'
    source: 'CopilotChats.md lines 2900-3100, optimize orchestrator integration'
  
  iterative_user_feedback_refinement:
    issue: Initial implementations may miss edge cases or misunderstand requirements
    validation_approach: Responsive iteration based on clarifying user feedback
    rationale: User feedback exposes edge cases and requirements misunderstandings early
    frequency: 1
    confidence: 1.0
    impact: medium
    last_seen: '2025-11-15'
    lesson: 'Implement initial solution quickly, then refine based on user clarification rather than over-analyzing upfront'
    example: 'Separator line removal: "remove all lines" → user clarifies "tables OK" → refine to "remove only conversational separators"'
    source: 'CopilotChats.md lines 2001-2400, response template cleanup'
  
  natural_language_entry_points:
    issue: Programmatic APIs create cognitive load and require documentation lookup
    validation_approach: Design feature entry points as natural language commands
    rationale: Natural language reduces cognitive load, aligns with AI-assisted development
    frequency: 3
    confidence: 0.9
    impact: medium
    last_seen: '2025-11-15'
    lesson: 'Support conversational commands (lets plan, optimize, proceed with) alongside programmatic APIs'
    examples:
      - 'YAML Phase Tracker: lets plan entry point'
      - 'Optimize Orchestrator: optimize command'
      - 'General workflows: proceed with command'
    source: 'CopilotChats.md throughout, natural language command usage'
  
  feature_viability_reassessment:
    issue: Implemented features may not deliver value proportional to complexity
    root_cause: Initial design assumptions about automation value proven incorrect in practice
    symptom: "Feature works but adds overhead (10-50ms per request), cross-platform complexity, maintenance burden"
    incident_date: '2025-11-15'
    impact: "Critical architectural decision - sometimes removing working code is better than maintaining it"
    solution: Systematic viability assessment with removal as valid outcome
    rationale:
      - "Working code isn't always valuable code"
      - "Complexity cost must justify benefit"
      - "Manual approaches can be superior when they provide control and clarity"
      - "Removal is refactoring - simplification improves architecture"
    viability_assessment_framework:
      complexity_factors:
        - "Cross-platform compatibility requirements"
        - "Per-request overhead (latency cost)"
        - "Thread management and concurrency complexity"
        - "Integration touchpoints (how many systems affected)"
      value_factors:
        - "User control vs automation trade-off"
        - "Reliability improvement vs manual baseline"
        - "Time saved vs overhead added"
        - "Maintenance burden vs benefit"
      decision_criteria:
        remove_if: "Complexity cost > Value delivered"
        keep_if: "Value clearly exceeds complexity"
        iterate_if: "Uncertain - need more user feedback"
    removal_indicators:
      - "User questions feature viability after implementation"
      - "Cross-platform bugs consume disproportionate time"
      - "Performance overhead measurable and concerning"
      - "Manual alternative exists and is simpler"
      - "Feature adds cognitive load without clear benefit"
    frequency: 1
    confidence: 0.98
    severity: high
    last_seen: '2025-11-15'
    lesson: "Be willing to remove recently implemented features if viability assessment shows poor complexity/value ratio"
    example: "Ambient daemon: 2,000+ lines, cross-platform issues, 10-50ms overhead → removed in favor of manual capture hints"
    source: 'CopilotChats.md lines 500-945, ambient daemon removal decision and execution'
  
  efficiency_vs_accuracy_tradeoff:
    issue: System health monitoring creates tension between freshness and performance
    validation_approach: Analyze per-request overhead vs background monitoring vs no monitoring
    rationale: Different monitoring strategies have different cost profiles
    frequency: 1
    confidence: 0.95
    impact: medium-high
    last_seen: '2025-11-15'
    lesson: 'Per-request checks add measurable latency; background checks add thread complexity; sometimes no monitoring is optimal'
    tradeoff_analysis:
      per_request_checks:
        accuracy: "100% - always current state"
        overhead: "10-50ms per request (unacceptable for performance-sensitive systems)"
        complexity: "Low - simple process check"
      background_monitoring:
        accuracy: "95% - stale up to check interval (5 minutes)"
        overhead: "Negligible per-request (<1ms), but adds background thread"
        complexity: "High - thread management, synchronization, caching"
      no_monitoring:
        accuracy: "Manual validation only"
        overhead: "Zero"
        complexity: "Zero"
    decision_framework:
      use_per_request: "Mission-critical systems where stale data unacceptable"
      use_background: "Important monitoring with acceptable staleness (minutes)"
      use_no_monitoring: "Feature not critical, or manual validation sufficient"
    example: "Daemon health check: Option A (5-min background) chosen over Option B (per-request) for efficiency, then Option C (removal) chosen for simplicity"
    source: 'CopilotChats.md lines 300-600, daemon health monitoring design discussion'
  
  automated_vs_manual_strategy:
    issue: Automation decisions often made prematurely without considering manual alternative value
    validation_approach: Explicit comparison of automated vs manual approaches before implementation
    rationale: Manual approaches provide control, simplicity, and often better UX for certain tasks
    frequency: 1
    confidence: 0.92
    impact: high
    last_seen: '2025-11-15'
    lesson: 'Evaluate manual alternatives seriously - they may be superior for tasks requiring user judgment or occasional execution'
    comparison_framework:
      automation_advantages:
        - "Zero user friction (happens automatically)"
        - "Consistent execution (no human error)"
        - "Works in background (no attention needed)"
      automation_disadvantages:
        - "Adds system complexity (cross-platform, threading, monitoring)"
        - "Removes user control (can't skip low-quality captures)"
        - "Performance overhead (always running even when not needed)"
      manual_advantages:
        - "User judgment (quality filtering)"
        - "Explicit control (capture only valuable conversations)"
        - "Zero overhead when not used"
        - "Simpler implementation"
      manual_disadvantages:
        - "Requires user action (friction)"
        - "Easy to forget (requires discipline)"
    decision_criteria:
      automate_if:
        - "Task happens frequently (multiple times per hour)"
        - "Quality judgment not needed (all instances valuable)"
        - "User forgets often (low discipline tasks)"
      manual_if:
        - "Task occasional (few times per day)"
        - "Quality judgment important (filter low-value instances)"
        - "User control valuable (selective execution)"
    example: "Conversation capture: Ambient daemon (automated) vs manual hints (user-triggered) → Manual chosen for quality control and simplicity"
    source: 'CopilotChats.md lines 600-945, daemon removal rationale and manual capture comparison'
version: 6.0.0
last_updated: 2025-11-15 18:00:00+00:00
workflow_patterns:
  yaml_migration_workflow:
    pattern: structured_document_migration
    description: "Convert structured Markdown documents to YAML to prevent GitHub Copilot conversation history loops"
    steps:
      - "1_identify: Scan for structured MD files (reports, status, analysis, planning)"
      - "2_categorize: Separate conversion candidates from narrative content (keep guides/stories as MD)"
      - "3_design_schema: Create JSON Schema for each document type"
      - "4_convert: Parse MD structure and map to YAML schema"
      - "5_validate: Run JSON Schema validation on converted YAML"
      - "6_commit_yaml: Git commit new YAML file with descriptive message"
      - "7_commit_deletion: Git commit deletion of MD file with reference to previous commit"
      - "8_verify: Confirm CORTEX can read new YAML format"
    rationale: "Large MD files cause 10-30 second 'Summarizing...' delays in GitHub Copilot. YAML reduces tokens by 60% and eliminates delays."
    success_rate: 1.0
    time_saved_per_interaction: "10-30 seconds"
    token_reduction: "60%"
    confidence: 0.98
    last_used: '2025-11-15'
    git_strategy:
      no_archiving: "Git history is the backup - no redundant archives folder"
      commit_sequence:
        - "Stage 1: Commit YAML (feat(migration): Convert {file} to YAML)"
        - "Stage 2: Commit deletion (chore(migration): Delete {file} after YAML conversion)"
      rollback: "git revert {commit_hash} restores from history"
    conversion_categories:
      convert_to_yaml:
        - "Implementation reports"
        - "Status tracking"
        - "Analysis results"
        - "Planning documents"
        - "Phase tracking"
        - "Metrics/statistics"
      keep_as_markdown:
        - "User guides"
        - "Story/narrative content"
        - "README files"
        - "Tutorial content"
        - "Human-readable explanations"
    benefits:
      performance: "Eliminates 10-30 second summarization delays"
      tokens: "60% reduction in conversation history token usage"
      validation: "JSON Schema catches data errors automatically"
      maintainability: "Programmatic updates easier with structured format"
      diffs: "Cleaner version control diffs for structured data"
  
  kds_quadrant_creation:
    pattern: four_perspective_documentation
    description: Create comprehensive documentation using KDS Quadrant structure
    steps:
    - '1_story: Write human-centered narrative (emotional connection, metaphors, day-in-life
      examples)'
    - '2_technical: Document detailed specifications (commands, parameters, file structures,
      code examples)'
    - '3_image_prompt: Create or describe visual representation (diagrams, flowcharts,
      progress indicators)'
    - '4_high_level_technical: Explain architectural overview (system design, workflows,
      integration points)'
    rationale: Four perspectives ensure complete understanding for different learning
      styles and use cases
    success_rate: 1.0
    time_saved_minutes: 45
    confidence: 1.0
    last_used: '2025-11-04'
    examples:
    - BRAIN system documentation
    - Setup process documentation
    - Agent architecture documentation
  powershell_script_debugging:
    pattern: test_individually_before_jobs
    description: Debug PowerShell scripts directly before using in Start-Job
    steps:
    - Run script directly with test parameters
    - Fix any syntax errors (especially regex patterns)
    - Test with various input scenarios
    - Only then integrate with Start-Job orchestration
    success_rate: 1.0
    time_saved_minutes: 30
    confidence: 0.95
    last_used: '2025-11-04T10:30:00Z'
  powershell_regex_best_practice:
    pattern: use_hex_escape_for_quotes
    description: Use hex escape sequences for quotes in regex patterns
    examples:
      single_quote: \x27 instead of `' or \'
      double_quote: \x22 instead of `" or \"
      both: '[\x27\x22] matches both quote types'
    rationale: Avoids PowerShell parsing ambiguity and escaping issues
    confidence: 0.95
    last_used: '2025-11-04T10:00:00Z'
  wpf_application_tdd:
    pattern: test_first_for_wpf_ui
    description: WPF applications MUST use TDD - build success does not guarantee
      runtime success
    steps:
    - 'Phase 0: Create test infrastructure (MaterialDesignIconTests, ViewModelTests,
      SmokeTests)'
    - 'Phase 1: RED - Write failing tests for icons, ViewModels, deployment'
    - 'Phase 2: GREEN - Implement XAML with VALIDATED icons, implement ViewModels'
    - 'Phase 3: REFACTOR - Expand features, keep tests green'
    rationale: XAML validation happens at runtime, not compile time. Silent failures
      are common.
    success_rate: 1.0
    failure_rate_without_tdd: 1.0
    time_saved_minutes: 30
    confidence: 0.95
    last_used: '2025-11-05'
    examples:
    - KDS Dashboard WPF - retroactive TDD caught all issues
    test_files_created:
    - MaterialDesignIconTests.cs (23 tests) - validates all PackIconKind enums
    - ApplicationSmokeTests.cs (10 tests) - validates deployment completeness
    - ViewModelTests.cs (8 tests) - validates data binding and instantiation
    - IntegrationTests.cs (7 tests) - validates component integration
  wpf_icon_validation_pattern:
    pattern: validate_material_design_icons_first
    description: Test Material Design icon names BEFORE using in XAML
    steps:
    - Extract all planned icon names (Brain, Flash, Heart, etc.)
    - 'Create Theory test: AllUsedIcons_ShouldBeValidPackIconKinds'
    - Use Enum.TryParse<PackIconKind> to validate each name
    - Test fails? Look up valid alternatives at materialdesignicons.com
    - Test passes? Safe to use in XAML
    success_rate: 1.0
    confidence: 0.95
    last_used: '2025-11-05'
    examples:
    - "Lightning (INVALID) \u2192 Flash (VALID)"
    - "HeartPulse (INVALID) \u2192 Heart (VALID)"
    benefits:
    - Catches invalid icons at test-time (not runtime)
    - Documents all icons used in project
    - Prevents XamlParseException crashes
    - Future-proof (auto-validates new icons)
  wpf_smoke_test_pattern:
    pattern: verify_wpf_deployment_complete
    description: Verify WPF application deployment before user testing
    tests:
    - Executable_ShouldExist (check .exe built)
    - MainDll_ShouldExist (check main assembly)
    - MaterialDesignThemes_DllShouldExist (check UI dependency)
    - RuntimeConfig_ShouldExist (check .NET runtime config)
    - ApplicationVersion_ShouldBeNet8 (check target framework)
    purpose: Catch deployment issues BEFORE user runs app
    confidence: 0.92
    last_used: '2025-11-05'
    time_saved: 10-15 minutes per broken deployment
  
  systematic_multi_stage_debugging:
    pattern: systematic_debugging
    description: '5-stage debugging approach for complex system issues with validation at each stage'
    steps:
    - 'Stage 1: Identify error symptoms (logs, exceptions, stack traces)'
    - 'Stage 2: Fix data layer issues (database schema, migrations)'
    - 'Stage 3: Fix API layer issues (method names, signatures, contracts)'
    - 'Stage 4: Fix platform layer issues (OS-specific functions, cross-platform compatibility)'
    - 'Stage 5: Integrate fixes into system health checks (orchestrators, monitoring)'
    - 'Validation: Test at each stage before proceeding to next'
    rationale: 'Layered debugging prevents cascading failures and ensures fixes are validated incrementally'
    success_rate: 1.0
    confidence: 0.95
    last_used: '2025-11-15'
    example: 'Ambient daemon fix: schema (created_at column) → API (detect_or_create_session) → platform (fcntl import) → platform (os.getuid) → integration (optimize orchestrator health check)'
    source: 'CopilotChats.md lines 2400-3100, ambient daemon 5-stage debugging'
  
  user_feedback_iteration_workflow:
    pattern: user_feedback_refinement
    description: 'Responsive iteration based on clarifying user feedback'
    steps:
    - 'Step 1: User reports issue or requests feature'
    - 'Step 2: Agent implements initial solution based on understanding'
    - 'Step 3: User provides clarifying feedback or identifies edge cases'
    - 'Step 4: Agent refines solution addressing specific feedback'
    - 'Step 5: Validate refinement meets updated requirements'
    success_rate: 1.0
    confidence: 1.0
    last_used: '2025-11-15'
    rationale: 'User feedback exposes edge cases and requirements misunderstandings early, avoiding over-analysis upfront'
    example: 'Separator line removal: initial "remove all lines" → user clarifies "tables OK" → refined to "remove only conversational separators, preserve table structures"'
    source: 'CopilotChats.md lines 2001-2400, iterative template cleanup'
  
  systematic_feature_removal:
    pattern: thorough_feature_removal
    description: '8-phase systematic cleanup when removing implemented features to prevent residual confusion'
    steps:
    - 'Phase 1: Delete source code files (main implementation files)'
    - 'Phase 2: Delete test files (unit tests, integration tests, fixtures)'
    - 'Phase 3: Remove configuration entries (tasks.json, settings, auto-start configs)'
    - 'Phase 4: Clean documentation (remove feature sections from guides, README)'
    - 'Phase 5: Update source comments (remove references in docstrings, inline comments)'
    - 'Phase 6: Add test skip decorators (preserve historical tests with skip reason)'
    - 'Phase 7: Remove runtime files (PID files, logs, temporary data)'
    - 'Phase 8: Verify manual alternatives (confirm replacement functionality exists)'
    rationale: 'Incomplete removal causes confusion and technical debt; thorough cleanup prevents future misunderstandings'
    success_rate: 1.0
    confidence: 0.98
    last_used: '2025-11-15'
    search_patterns:
      - 'Feature name variations (ambient_daemon, ambientDaemon, AmbientCapture)'
      - 'File type patterns (*.py, *.md, *.json, *.yaml, *.log)'
      - 'Documentation sections (### headers, configuration blocks)'
      - 'Code references (imports, function calls, class instantiations)'
    completion_criteria:
      critical_removed: 'All source code, tests, and configuration entries deleted'
      documentation_updated: 'All user-facing guides cleaned, no feature references'
      historical_preserved: 'CopilotChats.md archives kept, plugin metadata preserved (non-functional)'
      alternatives_verified: 'Manual/replacement functionality confirmed working'
    example: 'Ambient daemon removal: 10 files deleted, 8 files modified, 50+ doc lines removed, completion report generated'
    time_saved_avoiding_confusion: '30-60 minutes per developer encountering stale references'
    source: 'CopilotChats.md lines 600-945, systematic daemon removal execution'
  
  stub_replacement_pattern:
    pattern: deprecation_stub_instead_of_deletion
    description: 'Replace removed integrated features with deprecation stubs that return "Not available" messages'
    rationale: 'Preserves orchestrator/workflow continuity while clearly indicating feature removal'
    when_to_use:
      - 'Feature is integrated into orchestrator/workflow (not standalone)'
      - 'Complete method deletion would break orchestration flow'
      - 'Deprecation message provides clarity for users/developers'
    implementation:
      before: |
        def _check_ambient_daemon(self) -> str:
            \"\"\"Check if ambient daemon is running.\"\"\"
            # 65 lines of health check logic
            return status_message
      after: |
        def _check_ambient_daemon(self) -> str:
            \"\"\"Ambient daemon removed in CORTEX 3.0 (replaced with manual capture hints).\"\"\"
            return "❌ Not available (feature removed)"
    benefits:
      - 'Orchestrator continues to function (no breaking changes)'
      - 'Clear communication (users see explicit removal message)'
      - 'Clean audit trail (deprecation reason in docstring)'
      - 'Future-proof (stub can point to replacement feature)'
    success_rate: 1.0
    confidence: 0.95
    last_used: '2025-11-15'
    example: 'optimize_cortex_orchestrator.py Phase 6 daemon check: 65 lines → 4-line stub with deprecation message'
    source: 'CopilotChats.md lines 700-800, optimize orchestrator stub replacement'
architectural_patterns:
  api_auth: none
  service_naming: I{Name} interface
  test_test_types:
  - e2e
  - unit
  test_selector_strategy: id
  ui_di_pattern: none
  api_versioning: url-path
  service_di_registration: Program.cs
  ui_component_structure: feature-based
  service_layering: service-only
  test_framework: Playwright
  ui_naming: PascalCase
  api_routing: attribute-based
intent_patterns:
  kds_quadrant_documentation:
    pattern: update_documentation
    description: 'User wants to update KDS Quadrant (4-document structure: Story,
      Technical, Image Prompt, High-Level Technical)'
    triggers:
    - update documentation
    - update docs
    - publish docs
    - publish documentation
    - update the quadrant
    - refresh documentation
    - document this
    routes_to: work-planner
    action: create_or_update_kds_quadrant
    confidence: 0.95
    last_used: '2025-11-04'
  kds_quadrant_structure:
    name: KDS Quadrant
    description: Four-perspective documentation pattern for comprehensive concept
      explanation
    components:
      1_story: "Human-centered narrative (\U0001F9DA story for humans)"
      2_technical: Detailed technical specifications and implementation
      3_image_prompt: Visual representation or diagram prompt (implicit or explicit)
      4_high_level_technical: Architectural overview and system-level explanation
    purpose: 'Explain complex concepts from multiple angles: emotional, practical,
      visual, architectural'
    examples:
    - The Intern with Amnesia (Story) + SOLID Architecture (Technical) + Brain Visualization
      (Image) + BRAIN System Overview (High-Level)
    - Three-Story Brain (Story) + Tier 0-5 Specs (Technical) + Tier Diagram (Image)
      + Automatic Learning Workflow (High-Level)
    - Day in the Life (Story) + Command Reference (Technical) + Workflow Examples
      (Image) + How It Works (High-Level)
    confidence: 1.0
    created: '2025-11-04'
file_relationships:
  tests/fixtures/mock-project/tests/UI/dashboard.spec.ts:
  - relationship: test-coverage
    source: test-crawler
    confidence: 0.8
    related_file: Components/**/dashboard.razor
  backups/pre-independence-migration/tests/test-brain-integrity.spec.ts:
  - relationship: test-coverage
    source: test-crawler
    confidence: 0.8
    related_file: Components/**/test-brain-integrity.razor
  tests/fixtures/mock-project/tests/Unit/UserServiceTests.cs:
  - relationship: test-coverage
    source: test-crawler
    confidence: 0.8
    related_file: Components/**/UserService.razor
  backups/pre-independence-migration/tests/fixtures/mock-project/tests/Unit/UserServiceTests.cs:
  - relationship: test-coverage
    source: test-crawler
    confidence: 0.8
    related_file: Components/**/UserService.razor
  tests/test-brain-integrity.spec.ts:
  - relationship: test-coverage
    source: test-crawler
    confidence: 0.8
    related_file: Components/**/test-brain-integrity.razor
  backups/pre-independence-migration/tests/fixtures/mock-project/tests/UI/dashboard.spec.ts:
  - relationship: test-coverage
    source: test-crawler
    confidence: 0.8
    related_file: Components/**/dashboard.razor
test_patterns:
  selector_strategy: id
  element_ids:
  - confidence: 0.95
    component: backups/pre-independence-migration/tests/fixtures/mock-project/src/frontend/src/components/UserDashboard.tsx
    id: {}
    purpose: Unknown
  - confidence: 0.95
    component: tests/fixtures/mock-project/src/frontend/src/components/UserDashboard.tsx
    id: {}
    purpose: Unknown
  test_data: {}
optimization_patterns:
  hardcoded_violation_remediation:
    pattern_type: systematic_optimization
    confidence: 0.95
    success_rate: 0.98
    description: Systematic approach to fixing hardcoded data violations
    workflow:
    - analyze_violations_by_category
    - prioritize_critical_vs_non_critical
    - implement_batch_fixes_with_patterns
    - validate_improvements_with_tests
    learned_from: conv_optimization_20251114_191635_0a6e2ca4
    evidence:
      total_violations: 913
      critical_violations_fixed: 157
      files_improved: 5
      test_improvements: 7/7 passing after fixes
    transferable_techniques:
    - test_fixture_dynamic_patterns
    - environment_configuration_abstraction
    - violation_categorization_strategy
    - batch_processing_workflow
  test_fixture_optimization:
    pattern_type: testing_improvement
    confidence: 0.92
    description: Convert hardcoded test paths to dynamic fixtures
    before_after:
      before: hardcoded paths in test files
      after: dynamic fixture generation from environment
    implementation: Environment-aware test fixture patterns
    impact: Zero hardcoded paths in test files
    learned_from: test_session_correlation.py optimization
  environment_configuration:
    pattern_type: architectural_improvement
    confidence: 0.88
    description: Replace hardcoded values with configurable patterns
    examples:
    - "screenshot paths \u2192 environment-based configuration"
    - "file system paths \u2192 dynamic path resolution"
    - "deployment targets \u2192 configurable environments"
    architectural_benefit: Improved maintainability and environment flexibility
learning_sessions:
- session_id: optimization_20251114_191635
  timestamp: '2025-11-14T19:17:36.818664'
  source_conversation: CopilotChats.md hardcoded violation optimization
  patterns_learned: 3
  strategic_value: high
  quality_score: 9.5
  application_domains:
  - testing
  - configuration
  - systematic_optimization
  success_metrics:
    violations_fixed: 913
    critical_violations: 157
    architectural_improvements: 3

- session_id: track_a_phase_1_20251115_implementation
  timestamp: '2025-11-15T00:00:00.000000'
  source_conversation: CopilotChats.md Track A Phase 1 Complete Implementation
  patterns_learned: 7
  strategic_value: exceptional
  quality_score: 12.0
  application_domains:
  - feature_implementation
  - systematic_debugging
  - phased_development
  - test_driven_validation
  success_metrics:
    production_code_lines: 1757
    test_pass_rate: 100
    bugs_fixed: 7
    phases_completed: 3
    documentation_lines: 1000
  key_learnings:
    phased_implementation:
      description: Clear progression through foundation, core, and validation phases
      phases:
        - "Phase 1.1: Foundation Setup (directory structure, package initialization)"
        - "Phase 1.2: Core Implementation (4 major components, 1,757 lines)"
        - "Phase 1.3: Integration Validation (10 tests, 100% pass rate)"
      confidence: 0.98
      success_rate: 1.0
    systematic_debugging:
      description: Measured improvement through iterative debugging cycles
      progression:
        - "First Run: 40% pass rate (4/10 tests)"
        - "Second Run: 70% pass rate (7/10 tests) - +30% improvement"
        - "Third Run: 100% pass rate (10/10 tests) - +30% improvement"
      total_improvement: "+60 percentage points"
      bugs_fixed: 7
      batch_approach: "4 fixes, then 3 fixes (more efficient than sequential)"
      confidence: 0.95
    root_cause_analysis:
      description: Systematic identification and documentation of bug root causes
      bug_categories:
        - api_contract_consistency
        - return_structure_depth
        - fixture_isolation
        - field_naming_conventions
      documentation_pattern: before/after code samples for all fixes
      confidence: 0.92
    test_driven_validation:
      description: Integration tests catching issues before production
      approach: integration_tests_before_exhaustive_unit_tests
      rationale: Phase 1 validates pipeline, Phase 2 adds component tests
      value: "Pragmatic MVP strategy - validate end-to-end first"
      confidence: 0.90
    api_contract_consistency:
      issue: Field naming inconsistencies between components
      examples:
        - "is_multi_turn vs multi_turn (boolean field naming)"
        - "status field missing in return structures"
        - "conversation vs conversation_id (metadata stripping)"
      solution: Establish naming conventions early, validate with tests
      lesson: "API contracts must be explicit and consistent"
      confidence: 0.95
    fixture_isolation:
      issue: Test fixtures creating separate instances instead of sharing state
      problem: "Importer uses internal adapter, test queries different adapter"
      solution: "Use importer's internal adapter for assertions"
      lesson: "Cross-component state testing requires shared instances"
      confidence: 0.93
    return_structure_depth:
      issue: Stripping metadata when passing data between components
      problem: "retrieve_conversation returned just conversation dict, not full record"
      solution: "Return complete records with conversation_id and metadata"
      lesson: "Don't strip metadata - downstream components may need it"
      confidence: 0.94
  time_investment:
    implementation: 4.0 hours
    first_debugging: 0.3 hours
    second_debugging: 0.25 hours
    documentation: 0.25 hours
    total: 4.8 hours
  deliverables:
    production_components:
      - "CopilotConversationParser (288 lines)"
      - "SemanticExtractor (400 lines)"
      - "ConversationalChannelAdapter (226 lines)"
      - "ConversationImporter (445 lines)"
    test_suite:
      - "Integration tests (10 tests, 100% passing)"
      - "Fast test cycles (2.75s execution)"
    documentation:
      - "TRACK-A-PHASE-1-VALIDATION-COMPLETE.md (~500 lines)"
      - "TRACK-A-PHASE-1-COMPLETE.md (~500 lines)"
  transferable_patterns:
    - phased_implementation_strategy
    - systematic_debugging_methodology
    - batch_fix_efficiency
    - integration_test_first_approach
    - progress_measurement_tracking
    - complete_audit_trail_documentation

- session_id: ambient_daemon_removal_20251115
  timestamp: '2025-11-15T18:00:00.000000'
  source_conversation: CopilotChats.md Ambient Daemon Removal (lines 300-945)
  patterns_learned: 5
  strategic_value: exceptional
  quality_score: 11.5
  application_domains:
  - architectural_decision_making
  - feature_removal
  - complexity_management
  - viability_assessment
  success_metrics:
    files_deleted: 10
    files_modified: 8
    documentation_lines_removed: 50
    code_lines_removed: 2000
    cleanup_phases: 8
  key_learnings:
    feature_viability_reassessment:
      description: Systematic framework for deciding when to remove working features
      decision_factors:
        - "Complexity cost vs value delivered"
        - "Cross-platform compatibility burden"
        - "Per-request overhead (10-50ms)"
        - "Manual alternative superiority"
      outcome: "Removal accepted as valid architectural improvement"
      confidence: 0.98
    systematic_feature_removal:
      description: 8-phase cleanup methodology for thorough feature removal
      phases:
        - "Delete source code"
        - "Delete tests"
        - "Remove configuration"
        - "Clean documentation"
        - "Update comments"
        - "Skip historical tests"
        - "Remove runtime files"
        - "Verify alternatives"
      completeness: "Zero critical references remaining"
      confidence: 0.98
    efficiency_vs_accuracy_tradeoff:
      description: Framework for monitoring strategy selection
      options_analyzed:
        - "Per-request checks: 100% accuracy, 10-50ms overhead"
        - "Background monitoring: 95% accuracy, thread complexity"
        - "No monitoring: Manual validation, zero overhead"
      decision: "No monitoring (removal) chosen for simplicity"
      confidence: 0.95
    automated_vs_manual_strategy:
      description: Decision framework for automation level selection
      automation_evaluated: "Ambient daemon (full automation)"
      manual_alternative: "Capture hints with quality scoring"
      decision_criteria:
        - "User control important (quality filtering)"
        - "Task occasional (few per day)"
        - "Complexity not justified by frequency"
      outcome: "Manual approach chosen for simplicity and control"
      confidence: 0.92
    stub_replacement_pattern:
      description: Replace removed integrated features with deprecation stubs
      rationale: "Preserves orchestrator flow, provides clear deprecation message"
      implementation: "65-line health check → 4-line stub with explanation"
      benefit: "No breaking changes, clear communication"
      confidence: 0.95
  time_investment:
    viability_assessment: 0.5 hours
    implementation_discussion: 1.0 hours
    removal_execution: 1.5 hours
    total: 3.0 hours
  deliverables:
    cleanup_phases: 8
    completion_report: "Comprehensive removal summary with statistics"
    stub_replacements: 1
    documentation_updates: "setup-guide.md, optimize orchestrator"
  architectural_impact:
    complexity_reduced: "2,000+ lines removed"
    maintenance_burden_eliminated: "Cross-platform compatibility issues"
    performance_improved: "Zero overhead (was 10-50ms per request)"
    user_control_improved: "Manual capture with quality judgment"
  transferable_patterns:
    - feature_viability_assessment_framework
    - systematic_removal_workflow
    - efficiency_tradeoff_analysis
    - automation_decision_framework
    - stub_replacement_strategy

