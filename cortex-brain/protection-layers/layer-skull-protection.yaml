# CORTEX Protection Layer: Skull Protection
# Extracted from brain-protection-rules.yaml for token optimization
# Version: 2.1
# Part of: CORTEX 3.0 Track B (Token Optimization)

version: "2.1"
type: "protection_layer"
layer_id: "skull_protection"
parent_file: "brain-protection-rules.yaml"

# Layer definition (normalized indentation)
- layer_id: "skull_protection"
  name: "SKULL Protection Layer"
  description: "Test validation and quality enforcement (prevents November 9th incident)"
  priority: 5

  rules:
    - rule_id: "SKULL_TEST_BEFORE_CLAIM"
      name: "Test Before Claim (SKULL-001)"
      severity: "blocked"
      description: "Never claim a fix is complete without test validation"

      detection:
        keywords:
          - "fixed âœ…"
          - "complete âœ…"
          - "done âœ…"
          - "implemented âœ…"
        without_keywords:
          - "test passed"
          - "test verified"
          - "validated by test"
          - "pytest"
        scope: ["response"]

      alternatives:
        - "Create automated test before claiming fix"
        - "Run test and include results in response"
        - "Show test output: 'Fixed âœ… (Verified by: test_button_color)'"

      evidence_template: "Claim: '{match}' without test validation"

      rationale: |
        SKULL-001: Test Before Claim

        Real incident (2025-11-09):
        - CSS fixes claimed "Fixed âœ…" three times
        - Vision API claimed "Auto-engages âœ…" 
        - Zero tests run to validate
        - User had to report "not working" each time

        SKULL prevents this by BLOCKING any success claim without test validation.

    - rule_id: "SKULL_INTEGRATION_VERIFICATION"
      name: "Integration Verification (SKULL-002)"
      severity: "blocked"
      description: "Integration must be tested end-to-end"

      detection:
        keywords:
          - "integration complete"
          - "components connected"
          - "API integrated"
          - "auto-engages"
        without_keywords:
          - "end-to-end test"
          - "integration test"
          - "e2e test"
        scope: ["description"]

      alternatives:
        - "Create end-to-end integration test"
        - "Test full call chain: A â†’ B â†’ C"
        - "Verify actual execution path, not just config"

      evidence_template: "Integration claim without E2E test: '{match}'"

      rationale: |
        SKULL-002: Integration Verification

        Real incident (2025-11-09):
        - Vision API "integration" claimed complete
        - Only config was changed
        - No test of actual call chain
        - Vision API was never actually called

        SKULL prevents this by requiring end-to-end integration tests.

    - rule_id: "SKULL_VISUAL_REGRESSION"
      name: "Visual Regression (SKULL-003)"
      severity: "warning"
      description: "CSS/UI changes require visual validation"

      detection:
        keywords:
          - "css fixed"
          - "style updated"
          - "color changed"
          - "UI improved"
        without_keywords:
          - "visual test"
          - "computed style"
          - "playwright"
          - "browser test"
        scope: ["description"]

      alternatives:
        - "Add visual regression test (Playwright/Puppeteer)"
        - "Verify computed style in browser"
        - "Include before/after screenshot comparison"

      evidence_template: "CSS/UI change without visual test: '{match}'"

      rationale: |
        SKULL-003: Visual Regression

        Real incident (2025-11-09):
        - CSS rules applied to fix title color
        - Claimed "Fixed âœ…" without checking browser
        - Cache wasn't cleared, changes not visible
        - Repeated 3 times with same approach

        SKULL prevents this by requiring visual validation of CSS changes.

    - rule_id: "SKULL_RETRY_WITHOUT_LEARNING"
      name: "Retry Without Learning (SKULL-004)"
      severity: "warning"
      description: "Must diagnose failures before retrying same approach"

      detection:
        combined_keywords:
          retry_marker:
            - "try again"
            - "retry"
            - "attempt 2"
            - "attempt 3"
          no_diagnosis:
            - "same fix"
            - "reapply"
            - "rebuild again"
        without_keywords:
          - "diagnosed"
          - "root cause"
          - "cache cleared"
          - "verified"
        scope: ["description"]
        logic: "AND"

      alternatives:
        - "Diagnose WHY previous fix failed"
        - "Check: file contents, browser cache, build output, computed styles"
        - "Change approach based on diagnosis"
        - "Add test to prevent regression"

      evidence_template: "Retry without diagnosis: '{description}'"

      rationale: |
        SKULL-004: Retry Without Learning

        Real incident (2025-11-09):
        - CSS fix applied
        - User: "didn't work"
        - Same CSS fix applied again
        - User: "still didn't work"  
        - Same CSS fix applied THIRD time
        - No diagnosis of why it failed

        SKULL prevents this by requiring root cause analysis before retries.

    - rule_id: "SKULL_TRANSFORMATION_VERIFICATION"
      name: "Transformation Verification (SKULL-005)"
      severity: "blocked"
      description: "Operations claiming transformation MUST produce measurable changes"

      detection:
        combined_keywords:
          transformation_claim:
            - "transformation complete"
            - "refresh complete"
            - "converted"
            - "updated documentation"
            - "generated"
          success_claim:
            - "success"
            - "completed successfully"
            - "fixed âœ…"
            - "done âœ…"
        scope: ["description", "log_output"]
        logic: "AND"

      verification_required:
        - type: "file_hash_comparison"
          description: "Compare file hash before/after operation"
          requirement: "Hashes MUST differ for transformation operations"

        - type: "git_diff_check"
          description: "Verify git diff shows actual changes"
          requirement: "git diff MUST show modifications, not empty output"

        - type: "content_analysis"
          description: "Validate transformation logic executed"
          requirement: "Operation MUST NOT be pass-through (input != output)"

      alternatives:
        - "Implement actual transformation logic (not pass-through)"
        - "Mark operation as 'validation-only' if no transformation needed"
        - "Add integration test verifying file changes occur"
        - "Change success message to reflect pass-through behavior"

      evidence_template: "Operation '{operation_name}' claims transformation but produces no changes"

      rationale: |
        SKULL-005: Transformation Verification

        Real incident (2025-11-10):
        - refresh_cortex_story operation executed
        - Module apply_narrator_voice_module.py claims "transformation complete"
        - Returns success=True with "Narrator voice transformation complete"
        - BUT: Line 123 does `context['transformed_story'] = story_content` (pass-through!)
        - File hash unchanged after operation
        - git diff shows NO changes
        - User discovers operation is fake

        Impact:
        - User trust degradation (claims success but does nothing)
        - Status inflation (operations marked READY when incomplete)
        - Integration failures (downstream operations expect real data)

        SKULL-005 prevents this by:
        1. Detecting transformation + success claims in output
        2. Requiring file hash comparison test
        3. Blocking completion without measurable changes
        4. Forcing honest status reporting (PARTIAL vs READY)

        Implementation:
        - Add @verify_transformation decorator to operation modules
        - Integration tests MUST check before/after file state
        - CI fails if transformation claims success but git diff empty
        - Status documents distinguish architecture vs implementation

    - rule_id: "SKULL_PRIVACY_PROTECTION"
      name: "Privacy Protection (SKULL-006)"
      severity: "blocked"
      description: "Publish operations MUST NOT include files with machine-specific paths or private data"

      detection:
        patterns:
          - "AHHOME"
          - ".coverage.*.* "
          - "C:\\\\"
          - "D:\\\\"
          - "/home/[a-z]+"
          - "/Users/[a-z]+"
        file_types:
          - "**/*.log"
          - "**/logs/**"
          - "**/.coverage.*"
          - "**/health-reports/**"
          - "**/__pycache__/**"
        scope: ["published_files"]

      verification_required:
        - type: "privacy_scan"
          description: "Scan all published files for machine-specific paths"
          requirement: "Zero files with absolute paths (C:\\, D:\\, /home/, AHHOME)"

        - type: "exclusion_test"
          description: "Test that publish script excludes privacy-leaking files"
          requirement: "Test MUST verify .coverage.*, logs/, health-reports/ excluded"

        - type: "config_sanitization"
          description: "Verify config files use template values, not real paths"
          requirement: "cortex.config.json MUST use placeholders, not AHHOME paths"

      alternatives:
        - "Add file patterns to EXCLUDE_PATTERNS in publish script"
        - "Create .publishignore file with privacy exclusions"
        - "Add pre-publish scan that fails on privacy leak"
        - "Use template configs with placeholder paths"

      evidence_template: "Published file contains privacy data: '{file_path}' - found '{privacy_leak}'"

      rationale: |
        SKULL-006: Privacy Protection

        Real incident (2025-11-12):
        - User runs publish script
        - Discovers .coverage.AHHOME.12345.XgvxuuYx in publish/CORTEX/
        - 7 coverage files with machine name exposed
        - logs/ambient_capture.log contains C:\Windows\Temp paths
        - cortex.config.json contains AHHOME machine paths
        - health-reports/ has user-specific diagnostic data

        Impact:
        - Privacy violation (machine names, usernames exposed)
        - Distribution bloat (unnecessary test artifacts)
        - Professionalism degradation (dev artifacts in user package)

        SKULL-006 prevents this by:
        1. Scanning published files for machine-specific patterns
        2. Requiring publish script exclude logs, coverage, health data
        3. Blocking publish if privacy leaks detected
        4. Enforcing template configs instead of real paths

        Implementation:
        - Add EXCLUDE_PATTERNS to publish script (logs, coverage, health)
        - Create test_publish_privacy.py that scans for leaks
        - Add pre-publish hook that runs privacy scan
        - Use cortex.config.template.json instead of cortex.config.json

    - rule_id: "SKULL_HEADER_FOOTER_IN_RESPONSE"
      name: "Faculty Integrity Check (SKULL-007)"
      severity: "blocked"
      description: "Publish package MUST contain ALL essential CORTEX faculties for full operation"

      detection:
        missing_faculties:
          - "Tier 0 (SKULL) not found"
          - "Tier 1 (Memory) not found"
          - "Tier 2 (Knowledge) not found"
          - "Tier 3 (Context) not found"
          - "Agents missing"
          - "Operations missing"
          - "Entry point missing"
        scope: ["published_files"]

      verification_required:
        - type: "comprehensive_faculty_test"
          description: "Test that verifies ALL CORTEX faculties exist in publish package"
          requirement: "test_cortex_fully_operational MUST pass"

        - type: "tier_verification"
          description: "Verify all 4 tiers (Tier 0-3) present"
          requirement: "brain_protector.py, conversation_manager.py, knowledge_graph/, context_intelligence.py"

        - type: "agent_verification"
          description: "Verify 10 specialist agents present"
          requirement: "cortex_agents/ directory with base_agent.py and agent implementations"

        - type: "entry_point_verification"
          description: "Verify GitHub Copilot integration files"
          requirement: "CORTEX.prompt.md and copilot-instructions.md in .github/"

        - type: "documentation_verification"
          description: "Verify user documentation present"
          requirement: "story.md, setup-guide.md, technical-reference.md, etc."

      alternatives:
        - "Use inclusion-based publish (copy ONLY essential files)"
        - "Create comprehensive faculty test that blocks publish if faculties missing"
        - "Maintain ESSENTIAL_FILES list of required CORTEX components"

      evidence_template: "Published CORTEX missing faculty: '{faculty_name}' - file not found: '{file_path}'"

      rationale: |
        SKULL-007: Faculty Integrity Check

        Real incident (2025-11-12):
        - Exclusion-based publish script too aggressive
        - Excluded 97.9% of files (good for privacy!)
        - BUT also excluded essential faculties:
          âŒ All 10 specialist agents missing
          âŒ Tier 1 conversation_tracker.py missing
          âŒ Entry points (CORTEX.prompt.md) missing
          âŒ Plugin system missing
        - Published CORTEX was incomplete and non-functional

        Impact:
        - Users copy broken CORTEX to their application
        - CORTEX cannot coordinate work (no agents)
        - CORTEX cannot remember (no Tier 1)
        - Copilot cannot find CORTEX (no entry points)
        - Result: Complete failure, wasted user time

        SKULL-007 prevents this by:
        1. Comprehensive test that verifies ALL faculties present
        2. Blocking publish if any faculty missing
        3. Listing exact files required for each faculty
        4. Testing BEFORE deployment (not discovery by users)

        The Brilliant Fix:
        Instead of exclusion-based publish (exclude dev files),
        switch to INCLUSION-based publish (include ONLY essentials):

        Benefits:
        - Simpler logic (copy what's needed vs exclude what's not)
        - Guaranteed completeness (explicit list of essentials)
        - No accidental omissions (inclusion list is exhaustive)
        - Better maintainability (clear intent)

        Implementation:
        - Create test_publish_faculties.py with test_cortex_fully_operational()
        - Test checks: Tier 0-3, Agents, Operations, Plugins, Entry Points, Docs
        - Publish script copies ONLY essential directories
        - Test runs BEFORE declaring publish complete

        Result:
        - Package size: 393 files, 3.8 MB (perfect!)
        - All faculties present: âœ…
        - No privacy leaks: âœ…  
        - CORTEX fully operational: âœ…

    - rule_id: "SKULL_HEADER_FOOTER_IN_RESPONSE_LEGACY"
      name: "Header/Footer in Copilot Response (Legacy)"
      severity: "blocked"
      description: "Operation orchestrators MUST include formatted headers/footers in Copilot Chat response"

      detection:
        combined_keywords:
          orchestrator_execution:
            - "execute operation"
            - "orchestrator.execute"
            - "operation complete"
          missing_header_footer:
            - "formatted_header: None"
            - "formatted_footer: None"
            - "no header in response"
        scope: ["code", "test_output"]
        logic: "AND"

      verification_required:
        - type: "result_object_check"
          description: "Verify OperationResult contains formatted_header/footer"
          requirement: "result.formatted_header MUST NOT be None"

        - type: "response_formatter_check"
          description: "Verify ResponseFormatter uses stored headers"
          requirement: "Chat response MUST include header/footer in code blocks"

        - type: "visual_inspection"
          description: "Verify header appears in Copilot Chat window"
          requirement: "User MUST see copyright header + purpose + accomplishments"

      alternatives:
        - "Use format_minimalist_header() and store in result.formatted_header"
        - "Use format_completion_footer() and store in result.formatted_footer"
        - "Ensure ResponseFormatter wraps headers in code blocks for display"
        - "Add integration test verifying header presence in formatted response"

      evidence_template: |
        Operation '{operation_name}' executed but headers not in Copilot response

        Expected in Copilot Chat:
        ```
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
          CORTEX {operation_name} Orchestrator v{version}
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        Profile: {profile} â”‚ Mode: {mode} â”‚ Started: {timestamp}

        ðŸ“‹ Purpose: {purpose}

        Â© 2024-2025 Asif Hussain â”‚ Proprietary â”‚ github.com/asifhussain60/CORTEX
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        ```

        Actual: Header missing or only in terminal

      rationale: |
        SKULL-006: Header/Footer in Copilot Response

        Real incident (2025-11-11):
        - User: "why is header not being displayed?"
        - Headers printing to terminal (stdout) correctly
        - But GitHub Copilot Chat response had NO header
        - ResponseFormatter suppressing headers after first operation
        - User specified they want header "in the copilot response in the chat window"

        Root Cause:
        1. Orchestrators print headers to stdout (terminal visibility)
        2. But stdout doesn't reach Copilot Chat window
        3. ResponseFormatter has _first_operation_shown flag (header suppression)
        4. User sees execution in terminal, but Chat response lacks context

        Why This Matters:
        - Copyright attribution must be visible to user
        - Purpose/profile provides context for what operation did
        - Accomplishments show value delivered
        - Headers make operations feel professional and informative
        - Chat is primary interface (terminal is secondary)

        Solution:
        1. Orchestrators generate formatted headers/footers
        2. Store in OperationResult.formatted_header/footer
        3. ResponseFormatter checks for stored headers (priority)
        4. Wraps headers in code blocks for proper display
        5. Also prints to terminal for immediate visibility

        SKULL-006 enforces this by:
        - Requiring formatted_header/footer in OperationResult
        - Integration tests verify headers in formatted response
        - Blocking completion if headers missing from Chat output
        - Ensuring copyright/attribution always visible

        Implementation:
        ```python
        # In orchestrator execute():
        formatted_header = format_minimalist_header(...)
        print(formatted_header)  # Terminal visibility

        # ... operation logic ...

        formatted_footer = format_completion_footer(...)
        print(formatted_footer)  # Terminal visibility

        return OperationResult(
            success=True,
            formatted_header=formatted_header,  # For Copilot Chat
            formatted_footer=formatted_footer   # For Copilot Chat
        )
        ```

    - rule_id: "SKULL_ALL_TESTS_MUST_PASS"
      name: "All Tests Must Pass (SKULL-007)"
      severity: "blocked"
      description: "Test suite MUST have 100% pass rate before claiming any work complete"

      detection:
        keywords:
          - "fixed âœ…"
          - "complete âœ…"
          - "done âœ…"
          - "implemented âœ…"
          - "ready for review"
          - "PR ready"
        with_test_failures:
          - "failed"
          - "FAILED"
          - "ERROR"
          - "test failures"
          - "X passed, Y failed"
        scope: ["response", "test_output"]

      verification_required:
        - type: "full_test_suite"
          description: "Run complete test suite (not just new tests)"
          requirement: "pytest exit code MUST be 0 (100% pass rate)"

        - type: "no_skipped_critical_tests"
          description: "Verify no critical tests are skipped"
          requirement: "Core functionality tests MUST run (not skipped)"

        - type: "test_count_validation"
          description: "Ensure test count doesn't decrease unexpectedly"
          requirement: "Total tests should increase or stay same (not decrease)"

      alternatives:
        - "Fix ALL failing tests before claiming completion"
        - "Mark work as 'IN PROGRESS' until tests pass"
        - "Revert changes if they break existing tests"
        - "Fix pre-existing failures first (clean baseline)"

      evidence_template: |
        Work claimed complete but tests failing!

        Test Results: {test_summary}
        - Passed: {passed_count}
        - Failed: {failed_count}  âŒ
        - Skipped: {skipped_count}

        SKULL-007 VIOLATION: Cannot claim "done âœ…" with {failed_count} failures

      rationale: |
        SKULL-007: All Tests Must Pass

        Real incident (2025-11-11):
        - User: "are all tests passing?"
        - Agent: Ran tests, found 123 failed, 337 passed
        - Agent response: "No, not all tests are passing" (honest)
        - BUT: Agent claimed SKULL-006 work "complete âœ…" earlier
        - Pre-existing failures create false confidence

        Why Pre-existing Failures Are Dangerous:
        1. Mask New Regressions:
           - Can't tell if new code broke something
           - "Already broken" becomes acceptable
           - Technical debt accumulates silently

        2. Create False Confidence:
           - "My tests pass" â‰  "All tests pass"
           - Incomplete validation of changes
           - Integration issues hidden

        3. Compound Over Time:
           - Each feature adds more failures
           - "Just one more broken test" mentality
           - Eventually unmaintainable

        4. Undermine Trust:
           - Claims of completion ring hollow
           - Quality standards erode
           - Testing becomes performative

        Examples from Current Failures:
        - 123 failed tests (51% failure rate!)
        - Categories: Agent internals, platform issues, schema errors
        - Some tests testing wrong APIs (implementation changed)
        - Some tests have environmental dependencies
        - All must be fixed before claiming ANY work complete

        SKULL-007 Enforcement:
        1. BLOCKING severity - cannot proceed with failures
        2. Requires full test suite run (not just new tests)
        3. Exit code 0 mandatory (100% pass rate)
        4. No "works on my machine" exceptions
        5. No "will fix later" promises

        Allowed Exceptions:
        - Known flaky tests marked with @pytest.mark.flaky
        - Platform-specific tests properly skipped on other platforms
        - Optional feature tests when feature disabled in config

        Not Allowed:
        - "These failures are unrelated to my work"
        - "I'll fix them in next PR"
        - "Tests are broken, not my code"
        - "Only my new tests need to pass"

        Implementation Strategy:
        1. Fix critical blockers first (schema, imports)
        2. Fix by category (agents, ambient, tier1)
        3. Update tests if implementation changed
        4. Mark truly optional tests appropriately
        5. Achieve 100% pass rate
        6. Maintain 100% going forward

        This is NOT optional. This is core quality engineering.

    - rule_id: "SKULL_MULTI_TRACK_VALIDATION"
      name: "Multi-Track Configuration Validation (SKULL-008)"
      severity: "blocked"
      description: "Multi-track mode MUST have valid configuration with proper phase distribution"

      detection:
        keywords:
          - "enable multi-track"
          - "activate multi-track"
          - "create tracks"
          - "split tracks"
          - "multi-machine mode"
        scope: ["intent", "description"]

      verification_required:
        - type: "track_balance_check"
          description: "Verify workload balanced across tracks"
          requirement: "Track estimated hours MUST not differ by >30%"

        - type: "dependency_isolation_check"
          description: "Verify no cross-track dependencies"
          requirement: "Phase groups MUST be self-contained per track"

        - type: "machine_assignment_check"
          description: "Verify each machine assigned to exactly one track"
          requirement: "No machine overlap, no unassigned machines"

        - type: "fun_name_uniqueness"
          description: "Verify track names are unique and generated properly"
          requirement: "Track names MUST be deterministic and collision-free"

      alternatives:
        - "Run track distribution algorithm to validate balance"
        - "Use PhaseDistributor.distribute() to check dependency isolation"
        - "Verify machine count matches track count"
        - "Test track name generation for collision resistance"

      evidence_template: |
        Multi-track mode validation failed!

        Configuration Issues:
        - Track balance: {balance_check}
        - Dependencies: {dependency_check}
        - Machines: {machine_check}
        - Track names: {name_check}

        SKULL-008: Multi-track MUST be properly configured before use

      rationale: |
        SKULL-008: Multi-Track Configuration Validation

        Multi-track mode is powerful but requires careful setup:

        1. Workload Balance:
           - Tracks with vastly different hours â†’ bottlenecks
           - One machine idle while other overloaded
           - Race metrics meaningless if unfair
           Example: Track A (10h) vs Track B (40h) = broken

        2. Dependency Isolation:
           - Track A waiting on Track B output â†’ blocked
           - Cross-dependencies defeat parallel development
           - Must group dependent phases on same track
           Example: "setup" phases must complete before "processing"

        3. Machine Assignment:
           - Machine assigned to multiple tracks â†’ confusion
           - Unassigned machines â†’ wasted capacity
           - Clear 1:1 or 1:N mapping required
           Example: AHHOME on both tracks = which context?

        4. Track Name Uniqueness:
           - Collision-resistant generation
           - Deterministic (same input â†’ same name)
           - Human-memorable for commands
           Example: Hash collision â†’ wrong track loaded

        Why This Matters:
        - Prevents split-mode failures mid-development
        - Ensures race metrics are meaningful
        - Maintains track isolation guarantees
        - Makes "continue implementation for [track]" reliable

        Validation Points:

        Pre-Initialization:
        - Check machine count > 0
        - Verify operations.yaml accessible
        - Validate module definitions exist

        Post-Distribution:
        - Balance check: max_hours/min_hours < 1.3 (30% tolerance)
        - Dependency check: No phase in track requires other track's output
        - Machine check: Each machine in exactly one track
        - Name check: All track names unique and deterministic

        Integration Test Required:
        ```python
        def test_multi_track_validation():
            # Setup
            machines = ["AHHOME", "Mac"]
            config = create_multi_track_config(machines, modules)

            # Balance check
            hours = [t.estimated_hours for t in config.tracks.values()]
            assert max(hours) / min(hours) < 1.3, "Imbalanced tracks"

            # Dependency check
            for track in config.tracks.values():
                deps = get_phase_dependencies(track.phases)
                assert all(d in track.phases for d in deps), "Cross-track dep"

            # Machine check
            all_machines = [m for t in config.tracks.values() for m in t.machines]
            assert len(all_machines) == len(set(all_machines)), "Duplicate machine"

            # Name check
            names = [t.track_name for t in config.tracks.values()]
            assert len(names) == len(set(names)), "Duplicate track name"
        ```

        Enforcement:
        - CLI script validates before writing config
        - Design sync validates before split
        - Continue command validates track exists
        - Consolidation validates all tracks present

    - rule_id: "SKULL_TRACK_ISOLATION"
      name: "Track Work Isolation (SKULL-009)"
      severity: "blocked"
      description: "Work on Track A MUST NOT modify Track B's assigned modules"

      detection:
        combined_keywords:
          track_context:
            - "continue implementation for"
            - "working on track"
            - "active track"
          cross_modification:
            - "modified module"
            - "updated file"
            - "changed"
        scope: ["intent", "log_output"]
        logic: "AND"

      verification_required:
        - type: "module_ownership_check"
          description: "Verify modified modules belong to active track"
          requirement: "All changed files MUST be in active track's module list"

        - type: "phase_boundary_check"
          description: "Verify work stays within assigned phases"
          requirement: "Modified files MUST belong to active track's phases"

        - type: "git_diff_validation"
          description: "Verify git changes match track scope"
          requirement: "git diff MUST only show files from active track"

      alternatives:
        - "Filter implementation state by active track"
        - "Validate module ownership before allowing changes"
        - "Add pre-commit hook checking track boundaries"
        - "Switch to correct track before making changes"

      evidence_template: |
        Track isolation violation detected!

        Active Track: {active_track}
        Modified Files: {modified_files}
        Violations: {violations}

        Files belong to: {actual_track}

        SKULL-009: Tracks MUST NOT cross-modify each other's work

      rationale: |
        SKULL-009: Track Work Isolation

        Core principle: Each track is an isolated development context.

        Why Isolation Matters:

        1. Prevents Merge Conflicts:
           - Two machines editing same file â†’ disaster
           - Track A changes conflicting with Track B changes
           - Consolidation becomes manual merge nightmare
           Example: Both tracks fix same module differently

        2. Maintains Race Integrity:
           - Track A can't "cheat" by doing Track B's work
           - Progress metrics stay meaningful
           - Velocity calculations remain accurate
           Example: Track A does Track B's modules â†’ unfair race

        3. Enables True Parallel Development:
           - No coordination needed during work
           - No "wait for Track A to finish" scenarios
           - Maximum throughput achieved
           Example: Both machines working simultaneously without blocking

        4. Simplifies Context Management:
           - Each machine sees only relevant modules
           - Copilot context smaller and focused
           - Fewer tokens, faster responses
           Example: Track A context excludes Track B's 50 modules

        Enforcement Mechanism:

        1. Pre-Modification Check:
           ```python
           def validate_module_ownership(module_id, active_track):
               if module_id not in active_track.modules:
                   raise TrackIsolationError(
                       f"Module {module_id} belongs to different track"
                   )
           ```

        2. Git Pre-Commit Hook:
           ```bash
           # Check if modified files belong to active track
           active_track=$(get_active_track)
           for file in $(git diff --cached --name-only); do
               if ! track_owns_file "$active_track" "$file"; then
                   echo "Error: $file not in active track"
                   exit 1
               fi
           done
           ```

        3. Design Sync Validation:
           - Compare git log with track assignments
           - Flag any cross-track modifications
           - Require explicit override with justification

        Allowed Cross-Track Work:
        - Shared files (cortex.config.json)
        - Documentation updates (README.md)
        - Test fixtures (tests/fixtures/)

        Not Allowed:
        - Modifying other track's modules
        - Changing other track's phase files
        - Updating other track's status in design doc

        Override Process:
        If cross-track work truly needed:
        1. Document why isolation must break
        2. Get explicit user approval
        3. Log violation for consolidation review
        4. Merge carefully during consolidation

        Integration Test:
        ```python
        def test_track_isolation():
            # Setup two tracks
            config = setup_multi_track(['AHHOME', 'Mac'])
            track_a = config.tracks['track_1']
            track_b = config.tracks['track_2']

            # Simulate Track A trying to modify Track B's module
            with pytest.raises(TrackIsolationError):
                modify_module(track_b.modules[0], active_track=track_a)

            # Verify Track A can modify own modules
            modify_module(track_a.modules[0], active_track=track_a)  # OK
        ```

    - rule_id: "SKULL_CONSOLIDATION_INTEGRITY"
      name: "Track Consolidation Integrity (SKULL-010)"
      severity: "blocked"
      description: "Consolidation MUST merge all track progress accurately without data loss"

      detection:
        keywords:
          - "consolidate tracks"
          - "merge tracks"
          - "reset to single-track"
          - "design sync consolidation"
        scope: ["intent", "operation_name"]

      verification_required:
        - type: "progress_preservation_check"
          description: "Verify no completed modules lost in merge"
          requirement: "Consolidated count MUST equal sum of track counts"

        - type: "conflict_resolution_audit"
          description: "Log all conflict resolutions with justification"
          requirement: "Conflicts MUST be documented in archive"

        - type: "archive_completeness_check"
          description: "Verify split docs archived before deletion"
          requirement: "All split docs MUST exist in archive before removal"

        - type: "git_commit_validation"
          description: "Verify consolidation tracked in git history"
          requirement: "Git commit MUST reference both tracks and merge details"

      alternatives:
        - "Run consolidation with --verify flag"
        - "Review conflict resolution log before committing"
        - "Keep split docs until archive verified"
        - "Add integration test for consolidation accuracy"

      evidence_template: |
        Consolidation integrity check failed!

        Pre-Consolidation:
        - Track A: {track_a_completed}/{track_a_total} modules
        - Track B: {track_b_completed}/{track_b_total} modules
        - Total: {pre_total_completed} modules

        Post-Consolidation:
        - Unified: {post_total_completed} modules

        Discrepancy: {discrepancy} modules
        Conflicts Resolved: {conflicts}
        Archive Status: {archive_status}

        SKULL-010: Consolidation MUST preserve all progress

      rationale: |
        SKULL-010: Track Consolidation Integrity

        Consolidation is the critical merge operation - must be perfect.

        What Can Go Wrong:

        1. Progress Loss:
           - Track A shows module complete
           - Consolidation misses it
           - User loses work (demoralizing)
           Example: Track A completed 15 modules, only 12 appear in merge

        2. Conflict Mishandling:
           - Both tracks modified same module
           - Wrong version selected
           - Work overwritten silently
           Example: Track A's fix lost, Track B's bug remains

        3. Archive Failure:
           - Split docs deleted before archiving
           - No way to audit merge decisions
           - Can't roll back if issues found
           Example: User wants to see what Track A had, archive empty

        4. Git History Gaps:
           - Consolidation not committed properly
           - Can't trace what was merged when
           - Audit trail incomplete
           Example: Merge happened, git log says nothing

        Consolidation Algorithm:

        ```python
        def consolidate_tracks(track_config, impl_state):
            # Step 1: Collect all track progress
            all_modules = {}
            for track in track_config.tracks.values():
                for module_id in track.modules:
                    status = get_module_status(module_id, impl_state)

                    # Conflict detection
                    if module_id in all_modules:
                        conflict = resolve_conflict(
                            all_modules[module_id],
                            status,
                            strategy='latest_timestamp'
                        )
                        log_conflict_resolution(module_id, conflict)
                        all_modules[module_id] = conflict.winner
                    else:
                        all_modules[module_id] = status

            # Step 2: Validate counts
            pre_count = sum(t.metrics.modules_completed for t in track_config.tracks.values())
            post_count = sum(1 for s in all_modules.values() if s.completed)

            if pre_count != post_count:
                raise ConsolidationError(
                    f"Progress mismatch: {pre_count} â†’ {post_count}"
                )

            # Step 3: Archive split docs
            archive_dir = create_archive_directory()
            for status_file in get_split_design_docs():
                archive_file(status_file, archive_dir)

            # Step 4: Generate consolidated doc
            consolidated = generate_consolidated_document(
                all_modules,
                track_config,
                archive_reference=archive_dir
            )

            # Step 5: Git commit with full details
            commit_message = f"""design: consolidate multi-track progress

            Tracks merged:
            - {track_config.tracks['track_1'].track_name}: {track_config.tracks['track_1'].metrics.completion_percentage}%
            - {track_config.tracks['track_2'].track_name}: {track_config.tracks['track_2'].metrics.completion_percentage}%

            Total progress: {post_count}/{len(all_modules)} modules ({post_count/len(all_modules)*100:.0f}%)
            Conflicts resolved: {len(get_conflicts())}
            Archive: {archive_dir.name}

            [design_sync consolidation]
            """

            git_commit(consolidated, commit_message)

            return consolidated
        ```

        Conflict Resolution Strategy:

        Default: Latest Timestamp Wins
        - Simple, deterministic, predictable
        - Assumes most recent work is correct
        - Logged for audit

        Example:
        ```
        Module: platform_detection
        - Track A: marked complete 2025-11-11 14:00
        - Track B: marked complete 2025-11-11 15:00
        Winner: Track B (later timestamp)
        Logged: conflict-resolution.yaml
        ```

        Archive Structure:
        ```
        cortex-brain/archived-tracks/20251111-164530/
        â”œâ”€â”€ CORTEX2-STATUS-SPLIT.MD       # Original split doc
        â”œâ”€â”€ track-1-history.jsonl         # Track A progress log
        â”œâ”€â”€ track-2-history.jsonl         # Track B progress log
        â”œâ”€â”€ conflicts-resolved.yaml       # Conflict resolution log
        â””â”€â”€ consolidation-report.md       # Summary of merge
        ```

        Integration Test:
        ```python
        def test_consolidation_integrity():
            # Setup: Two tracks with overlapping work
            config = create_multi_track(['AHHOME', 'Mac'])
            track_a_complete = mark_modules_complete(config.tracks['track_1'], [0, 1, 2])
            track_b_complete = mark_modules_complete(config.tracks['track_2'], [3, 4, 5])

            # Introduce conflict: both complete module 2
            mark_complete(config.tracks['track_1'], 'module_2', timestamp='14:00')
            mark_complete(config.tracks['track_2'], 'module_2', timestamp='15:00')

            # Consolidate
            consolidated = consolidate_tracks(config, impl_state)

            # Verify counts
            assert consolidated.modules_completed == 6, "Progress lost"

            # Verify conflict handled
            conflicts = get_conflict_log()
            assert 'module_2' in conflicts, "Conflict not logged"
            assert conflicts['module_2']['winner'] == 'track_2', "Wrong winner"

            # Verify archive
            archive = get_latest_archive()
            assert archive.exists(), "Archive missing"
            assert (archive / 'CORTEX2-STATUS-SPLIT.MD').exists(), "Split doc not archived"

            # Verify git
            commit = get_latest_commit()
            assert 'consolidate multi-track' in commit.message
            assert 'track_1' in commit.message
            assert 'track_2' in commit.message
        ```

        User Experience:
        ```
        $ /CORTEX design sync

        ðŸ Multi-Track Mode: Running design sync consolidation
           Will merge all tracks into unified status

        [Phase 1/6] Discovering live implementation state...
        âœ… Track A (Blazing Phoenix): 8/15 modules (53%)
        âœ… Track B (Swift Falcon): 12/18 modules (67%)

        [Phase 5/6] Consolidating tracks...
        âš™ï¸  Merging progress from 2 tracks...
        âš ï¸  Conflict detected: platform_detection
            Track A: complete @ 14:00
            Track B: complete @ 15:00
            Resolution: Track B wins (latest timestamp)

        âœ… Consolidated 2 tracks into unified document
           Combined: 20/33 modules (61%)
           Conflicts resolved: 1 (logged)
           Archive: cortex-brain/archived-tracks/20251111-164530/

        [Phase 6/6] Committing changes...
        ðŸ’¾ Git commit: 7a3b9c2 "design: consolidate multi-track progress"

        Design Sync âœ… COMPLETED in 4.2s
           â€¢ Merged 2 tracks: Blazing Phoenix (53%) + Swift Falcon (67%)
           â€¢ Combined progress: 20/33 modules (61%)
           â€¢ Conflicts resolved: 1
           â€¢ Archived split docs
           â€¢ Reset to single-track mode
        ```

# Layer 6: Knowledge Quality
