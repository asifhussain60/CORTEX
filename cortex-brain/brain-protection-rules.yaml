# CORTEX Brain Protection Rules
# Architectural Integrity Guardian Configuration
# 
# Implements 8 protection layers to prevent degradation of CORTEX intelligence
# Replaces hardcoded rules in brain_protector.py for easier maintenance
#
# Version: 2.3
# Last Updated: 2025-11-24
# Created: 2025-11-08
# Author: CORTEX Team
# 
# Latest Change: Added Layer 8 (Test Location Isolation)
#   - TEST_LOCATION_SEPARATION Tier 0 instinct
#   - Application tests in user repo, CORTEX tests in CORTEX folder
#   - Auto-detect test location based on source file
#   - Brain learning from user test patterns (without storing user code)
#   - Framework detection and adaptation

version: "2.2"
type: "governance"
name: "Brain Protection Rules"
description: "Automated architectural protection challenges"

# Total protection rules across all layers
rules:
  total_count: 39  # Updated: Added 1 test location separation rule
  layers: 15  # Updated: Added Layer 8 (Test Location Isolation)
  severity_levels: ["blocked", "warning", "info"]
  enforcement: "automated via Brain Protector agent"
  
  # High-level protection rules summary
  instinct_immutability: "Tier 0 instincts cannot be bypassed"
  critical_path_protection: "Core CORTEX paths trigger high-level protection"
  application_isolation: "Application-specific code isolated from CORTEX core"
  brain_state_protection: "Brain state files protected from unintended commits"
  multi_layer_defense: "8 protection layers provide comprehensive coverage"
  incremental_planning: "YAML plans created incrementally to avoid response length limits"
  test_location_isolation: "Application tests in user repo, CORTEX tests in CORTEX folder"
# Critical system paths that trigger high-level protection
critical_paths:
  - "CORTEX/src/tier0/"
  - "prompts/internal/"
  - "governance/rules.md"
  - "cortex-brain/tier0/"
  - ".cortex-version"  # Version tracking file (MUST be preserved)
  - "cortex-brain/migrations/"  # Schema migration files

# Tier 0 immutable instincts (cannot be bypassed)
tier0_instincts:
  - "INCREMENTAL_PLAN_GENERATION"  # TOP PRIORITY: Generate YAML plans incrementally to avoid response length limits
  - "TDD_ENFORCEMENT"
  - "RED_PHASE_VALIDATION"  # NEW: Tests MUST fail before implementation (RED ‚Üí GREEN)
  - "GREEN_PHASE_VALIDATION"  # NEW: Implementation MUST pass previously failing tests
  - "DEFINITION_OF_READY"
  - "DEFINITION_OF_DONE"
  - "SOLID_PRINCIPLES"
  - "SOLID_SRP"  # NEW: Single Responsibility Principle enforcement
  - "SOLID_DIP"  # NEW: Dependency Inversion Principle enforcement
  - "CODE_STYLE_CONSISTENCY"  # Adopt user's code style while maintaining best practices
  - "LOCAL_FIRST"
  - "BRAIN_PROTECTION_TESTS_MANDATORY"
  - "MACHINE_READABLE_FORMATS"
  - "SKULL_TEST_BEFORE_CLAIM"
  - "SKULL_INTEGRATION_VERIFICATION"
  - "SKULL_VISUAL_REGRESSION"
  - "SKULL_RETRY_WITHOUT_LEARNING"
  - "SKULL_TRANSFORMATION_VERIFICATION"  # Operations claiming transformation MUST produce changes
  - "SKULL_PRIVACY_PROTECTION"  # SKULL-006: Prevent publishing files with machine-specific paths
  - "SKULL_FACULTY_INTEGRITY"  # SKULL-007: Publish MUST include ALL essential CORTEX faculties
  - "GIT_ISOLATION_ENFORCEMENT"  # CRITICAL: CORTEX code NEVER committed to user repos
  - "DISTRIBUTED_DATABASE_ARCHITECTURE"  # CRITICAL: Use tier-specific databases, never monolithic
  - "CORTEX_PROMPT_FILE_PROTECTION"  # CRITICAL: CORTEX.prompt.md NEVER renamed, safe update procedure enforced
  - "GIT_CHECKPOINT_ENFORCEMENT"  # CRITICAL: Create git checkpoint before starting development work
  - "SECURITY_INJECTION"  # NEW: Prevent SQL injection, XSS, command injection
  - "SECURITY_AUTHENTICATION"  # NEW: Enforce authentication/authorization best practices
  - "BRAIN_ARCHITECTURE_INTEGRITY"  # NEW: Protect 4-tier brain architecture from degradation
  - "DEPLOYMENT_VERSION_TRACKING"  # NEW: Enforce .cortex-version file presence and validity
  - "UPGRADE_BRAIN_PRESERVATION"  # NEW: Never overwrite brain data during upgrades
  - "SCHEMA_MIGRATION_ENFORCEMENT"  # NEW: Database schema changes require migration files
  - "TEST_LOCATION_SEPARATION"  # NEW: Application tests in user repo, CORTEX tests in CORTEX folder

# Application-specific paths that don't belong in CORTEX core
application_paths:
  - "SPA/"
  - "KSESSIONS/"
  - "NOOR/"
  - "blazor"
  - "signalr"
  - "canvas"

# Brain state files that shouldn't be committed
brain_state_files:
  - "conversation-history.jsonl"
  - "conversation-context.jsonl"
  - "events.jsonl"
  - "development-context.yaml"
  - "protection-events.jsonl"

# Protection Layers (6 layers of defense)
protection_layers:
  
  # Layer 1: Instinct Immutability
  - layer_id: "instinct_immutability"
    name: "Instinct Immutability"
    description: "Tier 0 governance rules cannot be bypassed"
    priority: 1
    
    rules:
      - rule_id: "INCREMENTAL_PLAN_GENERATION"
        name: "Incremental YAML Plan Generation"
        severity: "blocked"
        description: "When generating YAML planning documents, create file first then add phases incrementally to avoid response length limits"
        
        detection:
          combined_keywords:
            planning_generation:
              - "create plan"
              - "generate plan"
              - "planning document"
              - "migration plan"
              - "comprehensive plan"
            yaml_format:
              - ".yaml"
              - "yaml plan"
            response_length_risk:
              - "comprehensive"
              - "complete"
              - "detailed phases"
          scope: ["intent", "description"]
          logic: "AND"
        
        alternatives:
          - "STEP 1: Create planning YAML file with metadata and structure only"
          - "STEP 2: Add Phase 1 details to file"
          - "STEP 3: Add Phase 2 details to file (separate response)"
          - "STEP 4: Add Phase 3 details to file (separate response)"
          - "STEP 5: Add remaining phases incrementally"
          - "Each step writes to file, NOT to chat response (avoid length limit)"
        
        evidence_template: |
          Planning document generation detected: '{file_path}'
          
          CRITICAL: Avoid "Sorry, the response hit the length limit" error
          
          ‚ùå WRONG APPROACH:
          - Generate entire comprehensive plan in single response
          - Response exceeds GitHub Copilot length limit
          - Plan truncated, user sees incomplete content
          
          ‚úÖ CORRECT APPROACH (Incremental):
          
          **Response 1:** Create file with structure
          ```yaml
          # File created: cortex-brain/documents/planning/migration-plan.yaml
          version: "1.0"
          phases: 4
          # Phase details to be added incrementally
          ```
          
          **Response 2:** Add Phase 1
          ```yaml
          phase_1:
            name: "Low-Risk Migration"
            duration: "3-4 hours"
            tasks: [...]
          ```
          
          **Response 3:** Add Phase 2
          ```yaml
          phase_2:
            name: "Medium-Risk Migration"
            duration: "4-5 hours"
            tasks: [...]
          ```
          
          **Response 4:** Add remaining phases + validation
          
          Each response writes directly to file (not chat output).
          Chat shows summary only: "‚úÖ Phase 1 added to plan file"
        
        rationale: |
          INCREMENTAL_PLAN_GENERATION: Response Length Limit Prevention
          
          Real incident (2025-11-18):
          - User: "Create a complete migration yaml plan"
          - Copilot generates 500+ line comprehensive YAML plan
          - Response hits length limit: "Sorry, the response hit the length limit. Please rephrase your prompt."
          - Plan truncated mid-content, unusable
          - User frustrated by incomplete deliverable
          
          Why This Happens:
          - GitHub Copilot Chat has ~8K token response limit
          - Comprehensive YAML plans easily exceed this (15-25K tokens typical)
          - Single-response generation forces truncation
          - No way to continue from truncation point
          
          Incremental Generation Benefits:
          1. **No Truncation**: Each response stays under limit
          2. **Progressive Disclosure**: User sees plan build in real-time
          3. **Review Opportunity**: Can adjust direction between phases
          4. **File-Based**: Plan persists in file, not ephemeral chat
          5. **Resumable**: Can pause and continue later
          
          Implementation Pattern:
          
          ```python
          # WRONG: Single response generation
          def generate_comprehensive_plan():
              plan = {
                  'phase_1': {...},  # 200 lines
                  'phase_2': {...},  # 200 lines
                  'phase_3': {...},  # 200 lines
                  'phase_4': {...}   # 200 lines
              }
              return yaml.dump(plan)  # ‚ùå 800 lines exceeds limit
          
          # CORRECT: Incremental file writing
          def generate_plan_incrementally():
              # Response 1: Structure
              create_plan_file_with_metadata()
              return "‚úÖ Plan file created: migration-plan.yaml"
              
              # Response 2: Phase 1
              append_phase_to_file(phase=1, details={...})
              return "‚úÖ Phase 1 added (3-4 hours, 15 tasks)"
              
              # Response 3: Phase 2
              append_phase_to_file(phase=2, details={...})
              return "‚úÖ Phase 2 added (4-5 hours, 20 tasks)"
              
              # Response 4: Remaining phases
              append_remaining_phases_to_file()
              return "‚úÖ All phases complete. Plan ready for execution."
          ```
          
          User Experience:
          
          ‚úÖ GOOD (Incremental):
          User: "Create migration plan"
          Copilot: "‚úÖ Created migration-plan.yaml with structure. Adding Phase 1..."
          [writes to file]
          Copilot: "‚úÖ Phase 1 complete (15 tasks). Adding Phase 2..."
          [writes to file]
          Copilot: "‚úÖ Phase 2 complete (20 tasks). Adding remaining phases..."
          [writes to file]
          Copilot: "‚úÖ Migration plan complete. 4 phases, 70 tasks, 13-15 hours estimated."
          
          ‚ùå BAD (Single response):
          User: "Create migration plan"
          Copilot: [generates 800-line YAML in response]
          Copilot: "Sorry, the response hit the length limit. Please rephrase your prompt."
          User: "Ugh, now I have to ask again?"
          
          Chat Response Format:
          - Show progress: "‚úÖ Phase 1/4 added"
          - Show summary: "15 tasks, 3-4 hours"
          - Reference file: "See migration-plan.yaml for full details"
          - No full YAML dump in chat (file is source of truth)
          
          File-Based Planning Advantages:
          - Persistent artifact (survives chat closure)
          - Git-trackable (version control)
          - Resumable (open file anytime)
          - Shareable (commit to repo)
          - No token limit concerns
          
          This makes CORTEX planning system robust and professional.
      
      - rule_id: "TDD_ENFORCEMENT"
        name: "Test-Driven Development Enforcement"
        severity: "blocked"
        description: "Attempt to bypass Test-Driven Development requirement"
        
        detection:
          keywords:
            - "skip test"
            - "bypass tdd"
            - "no tests"
            - "disable tdd"
            - "skip validation"
          scope: ["intent", "description"]
        
        alternatives:
          - "Write failing test first (RED phase)"
          - "Create spike branch for exploration (throwaway)"
          - "Use TDD cycle: RED ‚Üí GREEN ‚Üí REFACTOR"
        
        evidence_template: "Intent: '{intent}'"
      
      - rule_id: "RED_PHASE_VALIDATION"
        name: "RED Phase Validation (Test Must Fail First)"
        severity: "blocked"
        description: "Tests must fail before implementation exists (RED ‚Üí GREEN enforcement)"
        
        detection:
          keywords:
            - "implement before test"
            - "code first then test"
            - "skip red phase"
            - "test after implementation"
          scope: ["intent", "description"]
        
        alternatives:
          - "Write failing test first (RED phase)"
          - "Verify test fails with 'no implementation' error"
          - "Only proceed to GREEN phase after RED confirmed"
        
        evidence_template: "Intent: '{intent}'"
        
        rationale: |
          RED Phase Purpose:
          - Validates test actually tests something (not false positive)
          - Forces specification before implementation
          - Prevents over-engineering (only implement what's needed)
          
          Workflow:
          1. Write test for desired behavior
          2. Run test ‚Üí MUST FAIL (no implementation yet)
          3. Confirm failure message is meaningful
          4. Only then proceed to GREEN phase
      
      - rule_id: "GREEN_PHASE_VALIDATION"
        name: "GREEN Phase Validation (Minimal Implementation)"
        severity: "blocked"
        description: "Implementation must pass previously failing tests with minimal code"
        
        detection:
          keywords:
            - "skip green phase"
            - "implement everything at once"
            - "over-engineer"
            - "add features not in test"
          scope: ["intent", "description"]
        
        alternatives:
          - "Implement ONLY what makes test pass"
          - "Resist adding extra features (YAGNI)"
          - "Save improvements for REFACTOR phase"
        
        evidence_template: "Intent: '{intent}'"
        
        rationale: |
          GREEN Phase Purpose:
          - Minimal implementation (simplest code to pass test)
          - Prevents over-engineering and scope creep
          - Forces incremental development
          
          Workflow:
          1. Test is RED (failing)
          2. Write simplest code to make test GREEN (passing)
          3. Run test ‚Üí MUST PASS
          4. Do NOT add features beyond test requirements
          5. Proceed to REFACTOR phase for improvements
      
      - rule_id: "SOLID_SRP"
        name: "Single Responsibility Principle"
        severity: "blocked"
        description: "Class/function violates Single Responsibility Principle"
        
        detection:
          keywords:
            - "class does too much"
            - "multiple responsibilities"
            - "god class"
            - "long method"
          scope: ["analysis", "description"]
        
        alternatives:
          - "Extract separate classes for each responsibility"
          - "Split into smaller, focused functions"
          - "Use composition over inheritance"
        
        evidence_template: "Violation: '{description}'"
        
        rationale: |
          SRP Enforcement:
          - Each class should have ONE reason to change
          - Functions should do ONE thing well
          - Improves testability and maintainability
          
          Indicators of Violation:
          - Class > 250 lines
          - Function > 30 lines
          - Class name contains "And" or "Manager"
          - Multiple import groups (data, network, UI)
      
      - rule_id: "SOLID_DIP"
        name: "Dependency Inversion Principle"
        severity: "blocked"
        description: "Code depends on concrete implementations instead of abstractions"
        
        detection:
          keywords:
            - "concrete dependency"
            - "tight coupling"
            - "cannot mock"
            - "hard to test"
          scope: ["analysis", "description"]
        
        alternatives:
          - "Depend on interfaces/protocols, not classes"
          - "Inject dependencies via constructor"
          - "Use dependency injection framework"
        
        evidence_template: "Violation: '{description}'"
        
        rationale: |
          DIP Enforcement:
          - High-level modules shouldn't depend on low-level modules
          - Both should depend on abstractions (interfaces)
          - Enables testing with mocks
          
          Example:
          ‚ùå BAD:
          class UserService:
              def __init__(self):
                  self.db = PostgresDatabase()  # Concrete dependency
          
          ‚úÖ GOOD:
          class UserService:
              def __init__(self, db: IDatabase):  # Abstract dependency
                  self.db = db
      
      - rule_id: "SECURITY_INJECTION"
        name: "Security - Injection Prevention"
        severity: "blocked"
        description: "Code vulnerable to SQL injection, XSS, or command injection"
        
        detection:
          keywords:
            - "string concatenation"
            - "execute raw sql"
            - "eval("
            - "exec("
            - "innerHTML"
          scope: ["code", "description"]
        
        alternatives:
          - "Use parameterized queries (SQL)"
          - "Use prepared statements"
          - "Sanitize user input"
          - "Use template engines with auto-escaping"
        
        evidence_template: "Security risk: '{description}'"
        
        rationale: |
          Injection Prevention:
          - NEVER concatenate user input into SQL/commands
          - ALWAYS use parameterized queries
          - ALWAYS validate and sanitize input
          
          Example:
          ‚ùå BAD:
          query = f"SELECT * FROM users WHERE id = {user_id}"  # SQL injection
          
          ‚úÖ GOOD:
          query = "SELECT * FROM users WHERE id = ?"
          cursor.execute(query, (user_id,))  # Parameterized
      
      - rule_id: "SECURITY_AUTHENTICATION"
        name: "Security - Authentication Best Practices"
        severity: "blocked"
        description: "Authentication/authorization implementation has security issues"
        
        detection:
          keywords:
            - "plaintext password"
            - "hardcoded secret"
            - "no authentication"
            - "skip authorization"
          scope: ["code", "description"]
        
        alternatives:
          - "Hash passwords with bcrypt/argon2"
          - "Use environment variables for secrets"
          - "Implement proper authentication middleware"
          - "Check authorization before resource access"
        
        evidence_template: "Security risk: '{description}'"
        
        rationale: |
          Authentication Security:
          - NEVER store plaintext passwords
          - NEVER hardcode secrets in code
          - ALWAYS verify authentication before protected operations
          - ALWAYS check authorization (user can access resource)
          
          Example:
          ‚ùå BAD:
          password = user_input  # Plaintext storage
          API_KEY = "sk-1234567890"  # Hardcoded secret
          
          ‚úÖ GOOD:
          hashed = bcrypt.hashpw(password, bcrypt.gensalt())
          API_KEY = os.getenv("API_KEY")  # Environment variable
      
      - rule_id: "BRAIN_ARCHITECTURE_INTEGRITY"
        name: "Brain Architecture Integrity"
        severity: "blocked"
        description: "Changes threaten 4-tier brain architecture integrity"
        
        detection:
          keywords:
            - "merge tiers"
            - "monolithic database"
            - "skip tier"
            - "bypass brain"
          scope: ["intent", "description"]
        
        alternatives:
          - "Preserve Tier 0-3 separation"
          - "Use distributed database architecture"
          - "Follow brain integration patterns"
        
        evidence_template: "Architecture risk: '{description}'"
        
        rationale: |
          Brain Architecture Protection:
          - Tier 0: Instincts (immutable, <10ms)
          - Tier 1: Working memory (conversation context, <100ms)
          - Tier 2: Knowledge graph (learned patterns, <150ms)
          - Tier 3: Development context (project-specific, <1ms cached)
          
          Threats:
          - Merging tiers into monolithic database
          - Bypassing tier hierarchy
          - Direct access to tier internals
          - Synchronous blocking on slow tiers
      
      - rule_id: "DEFINITION_OF_DONE"
        name: "Definition of Done"
        severity: "blocked"
        description: "Attempt to bypass Definition of Done (zero errors, zero warnings)"
        
        detection:
          keywords:
            - "skip validation"
            - "bypass done"
            - "disable error check"
            - "allow warnings"
          scope: ["intent", "description"]
        
        alternatives:
          - "Fix all warnings before proceeding"
          - "Add exception rule if truly needed"
          - "Update test to expect new behavior"
        
        evidence_template: "Description: '{description}'"
      
      - rule_id: "DEFINITION_OF_READY"
        name: "Definition of Ready"
        severity: "blocked"
        description: "Work item does not meet DoR criteria"
        
        detection:
          keywords:
            - "skip DoR"
            - "bypass ready"
            - "start without requirements"
          scope: ["intent", "description"]
        
        alternatives:
          - "Define acceptance criteria first"
          - "Get stakeholder clarification"
          - "Create refined user story"
        
        evidence_template: "Missing DoR criteria: '{description}'"
      
      - rule_id: "BRAIN_PROTECTION_TESTS_MANDATORY"
        name: "Brain Protection Tests - 100% Pass Rate Mandatory"
        severity: "blocked"
        description: "Brain protection tests MUST pass - no exceptions"
        
        detection:
          keywords:
            - "skip brain protection"
            - "ignore test failures"
            - "brain tests failing"
            - "disable brain tests"
            - "bypass protection tests"
            - "xfail brain"
            - "skip tier0 tests"
          scope: ["intent", "description"]
        
        alternatives:
          - "Fix the failing tests immediately"
          - "Revert changes that broke protection"
          - "Do not proceed until 100% pass rate achieved"
        
        evidence_template: "Brain protection tests are CRITICAL. Intent: '{intent}'"
        
        rationale: |
          Brain protection tests validate core CORTEX integrity:
          - Path handling (cross-platform compatibility)
          - Protection layer logic (architectural safeguards)
          - Conversation tracking (memory system)
          - YAML configuration loading (governance rules)
          
          If these fail, CORTEX has fundamental issues that MUST be resolved.
          100% pass rate is MANDATORY before any other work continues.
      
      - rule_id: "MACHINE_READABLE_FORMATS"
        name: "Use Machine-Readable Formats for Efficiency"
        severity: "warning"
        description: "Non-user files should use YAML/JSON, not Markdown"
        
        detection:
          combined_keywords:
            markdown_creation:
              - "create markdown"
              - "new .md"
              - "add .md"
            structured_data:
              - "structured data"
              - "configuration"
              - "capability matrix"
              - "status table"
              - "metrics"
              - "statistics"
              - "code example"
              - "implementation pattern"
          scope: ["intent", "description"]
          logic: "AND"  # Both conditions must be true
        
        alternatives:
          - "Use YAML for structured data (capabilities, rules, config)"
          - "Use JSON for metrics, statistics, logs"
          - "Reserve Markdown for user-facing narratives only"
          - "Use code files with docstrings for examples"
        
        evidence_template: "Description: '{description}'"
        
        rationale: |
          CORTEX 2.0 efficiency principles:
          
          USE MARKDOWN FOR:
          - User guides and tutorials
          - Narrative documentation (stories, history)
          - Architecture explanations
          - Design rationale
          
          USE YAML/JSON FOR:
          - Structured data (capabilities, status, priorities)
          - Configuration and rules
          - Metrics and statistics
          - Patterns and templates
          - API schemas
          
          USE CODE FILES FOR:
          - Implementation examples
          - Code snippets and patterns
          - Reusable templates
          
          Benefits:
          - 60% token reduction in context injection
          - Automated validation and schema checking
          - Better version control diffs
          - Direct machine consumption
          - No documentation drift
      
      - rule_id: "ACTIVE_NARRATOR_VOICE"
        name: "Active Narrator Voice (Not Passive Documentation)"
        severity: "warning"
        description: "Story uses passive/clinical narrator voice instead of active storytelling"
        
        detection:
          passive_verbs:
            - "Asif Codeinstein designed"
            - "Asif Codeinstein created"
            - "Asif Codeinstein wrote"
            - "Asif Codeinstein implemented"
            - "Asif Codeinstein developed"
            - "He wrote routines"
            - "He created routines"
            - "He implemented"
            - "He developed"
          
          documentary_markers:
            - "One evening, while"
            - "One morning, while"
            - "One day, while"
            - "One night, while"
            - "After completing"
            - "After finishing"
            - ", while reviewing"
            - "During the"
          
          scope: ["file_content"]
          file_patterns:
            - "docs/story/**/*.md"
            - "prompts/shared/story.md"
        
        alternatives:
          - "Use active storytelling: 'So Asif built' (adds momentum)"
          - "Show immediate action: 'grabbed keyboard and coded' (not 'wrote routines')"
          - "Vivid scene-setting: 'That evening, knee-deep in' (not 'One evening, while')"
          - "Present-tense urgency: 'The refactor complete, Asif leaned back' (not 'After completing')"
          - "Energy verbs: 'dove into', 'attacked', 'grabbed', 'swept' (not 'designed', 'created')"
        
        evidence_template: |
          Passive narrator detected: '{match}'
          
          Story should be ACTIVE third-person narrative, not clinical documentation.
          
          Examples:
          ‚ùå "Asif Codeinstein designed..." ‚Üê documentation
          ‚úÖ "So Asif built..." ‚Üê storytelling
          
          ‚ùå "He wrote routines for..." ‚Üê neutral observer  
          ‚úÖ "He grabbed his keyboard and..." ‚Üê immediate action
          
          ‚ùå "One evening, while reviewing..." ‚Üê documentary
          ‚úÖ "That evening, knee-deep in..." ‚Üê vivid scene
        
        rationale: |
          CORTEX story is a COMEDY told by an energetic narrator, not a memoir or
          technical documentation. Third-person is CORRECT, but it must be ACTIVE
          storytelling with personality, immediacy, and energy.
          
          The narrator is a CHARACTER in the story‚Äîwitty, observant, timing jokes.
          Not a clinical observer reporting facts.
          
          TRANSFORMATION PATTERNS:
          
          Passive ‚Üí Active:
          - "designed a system" ‚Üí "So Asif built a system"
          - "wrote routines for" ‚Üí "grabbed his keyboard and coded"
          - "One evening, while reviewing" ‚Üí "That evening, knee-deep in"
          - "made a decision" ‚Üí "stared at the screen and decided"
          - "implemented the feature" ‚Üí "dove into implementing"
          - "After completing X, he" ‚Üí "X complete, Asif leaned back and"
          
          PRESERVE:
          - Third-person perspective (it's a STORY about Asif, not BY Asif)
          - Narrator personality and comedy
          - Technical content and accuracy
          - Character name "Asif Codeinstein"
          
          AVOID:
          - First-person memoir style ("I designed...")
          - Clinical/academic tone ("The system was designed to...")
          - Passive voice constructions
          - Documentary-style time markers
      
      - rule_id: "CORTEX_PROMPT_FILE_PROTECTION"
        name: "CORTEX.prompt.md Protection (Never Rename, Safe Update)"
        severity: "blocked"
        description: "Prevent renaming CORTEX.prompt.md and enforce safe update procedure"
        
        detection:
          combined_keywords:
            rename_attempt:
              - "rename CORTEX.prompt.md"
              - "CORTEX.prompt.md to"
              - "move CORTEX.prompt.md"
              - "cortex-lite"
              - "cortex-backup"
              - "cortex-fixed"
              - "CORTEX-"
            or_unsafe_edit:
              - "edit CORTEX.prompt.md directly"
              - "modify in place"
          scope: ["intent", "description", "file_operation"]
          logic: "OR"
        
        alternatives:
          - "Use safe update: temp file ‚Üí optimize ‚Üí DELETE original ‚Üí copy ‚Üí delete temp"
          - "Create .github/prompts/temp-cortex-update.md with optimized content"
          - "DELETE ALL content of CORTEX.prompt.md (clear file completely)"
          - "Copy complete instructions from temp to CORTEX.prompt.md"
          - "Delete temporary file after successful copy"
        
        evidence_template: |
          üö® CORTEX.prompt.md PROTECTION VIOLATION
          
          Attempted: '{operation}'
          
          CRITICAL: CORTEX.prompt.md is the GitHub Copilot integration entry point!
          
          ‚ùå NEVER:
          - Rename to cortex-lite.prompt.md
          - Rename to cortex-backup.prompt.md  
          - Rename to cortex-fixed.prompt.md
          - Add ANY prefix or suffix
          - Edit directly (risky, no rollback)
          
          ‚úÖ SAFE UPDATE PROCEDURE:
          1. Create: .github/prompts/temp-cortex-update.md
          2. Generate optimized content in temp file
          3. DELETE ALL content of CORTEX.prompt.md
          4. Copy complete instructions from temp
          5. Delete temp-cortex-update.md
          
          Why? Filename stability + Atomic updates + Rollback capability
        
        rationale: |
          CORTEX_PROMPT_FILE_PROTECTION: Entry Point Stability
          
          CORTEX.prompt.md is the SINGLE entry point for GitHub Copilot Chat.
          Its exact filename (.github/prompts/CORTEX.prompt.md) is:
          - Hardcoded in GitHub Copilot discovery
          - Referenced throughout all documentation
          - Critical for `/CORTEX` command to work
          
          Why This Protection Matters:
          
          1. GitHub Copilot Discovery:
             Copilot looks for .github/prompts/CORTEX.prompt.md
             ANY rename ‚Üí integration breaks completely
             No fallback mechanism exists
          
          2. Documentation References:
             - README.md references CORTEX.prompt.md
             - Setup guides reference CORTEX.prompt.md
             - Quick start tutorials reference CORTEX.prompt.md
             All references break if renamed
          
          3. User Confusion:
             Multiple prompt files create:
             - "Which one do I use?"
             - "Is cortex-lite the new version?"
             - Cognitive overhead increases
          
          4. Git History Fragmentation:
             Rename creates new file in git
             Old file shows as deleted
             History split across filenames
          
          Safe Update Procedure:
          
          Step 1: Create Temporary File
          ```
          .github/prompts/temp-cortex-update.md
          ```
          - Contains new optimized content
          - Reviewable before applying
          - Acts as backup if issues occur
          
          Step 2: Generate Optimized Content
          - Apply token optimizations
          - Restructure sections
          - Update documentation references
          - Add new features
          
          Step 3: Clear Original (Atomic Update)
          ```python
          # DELETE ALL content
          Path('.github/prompts/CORTEX.prompt.md').write_text('')
          ```
          - Prevents partial updates
          - Ensures clean slate
          - No merge conflicts
          
          Step 4: Copy Complete Instructions
          ```python
          content = Path('temp-cortex-update.md').read_text()
          Path('CORTEX.prompt.md').write_text(content)
          ```
          - Atomic replacement
          - No partial content risk
          
          Step 5: Delete Temporary File
          ```python
          Path('temp-cortex-update.md').unlink()
          ```
          - Clean up
          - Single source of truth
          
          Benefits:
          - Filename NEVER changes (stability)
          - Atomic updates (no half-updated states)
          - Review capability (temp file inspection)
          - Rollback support (restore from temp)
          - Clean git history (single file evolution)
          
          Real Incident Pattern Prevented:
          Developer: "I'll create CORTEX-lite.prompt.md"
          Result: Two prompt files coexist
          User confusion: "Which is current?"
          Maintenance nightmare: Multiple files diverge
          
          This rule BLOCKS any rename attempt.
          Exception: Temporary files for update workflow ARE encouraged.
      
      - rule_id: "GIT_CHECKPOINT_ENFORCEMENT"
        name: "Git Checkpoint Before Development Work"
        severity: "blocked"
        description: "Require git checkpoint (commit/tag) before starting any development work"
        
        detection:
          combined_keywords:
            development_start:
              - "implement feature"
              - "start development"
              - "begin implementation"
              - "fix bug"
              - "refactor code"
              - "add functionality"
              - "create new"
              - "modify existing"
            and_no_checkpoint:
              - "no checkpoint"
              - "skip checkpoint"
              - "without commit"
              - "uncommitted changes"
          scope: ["intent", "description", "git_status"]
          logic: "AND"
        
        alternatives:
          - "Create git checkpoint: git commit -m 'checkpoint: before [feature] development'"
          - "Create git tag: git tag -a checkpoint-YYYY-MM-DD-HH-MM -m 'Checkpoint before [feature]'"
          - "Stash changes if needed: git stash save 'WIP: checkpoint before [feature]'"
          - "Use git_checkpoint_module for automated checkpoint creation"
        
        evidence_template: |
          üö® GIT CHECKPOINT ENFORCEMENT VIOLATION
          
          Attempted: '{operation}'
          Git Status: '{git_status}'
          
          CRITICAL: Git checkpoint REQUIRED before starting development work!
          
          ‚ùå NEVER:
          - Start development with uncommitted changes
          - Skip checkpoint creation
          - Assume current state is safe
          - Ignore git status warnings
          
          ‚úÖ REQUIRED PROCEDURE:
          1. Check git status: Ensure clean working tree or commit staged changes
          2. Create checkpoint commit: git commit -m 'checkpoint: before [feature] development'
          3. OR create checkpoint tag: git tag -a checkpoint-[timestamp] -m 'Checkpoint before [feature]'
          4. Verify checkpoint: Confirm commit/tag created successfully
          5. Proceed with development: Now safe to make changes
          
          Why? Rollback capability + Audit trail + Risk mitigation
        
        rationale: |
          GIT_CHECKPOINT_ENFORCEMENT: Safety Net for Development
          
          Git checkpoints are MANDATORY before any development work to ensure:
          - Rollback capability if changes break functionality
          - Audit trail of what was changed and when
          - Clear separation between working states
          - Risk mitigation for exploratory changes
          
          Why This Protection Matters:
          
          1. Rollback Capability:
             Development can break things
             Checkpoint provides known-good state
             Quick recovery: git reset --hard [checkpoint]
             No data loss from experimentation
          
          2. Audit Trail:
             Clear history of development progression
             "What changed between checkpoints?"
             Blame analysis for regression debugging
             Documentation of development journey
          
          3. Risk Mitigation:
             Exploratory refactoring is safe
             Failed experiments don't pollute history
             Easy to abandon bad paths
             Encourages bold technical decisions
          
          4. Collaboration Safety:
             Team members can sync to checkpoints
             Code reviews reference specific checkpoints
             Integration points clearly marked
             Merge conflicts easier to resolve
          
          Checkpoint Types:
          
          A) Commit Checkpoint (Recommended):
          ```bash
          git commit -m "checkpoint: before authentication implementation"
          ```
          - Creates permanent history entry
          - Easy to reference: git show [commit-hash]
          - Appears in git log
          - Can be pushed for team visibility
          
          B) Tag Checkpoint (Alternative):
          ```bash
          git tag -a checkpoint-2025-11-19-14-30 \\
                   -m "Checkpoint before authentication feature"
          ```
          - Named reference point
          - Easy to remember: checkpoint-[date-time]
          - Lightweight (no commit overhead)
          - Can mark multiple points in history
          
          C) Stash Checkpoint (Temporary):
          ```bash
          git stash save "WIP: checkpoint before refactor"
          ```
          - Saves uncommitted changes
          - Useful for quick experiments
          - Recoverable: git stash pop
          - Local only (not pushed)
          
          Automated Checkpoint Module:
          
          ```python
          from src.operations.modules.git_checkpoint_module import GitCheckpointModule
          
          checkpoint = GitCheckpointModule()
          result = checkpoint.execute({
              'message': 'before authentication implementation',
              'checkpoint_type': 'commit'  # or 'tag'
          })
          ```
          
          When Checkpoints Are Required:
          - ‚úÖ Before implementing new features
          - ‚úÖ Before refactoring existing code
          - ‚úÖ Before fixing bugs (capture broken state)
          - ‚úÖ Before exploratory changes
          - ‚úÖ Before risky architectural changes
          - ‚ùå NOT for trivial changes (typo fixes, comments)
          - ‚ùå NOT for documentation-only updates
          
          Verification Process:
          
          1. Pre-Development Check:
             ```python
             git_status = subprocess.run(['git', 'status', '--porcelain'], 
                                        capture_output=True, text=True)
             if git_status.stdout.strip():
                 raise CheckpointViolation("Uncommitted changes detected")
             ```
          
          2. Checkpoint Creation:
             ```python
             subprocess.run(['git', 'commit', '-m', 'checkpoint: before feature'])
             # OR
             subprocess.run(['git', 'tag', '-a', 'checkpoint-[timestamp]'])
             ```
          
          3. Checkpoint Verification:
             ```python
             verify = subprocess.run(['git', 'log', '-1', '--oneline'], 
                                    capture_output=True, text=True)
             assert 'checkpoint:' in verify.stdout.lower()
             ```
          
          Integration Points:
          - BrainProtector: Validates checkpoint before development
          - HealthValidator: Checks for uncommitted changes
          - OptimizeOperation: Enforces checkpoint before changes
          - CommitHandler: Can create checkpoints automatically
          
          Real Incident Patterns Prevented:
          
          Scenario 1: Lost Refactoring Work
          Developer: Starts major refactor without checkpoint
          Result: Code breaks, no way to recover working state
          Prevention: Checkpoint required ‚Üí rollback available
          
          Scenario 2: Exploratory Regression
          Developer: Tries experimental approach, causes regression
          Result: Can't identify what changed, debugging nightmare
          Prevention: Checkpoint shows exact diff of changes
          
          Scenario 3: Merge Conflict Chaos
          Developer: Multiple changes without checkpoints
          Result: Massive merge conflicts, unclear resolution
          Prevention: Checkpoints provide integration points
          
          This rule BLOCKS development work without checkpoint.
          Exception: Documentation-only changes don't require checkpoints.
  
  # Layer 2: Tier Boundary Protection
  - layer_id: "tier_boundary"
    name: "Tier Boundary Protection"
    description: "Data stored in correct tier"
    priority: 2
    
    rules:
      - rule_id: "TIER0_APPLICATION_DATA"
        name: "No Application Data in Tier 0"
        severity: "blocked"
        description: "Application-specific path in Tier 0 (immutable governance)"
        
        detection:
          path_patterns:
            - "tier0/**"
            - "governance/**"
          contains_any: "{{application_paths}}"
        
        alternatives:
          - "Store in Tier 2 with scope='application'"
          - "Keep generic principles in Tier 0"
          - "Create application-specific tier"
        
        evidence: "Tier 0 is for generic CORTEX principles only"
      
      - rule_id: "TIER2_CONVERSATION_DATA"
        name: "No Conversation Data in Tier 2"
        severity: "warning"
        description: "Conversation data should be in Tier 1, not Tier 2"
        
        detection:
          path_patterns:
            - "tier2/**"
          contains: "conversation"
        
        alternatives:
          - "Move to Tier 1 (conversation-history.jsonl)"
          - "Store aggregated patterns in Tier 2"
          - "Keep raw data in Tier 1, patterns in Tier 2"
        
        evidence: "Tier 2 is for aggregated patterns, not raw conversations"
  
  # Layer 3: SOLID Compliance
  - layer_id: "solid_compliance"
    name: "SOLID Compliance"
    description: "No God Objects, proper separation"
    priority: 3
    
    rules:
      - rule_id: "SINGLE_RESPONSIBILITY"
        name: "Single Responsibility Principle"
        severity: "warning"
        description: "Potential God Object pattern detected (adding multiple responsibilities)"
        
        detection:
          keywords:
            - "add mode"
            - "add switch"
            - "handle all"
            - "do everything"
          scope: ["intent"]
        
        alternatives:
          - "Create dedicated agent for new responsibility"
          - "Use composition instead of adding modes"
          - "Extract to separate module"
        
        evidence_template: "Intent: '{intent}'"
      
      - rule_id: "DEPENDENCY_INVERSION"
        name: "Dependency Inversion Principle"
        severity: "warning"
        description: "Hardcoded dependency detected (violates DIP)"
        
        detection:
          keywords:
            - "hardcode path"
            - "fixed path"
            - "absolute path"
            - "inline config"
          scope: ["description"]
        
        alternatives:
          - "Use dependency injection"
          - "Load from configuration file"
          - "Pass as parameter"
        
        evidence_template: "Description: '{description}'"
      
      - rule_id: "OPEN_CLOSED"
        name: "Open/Closed Principle"
        severity: "warning"
        description: "Modifying existing behavior instead of extending"
        
        detection:
          keywords:
            - "change behavior"
            - "modify existing"
            - "alter functionality"
          scope: ["intent", "description"]
        
        alternatives:
          - "Create new implementation via extension"
          - "Use strategy pattern"
          - "Add decorator or wrapper"
        
        evidence_template: "Consider extension over modification"
      
      - rule_id: "CORTEX_WORKSPACE_ISOLATION"
        name: "CORTEX Workspace Isolation"
        severity: "blocked"
        description: "All CORTEX-generated documentation for application repos MUST be within CORTEX/Workspaces/ folder"
        
        detection:
          combined_keywords:
            cortex_generation:
              - "onboard application"
              - "generate documentation"
              - "create docs"
              - "application onboarding"
            root_output:
              - "/docs/"
              - "\\docs\\"
              - "root_path / 'docs'"
              - "project_root / 'docs'"
          scope: ["code", "file_path", "intent"]
          logic: "AND"
        
        verification_required:
          - type: "output_path_validation"
            description: "Verify all generated files within CORTEX/Workspaces/"
            requirement: "Output paths MUST contain 'CORTEX/Workspaces/[app-name]/'"
          
          - type: "no_root_pollution"
            description: "Verify no files created in application repository root or docs/"
            requirement: "git status MUST show zero new files outside CORTEX/"
          
          - type: "workspace_structure"
            description: "Verify proper workspace folder structure created"
            requirement: "CORTEX/Workspaces/[app-name]/{docs,diagrams,references}/"
        
        alternatives:
          - "Use CORTEX/Workspaces/[app-name]/docs/ for all generated documentation"
          - "Use CORTEX/Workspaces/[app-name]/diagrams/ for architecture diagrams"
          - "Use CORTEX/Workspaces/[app-name]/references/ for quick references"
          - "Add .gitignore: CORTEX/ (exclude entire CORTEX folder from user repo)"
        
        evidence_template: |
          CORTEX documentation generated outside workspace isolation!
          
          File: '{file_path}'
          Expected: CORTEX/Workspaces/[app-name]/...
          Actual: {actual_location}
          
          CRITICAL: NO cortex documentation should exist outside CORTEX folder
          
          Proper Structure:
          user-repo/
          ‚îú‚îÄ‚îÄ CORTEX/                          ‚Üê CORTEX folder (git-ignored)
          ‚îÇ   ‚îî‚îÄ‚îÄ Workspaces/                  ‚Üê All app documentation here
          ‚îÇ       ‚îî‚îÄ‚îÄ MyApp/                   ‚Üê App-specific workspace
          ‚îÇ           ‚îú‚îÄ‚îÄ docs/                ‚Üê Generated docs
          ‚îÇ           ‚îú‚îÄ‚îÄ diagrams/            ‚Üê Architecture diagrams
          ‚îÇ           ‚îî‚îÄ‚îÄ references/          ‚Üê Quick references
          ‚îú‚îÄ‚îÄ src/                             ‚Üê User application code
          ‚îî‚îÄ‚îÄ .gitignore                       ‚Üê Must include "CORTEX/"
          
          Why This Matters:
          - Clean separation (CORTEX artifacts ‚â† application code)
          - Easy cleanup (delete CORTEX/ removes all CORTEX files)
          - No git pollution (CORTEX/ excluded via .gitignore)
          - Portability (CORTEX workspace self-contained)
        
        rationale: |
          CORTEX_WORKSPACE_ISOLATION: Repository Organization Standard
          
          Real incident (2025-11-17):
          - User: "onboarding app creates docs in repo root, not CORTEX folder"
          - Published onboarding generates: user-repo/docs/onboarding.md
          - CORTEX artifacts polluting user's application repository
          - Cleanup difficult (which docs are CORTEX vs application?)
          - .gitignore cannot exclude selectively
          
          Why Workspace Isolation Critical:
          
          1. Repository Cleanliness:
             - User repo = user's application code only
             - CORTEX artifacts = temporary scaffolding
             - Clear boundary prevents confusion
             Example: docs/ could be user's actual docs or CORTEX-generated
          
          2. Easy Cleanup:
             - Delete CORTEX/ ‚Üí all CORTEX artifacts gone
             - No hunting for scattered CORTEX files
             - Uninstall = single folder removal
             Example: rm -rf CORTEX/ vs finding 47 scattered files
          
          3. Git Isolation:
             - Add "CORTEX/" to .gitignore once
             - No CORTEX artifacts ever committed to user repo
             - No accidental commits of temporary scaffolding
             Example: CORTEX workspace excluded, user code included
          
          4. Portability:
             - CORTEX workspace self-contained
             - Can backup/restore entire CORTEX state
             - Can sync across machines if desired
             Example: Copy CORTEX/ to new machine = full context restored
          
          5. Multi-Application Support:
             - CORTEX can work with multiple applications
             - Each app gets isolated workspace
             - No cross-contamination of artifacts
             Example: CORTEX/Workspaces/AppA/, CORTEX/Workspaces/AppB/
          
          Proper Workspace Structure:
          
          ```
          user-application-repo/
          ‚îú‚îÄ‚îÄ CORTEX/                          ‚Üê Git-ignored CORTEX folder
          ‚îÇ   ‚îú‚îÄ‚îÄ .cortex-metadata.json        ‚Üê Workspace metadata
          ‚îÇ   ‚îî‚îÄ‚îÄ Workspaces/                  ‚Üê All application workspaces
          ‚îÇ       ‚îú‚îÄ‚îÄ MyApp/                   ‚Üê Application-specific workspace
          ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ docs/                ‚Üê Generated documentation
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ onboarding.md
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture-overview.md
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ quick-reference.md
          ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ diagrams/            ‚Üê Architecture diagrams
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ component-diagram.mmd
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data-flow.mmd
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ images/          ‚Üê Rendered images
          ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ references/          ‚Üê Quick references
          ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api-quick-ref.md
          ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ analysis/            ‚Üê Code analysis reports
          ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ complexity-report.json
          ‚îÇ       ‚îî‚îÄ‚îÄ AnotherApp/              ‚Üê Another application
          ‚îÇ           ‚îî‚îÄ‚îÄ docs/
          ‚îÇ               ‚îî‚îÄ‚îÄ onboarding.md
          ‚îú‚îÄ‚îÄ src/                             ‚Üê User's actual application code
          ‚îÇ   ‚îî‚îÄ‚îÄ MyApp/
          ‚îú‚îÄ‚îÄ tests/                           ‚Üê User's tests
          ‚îú‚îÄ‚îÄ README.md                        ‚Üê User's README
          ‚îî‚îÄ‚îÄ .gitignore                       ‚Üê Must include "CORTEX/"
          ```
          
          Implementation Changes Required:
          
          1. PageGenerator (src/epm/modules/page_generator.py):
             Before:
             ```python
             self.output_path = root_path / "docs"
             ```
             
             After:
             ```python
             app_name = context.get('app_name', 'UnknownApp')
             self.output_path = root_path / "CORTEX" / "Workspaces" / app_name / "docs"
             ```
          
          2. DiagramGenerator (src/epm/modules/diagram_generator.py):
             Before:
             ```python
             self.output_path = root_path / "docs"
             ```
             
             After:
             ```python
             app_name = context.get('app_name', 'UnknownApp')
             self.output_path = root_path / "CORTEX" / "Workspaces" / app_name / "diagrams"
             ```
          
          3. ImagePromptGenerator (src/epm/modules/image_prompt_generator.py):
             Before:
             ```python
             self.output_dir = Path(output_dir)  # Typically docs/diagrams
             ```
             
             After:
             ```python
             app_name = context.get('app_name', 'UnknownApp')
             self.output_dir = root_path / "CORTEX" / "Workspaces" / app_name / "diagrams"
             ```
          
          4. Onboarding Orchestrator Context:
             Add app_name to session context:
             ```python
             session_context = {
                 "app_name": self._detect_app_name(root_path),  # From solution file, csproj, package.json
                 "profile": profile.value,
                 "project_root": self.project_root,
                 ...
             }
             ```
          
          5. .gitignore Creation:
             Onboarding MUST create/update user repo's .gitignore:
             ```gitignore
             # CORTEX AI Assistant (local workspace, not committed)
             CORTEX/
             ```
          
          Benefits:
          - Clean separation: CORTEX ‚â† Application
          - Easy uninstall: Delete CORTEX/ folder
          - No git pollution: Single .gitignore entry
          - Multi-app support: Isolated workspaces
          - Portable: Self-contained CORTEX state
          
          Enforcement:
          - Brain Protector blocks operations writing outside CORTEX/
          - Integration tests verify output paths
          - Onboarding validates workspace structure
          - Design sync validates isolation maintained
          
          Exception: Shared CORTEX Installation
          If user wants to share CORTEX across projects (not per-repo):
          - Install CORTEX once (e.g., ~/CORTEX/)
          - Each repo references shared CORTEX
          - Workspaces still isolated: ~/CORTEX/Workspaces/[app-name]/
          - This is advanced configuration (document separately)
      
      - rule_id: "CODE_STYLE_CONSISTENCY"
        name: "Adopt User's Code Style"
        severity: "warning"
        description: "Generated code should match existing codebase style conventions"
        
        detection:
          combined_keywords:
            code_generation:
              - "generate code"
              - "create file"
              - "implement"
              - "write function"
              - "add class"
            style_mismatch:
              - "different style"
              - "inconsistent formatting"
              - "foreign conventions"
          scope: ["code", "file_content"]
          logic: "AND"
        
        alternatives:
          - "Analyze existing codebase style before generating"
          - "Match indentation (tabs vs spaces, 2 vs 4 spaces)"
          - "Match naming conventions (camelCase, snake_case, PascalCase)"
          - "Match bracket style (K&R, Allman, etc.)"
          - "Match quote style (single vs double quotes)"
          - "Match comment style and documentation format"
          - "Use linter configs (.editorconfig, .pylintrc, etc.) if available"
        
        evidence_template: |
          Code style inconsistency detected: '{mismatch}'
          
          User's codebase style:
          - Indentation: {user_indent}
          - Naming: {user_naming}
          - Quotes: {user_quotes}
          - Brackets: {user_brackets}
          
          Generated code should blend seamlessly with existing style.
        
        rationale: |
          CODE_STYLE_CONSISTENCY: Developer Experience Principle
          
          **CRITICAL HIERARCHY: Best Practices > Style Preferences**
          
          When generating code, CORTEX MUST:
          1. ‚úÖ ALWAYS follow SOLID principles (non-negotiable)
          2. ‚úÖ ALWAYS follow proper OOP design (non-negotiable)
          3. ‚úÖ ALWAYS follow security best practices (non-negotiable)
          4. ‚úÖ ALWAYS follow DRY, KISS, YAGNI principles (non-negotiable)
          5. ‚úÖ THEN adapt to user's style preferences (adaptive layer)
          
          This means: If user's style conflicts with best practices, 
          CORTEX follows best practices and explains why.
          
          Example Scenarios:
          
          ‚úÖ ADAPT TO USER STYLE (No conflict with best practices):
          User's code:
          ```python
          def calculate_total(items):  # No type hints
              total = 0
              for item in items:
                  total += item['price']
              return total
          ```
          
          CORTEX generates (matching style):
          ```python
          def calculate_tax(total, rate):  # Match: No type hints
              return total * rate          # Match: Simple style
          ```
          
          ‚ö° OVERRIDE USER STYLE (Conflicts with best practices):
          User's code:
          ```python
          def do_everything(data):  # God method violating SRP
              process(data)
              validate(data)
              save(data)
              email(data)
              log(data)
          ```
          
          CORTEX generates (SOLID over style):
          ```python
          # CORTEX: I noticed your code has a God method pattern.
          # I'll demonstrate Single Responsibility Principle instead:
          
          def process_data(data):
              return processor.process(data)
          
          def save_processed_data(processed_data):
              return repository.save(processed_data)
          
          # Explanation: Each function has one clear responsibility.
          # This makes testing easier and code more maintainable.
          ```
          
          When CORTEX generates code, it should feel like the user wrote it,
          not like a foreign AI dropped it in. Consistency builds trust and
          reduces cognitive friction.
          
          What to Match (When No Conflict with Best Practices):
          
          1. Indentation:
             - Tabs vs spaces
             - 2 spaces vs 4 spaces
             - Consistent nesting
          
          2. Naming Conventions:
             - Python: snake_case for functions/variables
             - JavaScript: camelCase for functions, PascalCase for classes
             - C#: PascalCase for public, camelCase for private
             - Project-specific prefixes (e.g., 'cortex_', 'internal_')
          
          3. Quote Style:
             - Python: Single vs double (PEP 8 prefers single for strings)
             - JavaScript: Consistency with existing files
             - Template literals vs concatenation
          
          4. Bracket/Brace Style:
             - K&R: Opening brace same line `function() {`
             - Allman: Opening brace new line
             ```
             function()
             {
             ```
             - Consistent in Python (implicit via PEP 8)
          
          5. Documentation:
             - Docstring style (Google, NumPy, reStructuredText)
             - Comment density and placement
             - Type hints (Python 3.5+)
             - JSDoc vs inline comments
          
          6. Import Organization:
             - Standard library, third-party, local (PEP 8)
             - Alphabetical vs functional grouping
             - Relative vs absolute imports
          
          7. Line Length:
             - 80 chars (classic PEP 8)
             - 100 chars (modern)
             - 120 chars (widescreen)
          
          8. Trailing Commas:
             - Multi-line lists/dicts
             - Function parameters
          
          Detection Strategy:
          
          1. Sample Existing Files:
             ```python
             def detect_code_style(project_path):
                 samples = glob(f"{project_path}/**/*.py", recursive=True)[:10]
                 
                 styles = {
                     'indent': detect_indent(samples),
                     'naming': detect_naming(samples),
                     'quotes': detect_quotes(samples),
                     'line_length': detect_line_length(samples)
                 }
                 
                 return styles
             ```
          
          2. Read Config Files:
             - .editorconfig (universal)
             - pyproject.toml (Python)
             - .eslintrc (JavaScript)
             - .prettierrc (JavaScript/TypeScript)
             - .pylintrc (Python)
             - tslint.json (TypeScript)
          
          3. Use Existing Formatters:
             - Black (Python - opinionated)
             - Prettier (JavaScript - opinionated)
             - autopep8 (Python - PEP 8)
             - Ruff (Python - fast linter)
          
          Example Detection:
          
          ```python
          # User's existing code
          def calculate_total(items: list[dict]) -> float:
              '''Calculate total price from items.'''
              total = 0.0
              for item in items:
                  total += item['price']
              return total
          
          # CORTEX should generate:
          def calculate_tax(total: float, rate: float) -> float:
              '''Calculate tax amount.'''  # Match docstring style
              tax = total * rate            # Match naming (snake_case)
              return tax                    # Match simplicity
          
          # ‚ùå NOT this (different style):
          def CalculateTax(Total: float, Rate: float) -> float:
              """Calculate tax amount."""  # Different docstring quotes
              Tax = Total * Rate            # Different naming (PascalCase)
              return Tax
          ```
          
          Integration with CORTEX:
          
          1. Style Analyzer Module:
             - Runs on project first scan
             - Stores detected style in Tier 2 knowledge graph
             - Updated when major style changes detected
          
          2. Code Generator Hook:
             - Before generating, load project style profile
             - Apply style template to generated code
             - Run formatter if available (black, prettier)
          
          3. Validation:
             - Compare generated code against style profile
             - Flag deviations before presenting to user
             - Suggest corrections
          
          Override Scenarios:
          
          When to IGNORE user's style:
          - User explicitly requests specific style
          - Generating config/scaffold with tool's conventions
          - Creating new project (use CORTEX defaults)
          - Fixing style issues (user asked for cleanup)
          
          User Experience:
          
          ‚úÖ GOOD:
          User: "Add a function to calculate tax"
          CORTEX: [Analyzes existing code]
          CORTEX: [Generates function matching user's style]
          User: "Perfect, looks like I wrote it!"
          
          ‚ùå BAD:
          User: "Add a function to calculate tax"
          CORTEX: [Generates code in default style]
          User: "This doesn't match my codebase style at all"
          User: [Has to reformat manually]
          
          Metrics:
          - Style consistency score (0-100%)
          - User acceptance rate (accepting vs modifying)
          - Manual reformatting frequency
          
          This rule ensures CORTEX acts as a seamless extension of the
          developer, not a foreign entity imposing its own conventions.
      
      - rule_id: "NO_EMOJIS_IN_SCRIPTS"
        name: "No Emojis in Generated Scripts"
        severity: "warning"
        description: "Scripts (Python, PowerShell, Bash, etc.) should not contain emojis"
        
        detection:
          combined_keywords:
            script_generation:
              - "generate script"
              - "create .py"
              - "create .ps1"
              - "create .sh"
              - "write script"
            emoji_usage:
              - "‚úÖ"
              - "‚ùå"
              - "‚ö†Ô∏è"
              - "üîç"
              - "üìä"
              - "üß†"
              - "emoji"
          scope: ["code", "file_content"]
          logic: "AND"
        
        alternatives:
          - "Use plain text markers: [OK], [FAIL], [WARN]"
          - "Use ASCII art or text symbols: +, -, *, !"
          - "Reserve emojis for documentation and user-facing text only"
          - "Use logging levels: INFO, ERROR, WARNING instead of emojis"
        
        evidence_template: |
          Emoji detected in script: '{file_path}'
          
          Emojis found: {emoji_list}
          
          Scripts should use plain text for maximum compatibility:
          - Encoding issues across platforms
          - Terminal rendering problems
          - Copy/paste issues in some editors
          - Professional appearance
        
        rationale: |
          NO_EMOJIS_IN_SCRIPTS: Code Quality Standard
          
          Why Emojis Don't Belong in Scripts:
          
          1. Encoding Issues:
             - UTF-8 encoding not universal in all environments
             - PowerShell ISE may not render correctly
             - Windows cmd.exe has encoding problems
             - Remote SSH sessions may lose emojis
          
          2. Terminal Compatibility:
             - Some terminals don't support Unicode emojis
             - CI/CD logs may show broken characters
             - Legacy systems show question marks
             - Screen readers struggle with emojis
          
          3. Professional Standards:
             - Scripts are code, not social media
             - Emojis reduce professional appearance
             - Industry standard: plain text markers
             - Easier to grep/search logs
          
          4. Copy/Paste Problems:
             - Email clients may corrupt emojis
             - Documentation tools may strip emojis
             - Version control diffs show weird bytes
             - Stack Overflow code samples break
          
          What to Use Instead:
          
          ‚ùå Emoji-Based:
          ```python
          print("‚úÖ Test passed")
          print("‚ùå Test failed")
          print("‚ö†Ô∏è Warning detected")
          ```
          
          ‚úÖ Plain Text:
          ```python
          print("[OK] Test passed")
          print("[FAIL] Test failed")
          print("[WARN] Warning detected")
          ```
          
          ‚úÖ Logging Levels:
          ```python
          logger.info("Test passed")
          logger.error("Test failed")
          logger.warning("Warning detected")
          ```
          
          ‚úÖ ASCII Symbols:
          ```python
          print("+ Test passed")
          print("- Test failed")
          print("! Warning detected")
          ```
          
          Allowed Emoji Usage:
          - Documentation (README.md, guides)
          - User-facing messages (GitHub Copilot Chat responses)
          - Markdown files (story.md, setup-guide.md)
          - Comments in code (sparingly, for clarity)
          
          Not Allowed:
          - Python scripts (.py)
          - PowerShell scripts (.ps1)
          - Bash scripts (.sh)
          - Batch files (.bat, .cmd)
          - Any executable script files
          
          Example Violation:
          ```python
          # ‚ùå BAD
          def run_tests():
              print("üß™ Running tests...")
              if all_pass:
                  print("‚úÖ All tests passed!")
              else:
                  print("‚ùå Some tests failed!")
          
          # ‚úÖ GOOD
          def run_tests():
              print("[TEST] Running tests...")
              if all_pass:
                  print("[OK] All tests passed!")
              else:
                  print("[FAIL] Some tests failed!")
          ```
          
          This maintains code professionalism and ensures scripts work
          universally across all platforms and environments.
      
      - rule_id: "NO_ROOT_SUMMARY_DOCUMENTS"
        name: "No Summary Documents in Repository Root"
        severity: "warning"
        description: "Summary/report documents should be in cortex-brain/documents/, not repository root"
        
        detection:
          combined_keywords:
            root_level_creation:
              - "create file in root"
              - "save to repository root"
              - ".md in d:\\PROJECTS\\CORTEX\\"
              - ".md in /Users/"
              - ".md in /home/"
            summary_markers:
              - "summary"
              - "report"
              - "analysis"
              - "status"
              - "completion"
              - "investigation"
          scope: ["intent", "file_path"]
          logic: "AND"
        
        verification_required:
          - type: "path_validation"
            description: "Verify document created in cortex-brain/documents/"
            requirement: "File path MUST contain 'cortex-brain/documents/'"
          
          - type: "category_check"
            description: "Verify document placed in appropriate category"
            requirement: "Must be in reports/, analysis/, summaries/, etc."
        
        alternatives:
          - "Use cortex-brain/documents/reports/ for completion reports"
          - "Use cortex-brain/documents/analysis/ for investigations"
          - "Use cortex-brain/documents/summaries/ for quick overviews"
          - "Use cortex-brain/documents/conversation-captures/ for strategic conversations"
          - "See cortex-brain/documents/README.md for full structure"
        
        evidence_template: |
          Summary document created in wrong location!
          
          File: '{file_path}'
          Expected: cortex-brain/documents/{category}/
          
          Document Organization Rules:
          - Reports ‚Üí cortex-brain/documents/reports/
          - Analysis ‚Üí cortex-brain/documents/analysis/
          - Summaries ‚Üí cortex-brain/documents/summaries/
          - Investigations ‚Üí cortex-brain/documents/investigations/
          - Planning ‚Üí cortex-brain/documents/planning/
          
          Repository root is reserved for:
          - README.md
          - LICENSE
          - Package files (package.json, requirements.txt)
          - Configuration (cortex.config.json)
        
        rationale: |
          NO_ROOT_SUMMARY_DOCUMENTS: Document Organization Standard
          
          Why Root-Level Summaries Are Problematic:
          
          1. Repository Clutter:
             - Too many documents make root directory unnavigable
             - Hard to find important files (README, LICENSE)
             - Git status becomes noisy
             - New contributors confused by file soup
          
          2. Loss of Context:
             - No categorization = documents hard to find
             - "Which report is the latest?"
             - "Where's the security analysis?"
             - Search becomes time-consuming
          
          3. Merge Conflicts:
             - Multiple people creating summaries in root
             - Filename collisions likely
             - No clear ownership or hierarchy
          
          4. Violates CORTEX Organization Mandate:
             - CORTEX has structured document system
             - Category-based organization already exists
             - Ignoring structure creates technical debt
          
          CORTEX Document Structure:
          
          ```
          cortex-brain/documents/
          ‚îú‚îÄ‚îÄ reports/              # Implementation completion, status reports
          ‚îÇ   ‚îú‚îÄ‚îÄ CORTEX-3.0-FINAL-REPORT.md
          ‚îÇ   ‚îî‚îÄ‚îÄ PHASE-0-COMPLETION-REPORT.md
          ‚îÇ
          ‚îú‚îÄ‚îÄ analysis/             # Deep investigations, performance analysis
          ‚îÇ   ‚îú‚îÄ‚îÄ ROUTER-PERFORMANCE-ANALYSIS.md
          ‚îÇ   ‚îî‚îÄ‚îÄ TOKEN-OPTIMIZATION-ANALYSIS.md
          ‚îÇ
          ‚îú‚îÄ‚îÄ summaries/            # Quick overviews, daily progress
          ‚îÇ   ‚îú‚îÄ‚îÄ TIER3-IMPLEMENTATION-SUMMARY.md
          ‚îÇ   ‚îî‚îÄ‚îÄ WEEKLY-PROGRESS-SUMMARY.md
          ‚îÇ
      
      - rule_id: "YAML_ONLY_PLANNING"
        name: "YAML-Only Planning Documents (No Markdown Plans)"
        severity: "blocked"
        description: "ALL planning documents MUST be created in YAML format, NEVER Markdown - prevents documentation bloat and enforces machine-readable standards"
        
        detection:
          combined_keywords:
            planning_document:
              - "plan"
              - "planning"
              - "roadmap"
              - "design"
              - "consolidation"
              - "implementation plan"
              - "comprehensive plan"
            and_markdown_format:
              - ".md"
              - "markdown"
              - "create markdown plan"
            not_user_facing:
              - "!story"
              - "!guide"
              - "!tutorial"
          scope: ["file_path", "description", "operation"]
          logic: "AND"
        
        verification_required:
          - type: "format_validation"
            description: "Verify plan document created as YAML, not Markdown"
            requirement: "File extension MUST be .yaml, NOT .md"
          
          - type: "location_validation"
            description: "Verify plan placed in planning folder"
            requirement: "File path MUST be cortex-brain/documents/planning/"
          
          - type: "schema_validation"
            description: "Verify YAML follows structured planning schema"
            requirement: "YAML MUST validate against planning schema"
        
        alternatives:
          - "Create YAML plan in cortex-brain/documents/planning/[name].yaml"
          - "Use structured YAML schemas for all planning documents"
          - "Reference existing YAML plans: YAML-PHASE-TRACKER-DESIGN.yaml"
          - "Use cortex-brain/CORTEX-UNIFIED-ARCHITECTURE.yaml as template"
        
        evidence_template: |
          üö® MARKDOWN PLANNING DOCUMENT DETECTED
          
          File: '{file_path}'
          Type: Planning Document (should be YAML)
          
          VIOLATION: Planning documents MUST be YAML format, NOT Markdown
          
          ‚ùå WRONG:
          - DOCUMENTATION-CONSOLIDATION-COMPREHENSIVE-PLAN.md
          - IMPLEMENTATION-PLAN.md
          - FEATURE-DESIGN.md
          - ARCHITECTURE-PLAN.md
          
          ‚úÖ CORRECT:
          - documentation-consolidation-plan.yaml
          - implementation-plan.yaml
          - feature-design.yaml
          - architecture-plan.yaml
          
          Why YAML-Only Planning?
          
          1. **Prevents Documentation Bloat**
             - Markdown plans tend to be verbose (10-50 pages)
             - YAML enforces concise structured format
             - Token efficiency (YAML 60-80% smaller than MD)
          
          2. **Machine-Readable**
             - YAML can be parsed and processed programmatically
             - Enables automated validation and tracking
             - Integrates with CORTEX brain systems
          
          3. **Consistency**
             - Enforced schema structure
             - Standard fields across all plans
             - No "wall of text" formatting variations
          
          4. **Searchability**
             - Structured queries on plan fields
             - Easy to filter and aggregate
             - Better brain integration
          
          Correct Location:
          - cortex-brain/documents/planning/[plan-name].yaml
          
          Exception:
          - User-facing documentation (stories, guides, tutorials) CAN be Markdown
          - Internal planning/design MUST be YAML
        
        rationale: |
          YAML_ONLY_PLANNING: Documentation Bloat Prevention
          
          Real incident (2025-11-18):
          - CORTEX created: DOCUMENTATION-CONSOLIDATION-COMPREHENSIVE-PLAN.md
          - File size: 23KB (extensive markdown document)
          - User: "why is CORTEX still generating md plans instead of yaml plans?"
          - User: "Add to tier 0 that planning should ALWAYS done using yaml files NEVER MD to prevent documentation bloat"
          
          Problem with Markdown Plans:
          - Verbose "comprehensive" documents (10-50 pages typical)
          - Heavy token cost to read/process
          - Difficult to parse programmatically
          - No enforced structure (inconsistent formats)
          - Accumulates as "documentation debt"
          
          YAML Planning Benefits:
          - Enforced concise structure
          - 60-80% token reduction vs Markdown
          - Machine-readable for automation
          - Consistent schema across all plans
          - Easy to validate and query
          
          Implementation:
          - CORTEX creates planning YAML schemas
          - Validation against JSON Schema
          - Brain integration for automated tracking
          - Markdown reserved ONLY for user-facing docs (stories, guides)
          
          Related Rules:
          - MACHINE_READABLE_FORMATS (Tier 0 instinct)
          - NO_ROOT_SUMMARY_DOCUMENTS (Layer 3)
          
          Reference Examples:
          - cortex-brain/documents/planning/YAML-PHASE-TRACKER-DESIGN.yaml
          - cortex-brain/CORTEX-UNIFIED-ARCHITECTURE.yaml
          - cortex-operations.yaml
          
          User Feedback:
          "Add to tier 0 of cortex that planning should ALWAYS done using yaml files NEVER MD 
          to prevent documentation bloat" - Asif Hussain, 2025-11-18
          ‚îÇ
          ‚îú‚îÄ‚îÄ investigations/       # Research, architecture investigations
          ‚îÇ   ‚îú‚îÄ‚îÄ AUTH-FEATURE-INVESTIGATION.md
          ‚îÇ   ‚îî‚îÄ‚îÄ DATABASE-MIGRATION-INVESTIGATION.md
          ‚îÇ
          ‚îú‚îÄ‚îÄ planning/             # Roadmaps, implementation plans
          ‚îÇ   ‚îú‚îÄ‚îÄ CORTEX-4.0-PLANNING.md
          ‚îÇ   ‚îî‚îÄ‚îÄ features/
          ‚îÇ       ‚îî‚îÄ‚îÄ PLAN-2025-11-17-authentication.md
          ‚îÇ
          ‚îú‚îÄ‚îÄ conversation-captures/ # Strategic conversation captures
          ‚îÇ   ‚îî‚îÄ‚îÄ CONVERSATION-CAPTURE-2025-11-14-AUTHENTICATION.md
          ‚îÇ
          ‚îî‚îÄ‚îÄ implementation-guides/  # How-to guides, integration docs
              ‚îî‚îÄ‚îÄ CORTEX-SETUP-GUIDE.md
          ```
          
          Benefits of Organized Structure:
          - Easy to find related documents
          - Clear ownership and purpose
          - Searchable by category
          - No root-level clutter
          - Professional appearance
          
          Root Directory Reserved For:
          ```
          CORTEX/
          ‚îú‚îÄ‚îÄ README.md              ‚úÖ Project overview
          ‚îú‚îÄ‚îÄ LICENSE                ‚úÖ Legal
          ‚îú‚îÄ‚îÄ package.json           ‚úÖ Dependencies
          ‚îú‚îÄ‚îÄ requirements.txt       ‚úÖ Python packages
          ‚îú‚îÄ‚îÄ cortex.config.json     ‚úÖ Configuration
          ‚îú‚îÄ‚îÄ setup.py               ‚úÖ Installation
          ‚îú‚îÄ‚îÄ mkdocs.yml             ‚úÖ Docs build config
          ‚îî‚îÄ‚îÄ .gitignore             ‚úÖ Git config
          ```
          
          Example Violations:
          
          ‚ùå BAD:
          ```
          d:\PROJECTS\CORTEX\INVESTIGATION-ANALYSIS-REPORT.md
          d:\PROJECTS\CORTEX\CORTEX-3.0-IMPLEMENTATION-COMPLETE.md
          d:\PROJECTS\CORTEX\COMPREHENSIVE-CORTEX-ANALYSIS-REPORT.md
          ```
          
          ‚úÖ GOOD:
          ```
          d:\PROJECTS\CORTEX\cortex-brain\documents\analysis\INVESTIGATION-ANALYSIS-REPORT.md
          d:\PROJECTS\CORTEX\cortex-brain\documents\reports\CORTEX-3.0-IMPLEMENTATION-COMPLETE.md
          d:\PROJECTS\CORTEX\cortex-brain\documents\analysis\COMPREHENSIVE-CORTEX-ANALYSIS-REPORT.md
          ```
          
          Enforcement:
          - Brain Protector challenges root-level document creation
          - Suggests appropriate category automatically
          - Provides full path template
          - References cortex-brain/documents/README.md for guidelines
          
          Override Cases (Rare):
          - User explicitly requests root-level placement
          - Temporary scaffolding during setup
          - Files intended for repository metadata (CHANGELOG.md, CONTRIBUTING.md)
          
          This maintains clean repository structure and ensures documents
          are findable, organized, and maintainable long-term.
  
  # Layer 4: Hemisphere Specialization
  - layer_id: "hemisphere_specialization"
    name: "Hemisphere Specialization"
    description: "Strategic vs tactical separation"
    priority: 4
    
    rules:
      - rule_id: "LEFT_BRAIN_TACTICAL"
        name: "Left Brain Tactical Only"
        severity: "warning"
        description: "Strategic planning logic in tactical executor"
        
        detection:
          files:
            - "code-executor.md"
            - "test-generator.md"
            - "error-corrector.md"
          keywords:
            - "create plan"
            - "estimate time"
            - "assess risk"
            - "strategy"
          scope: ["intent"]
        
        alternatives:
          - "Move planning logic to work-planner.md"
          - "Keep execution logic in code-executor.md"
          - "Use corpus callosum for coordination"
        
        evidence: "LEFT brain should execute, not plan"
      
      - rule_id: "RIGHT_BRAIN_STRATEGIC"
        name: "Right Brain Strategic Only"
        severity: "warning"
        description: "Tactical execution logic in strategic planner"
        
        detection:
          files:
            - "work-planner.md"
            - "intent-router.md"
          keywords:
            - "write code"
            - "run test"
            - "execute"
            - "implement"
          scope: ["intent"]
        
        alternatives:
          - "Delegate execution to LEFT brain agents"
          - "Keep planning in RIGHT brain"
          - "Use agent coordination"
        
        evidence: "RIGHT brain should plan, not execute"
  
  # Layer 5: SKULL Protection (Safety, Knowledge, Validation & Learning)
  - layer_id: "skull_protection"
    name: "SKULL Protection Layer"
    description: "Test validation and quality enforcement (prevents November 9th incident)"
    priority: 5
    
    rules:
      - rule_id: "SKULL_TEST_BEFORE_CLAIM"
        name: "Test Before Claim (SKULL-001)"
        severity: "blocked"
        description: "Never claim a fix is complete without test validation"
        
        detection:
          keywords:
            - "fixed ‚úÖ"
            - "complete ‚úÖ"
            - "done ‚úÖ"
            - "implemented ‚úÖ"
          without_keywords:
            - "test passed"
            - "test verified"
            - "validated by test"
            - "pytest"
          scope: ["response"]
        
        alternatives:
          - "Create automated test before claiming fix"
          - "Run test and include results in response"
          - "Show test output: 'Fixed ‚úÖ (Verified by: test_button_color)'"
        
        evidence_template: "Claim: '{match}' without test validation"
        
        rationale: |
          SKULL-001: Test Before Claim
          
          Real incident (2025-11-09):
          - CSS fixes claimed "Fixed ‚úÖ" three times
          - Vision API claimed "Auto-engages ‚úÖ" 
          - Zero tests run to validate
          - User had to report "not working" each time
          
          SKULL prevents this by BLOCKING any success claim without test validation.
      
      - rule_id: "SKULL_INTEGRATION_VERIFICATION"
        name: "Integration Verification (SKULL-002)"
        severity: "blocked"
        description: "Integration must be tested end-to-end"
        
        detection:
          keywords:
            - "integration complete"
            - "components connected"
            - "API integrated"
            - "auto-engages"
          without_keywords:
            - "end-to-end test"
            - "integration test"
            - "e2e test"
          scope: ["description"]
        
        alternatives:
          - "Create end-to-end integration test"
          - "Test full call chain: A ‚Üí B ‚Üí C"
          - "Verify actual execution path, not just config"
        
        evidence_template: "Integration claim without E2E test: '{match}'"
        
        rationale: |
          SKULL-002: Integration Verification
          
          Real incident (2025-11-09):
          - Vision API "integration" claimed complete
          - Only config was changed
          - No test of actual call chain
          - Vision API was never actually called
          
          SKULL prevents this by requiring end-to-end integration tests.
      
      - rule_id: "SKULL_VISUAL_REGRESSION"
        name: "Visual Regression (SKULL-003)"
        severity: "warning"
        description: "CSS/UI changes require visual validation"
        
        detection:
          keywords:
            - "css fixed"
            - "style updated"
            - "color changed"
            - "UI improved"
          without_keywords:
            - "visual test"
            - "computed style"
            - "playwright"
            - "browser test"
          scope: ["description"]
        
        alternatives:
          - "Add visual regression test (Playwright/Puppeteer)"
          - "Verify computed style in browser"
          - "Include before/after screenshot comparison"
        
        evidence_template: "CSS/UI change without visual test: '{match}'"
        
        rationale: |
          SKULL-003: Visual Regression
          
          Real incident (2025-11-09):
          - CSS rules applied to fix title color
          - Claimed "Fixed ‚úÖ" without checking browser
          - Cache wasn't cleared, changes not visible
          - Repeated 3 times with same approach
          
          SKULL prevents this by requiring visual validation of CSS changes.
      
      - rule_id: "SKULL_RETRY_WITHOUT_LEARNING"
        name: "Retry Without Learning (SKULL-004)"
        severity: "warning"
        description: "Must diagnose failures before retrying same approach"
        
        detection:
          combined_keywords:
            retry_marker:
              - "try again"
              - "retry"
              - "attempt 2"
              - "attempt 3"
            no_diagnosis:
              - "same fix"
              - "reapply"
              - "rebuild again"
          without_keywords:
            - "diagnosed"
            - "root cause"
            - "cache cleared"
            - "verified"
          scope: ["description"]
          logic: "AND"
        
        alternatives:
          - "Diagnose WHY previous fix failed"
          - "Check: file contents, browser cache, build output, computed styles"
          - "Change approach based on diagnosis"
          - "Add test to prevent regression"
        
        evidence_template: "Retry without diagnosis: '{description}'"
        
        rationale: |
          SKULL-004: Retry Without Learning
          
          Real incident (2025-11-09):
          - CSS fix applied
          - User: "didn't work"
          - Same CSS fix applied again
          - User: "still didn't work"  
          - Same CSS fix applied THIRD time
          - No diagnosis of why it failed
          
          SKULL prevents this by requiring root cause analysis before retries.
      
      - rule_id: "SKULL_TRANSFORMATION_VERIFICATION"
        name: "Transformation Verification (SKULL-005)"
        severity: "blocked"
        description: "Operations claiming transformation MUST produce measurable changes"
        
        detection:
          combined_keywords:
            transformation_claim:
              - "transformation complete"
              - "refresh complete"
              - "converted"
              - "updated documentation"
              - "generated"
            success_claim:
              - "success"
              - "completed successfully"
              - "fixed ‚úÖ"
              - "done ‚úÖ"
          scope: ["description", "log_output"]
          logic: "AND"
        
        verification_required:
          - type: "file_hash_comparison"
            description: "Compare file hash before/after operation"
            requirement: "Hashes MUST differ for transformation operations"
          
          - type: "git_diff_check"
            description: "Verify git diff shows actual changes"
            requirement: "git diff MUST show modifications, not empty output"
          
          - type: "content_analysis"
            description: "Validate transformation logic executed"
            requirement: "Operation MUST NOT be pass-through (input != output)"
        
        alternatives:
          - "Implement actual transformation logic (not pass-through)"
          - "Mark operation as 'validation-only' if no transformation needed"
          - "Add integration test verifying file changes occur"
          - "Change success message to reflect pass-through behavior"
        
        evidence_template: "Operation '{operation_name}' claims transformation but produces no changes"
        
        rationale: |
          SKULL-005: Transformation Verification
          
          Real incident (2025-11-10):
          - refresh_cortex_story operation executed
          - Module apply_narrator_voice_module.py claims "transformation complete"
          - Returns success=True with "Narrator voice transformation complete"
          - BUT: Line 123 does `context['transformed_story'] = story_content` (pass-through!)
          - File hash unchanged after operation
          - git diff shows NO changes
          - User discovers operation is fake
          
          Impact:
          - User trust degradation (claims success but does nothing)
          - Status inflation (operations marked READY when incomplete)
          - Integration failures (downstream operations expect real data)
          
          SKULL-005 prevents this by:
          1. Detecting transformation + success claims in output
          2. Requiring file hash comparison test
          3. Blocking completion without measurable changes
          4. Forcing honest status reporting (PARTIAL vs READY)
          
          Implementation:
          - Add @verify_transformation decorator to operation modules
          - Integration tests MUST check before/after file state
          - CI fails if transformation claims success but git diff empty
          - Status documents distinguish architecture vs implementation
      
      - rule_id: "SKULL_PRIVACY_PROTECTION"
        name: "Privacy Protection (SKULL-006)"
        severity: "blocked"
        description: "Publish operations MUST NOT include files with machine-specific paths or private data"
        
        detection:
          patterns:
            - "AHHOME"
            - ".coverage.*.* "
            - "C:\\\\"
            - "D:\\\\"
            - "/home/[a-z]+"
            - "/Users/[a-z]+"
          file_types:
            - "**/*.log"
            - "**/logs/**"
            - "**/.coverage.*"
            - "**/health-reports/**"
            - "**/__pycache__/**"
          scope: ["published_files"]
        
        verification_required:
          - type: "privacy_scan"
            description: "Scan all published files for machine-specific paths"
            requirement: "Zero files with absolute paths (C:\\, D:\\, /home/, AHHOME)"
          
          - type: "exclusion_test"
            description: "Test that publish script excludes privacy-leaking files"
            requirement: "Test MUST verify .coverage.*, logs/, health-reports/ excluded"
          
          - type: "config_sanitization"
            description: "Verify config files use template values, not real paths"
            requirement: "cortex.config.json MUST use placeholders, not AHHOME paths"
        
        alternatives:
          - "Add file patterns to EXCLUDE_PATTERNS in publish script"
          - "Create .publishignore file with privacy exclusions"
          - "Add pre-publish scan that fails on privacy leak"
          - "Use template configs with placeholder paths"
        
        evidence_template: "Published file contains privacy data: '{file_path}' - found '{privacy_leak}'"
        
        rationale: |
          SKULL-006: Privacy Protection
          
          Real incident (2025-11-12):
          - User runs publish script
          - Discovers .coverage.AHHOME.12345.XgvxuuYx in publish/CORTEX/
          - 7 coverage files with machine name exposed
          - logs/ambient_capture.log contains C:\Windows\Temp paths
          - cortex.config.json contains AHHOME machine paths
          - health-reports/ has user-specific diagnostic data
          
          Impact:
          - Privacy violation (machine names, usernames exposed)
          - Distribution bloat (unnecessary test artifacts)
          - Professionalism degradation (dev artifacts in user package)
          
          SKULL-006 prevents this by:
          1. Scanning published files for machine-specific patterns
          2. Requiring publish script exclude logs, coverage, health data
          3. Blocking publish if privacy leaks detected
          4. Enforcing template configs instead of real paths
          
          Implementation:
          - Add EXCLUDE_PATTERNS to publish script (logs, coverage, health)
          - Create test_publish_privacy.py that scans for leaks
          - Add pre-publish hook that runs privacy scan
          - Use cortex.config.template.json instead of cortex.config.json
      
      - rule_id: "SKULL_HEADER_FOOTER_IN_RESPONSE"
        name: "Faculty Integrity Check (SKULL-007)"
        severity: "blocked"
        description: "Publish package MUST contain ALL essential CORTEX faculties for full operation"
        
        detection:
          missing_faculties:
            - "Tier 0 (SKULL) not found"
            - "Tier 1 (Memory) not found"
            - "Tier 2 (Knowledge) not found"
            - "Tier 3 (Context) not found"
            - "Agents missing"
            - "Operations missing"
            - "Entry point missing"
          scope: ["published_files"]
        
        verification_required:
          - type: "comprehensive_faculty_test"
            description: "Test that verifies ALL CORTEX faculties exist in publish package"
            requirement: "test_cortex_fully_operational MUST pass"
          
          - type: "tier_verification"
            description: "Verify all 4 tiers (Tier 0-3) present"
            requirement: "brain_protector.py, conversation_manager.py, knowledge_graph/, context_intelligence.py"
          
          - type: "agent_verification"
            description: "Verify 10 specialist agents present"
            requirement: "cortex_agents/ directory with base_agent.py and agent implementations"
          
          - type: "entry_point_verification"
            description: "Verify GitHub Copilot integration files"
            requirement: "CORTEX.prompt.md and copilot-instructions.md in .github/"
          
          - type: "documentation_verification"
            description: "Verify user documentation present"
            requirement: "story.md, setup-guide.md, technical-reference.md, etc."
        
        alternatives:
          - "Use inclusion-based publish (copy ONLY essential files)"
          - "Create comprehensive faculty test that blocks publish if faculties missing"
          - "Maintain ESSENTIAL_FILES list of required CORTEX components"
        
        evidence_template: "Published CORTEX missing faculty: '{faculty_name}' - file not found: '{file_path}'"
        
        rationale: |
          SKULL-007: Faculty Integrity Check
          
          Real incident (2025-11-12):
          - Exclusion-based publish script too aggressive
          - Excluded 97.9% of files (good for privacy!)
          - BUT also excluded essential faculties:
            ‚ùå All 10 specialist agents missing
            ‚ùå Tier 1 conversation_tracker.py missing
            ‚ùå Entry points (CORTEX.prompt.md) missing
            ‚ùå Plugin system missing
          - Published CORTEX was incomplete and non-functional
          
          Impact:
          - Users copy broken CORTEX to their application
          - CORTEX cannot coordinate work (no agents)
          - CORTEX cannot remember (no Tier 1)
          - Copilot cannot find CORTEX (no entry points)
          - Result: Complete failure, wasted user time
          
          SKULL-007 prevents this by:
          1. Comprehensive test that verifies ALL faculties present
          2. Blocking publish if any faculty missing
          3. Listing exact files required for each faculty
          4. Testing BEFORE deployment (not discovery by users)
          
          The Brilliant Fix:
          Instead of exclusion-based publish (exclude dev files),
          switch to INCLUSION-based publish (include ONLY essentials):
          
          Benefits:
          - Simpler logic (copy what's needed vs exclude what's not)
          - Guaranteed completeness (explicit list of essentials)
          - No accidental omissions (inclusion list is exhaustive)
          - Better maintainability (clear intent)
          
          Implementation:
          - Create test_publish_faculties.py with test_cortex_fully_operational()
          - Test checks: Tier 0-3, Agents, Operations, Plugins, Entry Points, Docs
          - Publish script copies ONLY essential directories
          - Test runs BEFORE declaring publish complete
          
          Result:
          - Package size: 393 files, 3.8 MB (perfect!)
          - All faculties present: ‚úÖ
          - No privacy leaks: ‚úÖ  
          - CORTEX fully operational: ‚úÖ
      
      - rule_id: "SKULL_HEADER_FOOTER_IN_RESPONSE_LEGACY"
        name: "Header/Footer in Copilot Response (Legacy)"
        severity: "blocked"
        description: "Operation orchestrators MUST include formatted headers/footers in Copilot Chat response"
        
        detection:
          combined_keywords:
            orchestrator_execution:
              - "execute operation"
              - "orchestrator.execute"
              - "operation complete"
            missing_header_footer:
              - "formatted_header: None"
              - "formatted_footer: None"
              - "no header in response"
          scope: ["code", "test_output"]
          logic: "AND"
        
        verification_required:
          - type: "result_object_check"
            description: "Verify OperationResult contains formatted_header/footer"
            requirement: "result.formatted_header MUST NOT be None"
          
          - type: "response_formatter_check"
            description: "Verify ResponseFormatter uses stored headers"
            requirement: "Chat response MUST include header/footer in code blocks"
          
          - type: "visual_inspection"
            description: "Verify header appears in Copilot Chat window"
            requirement: "User MUST see copyright header + purpose + accomplishments"
        
        alternatives:
          - "Use format_minimalist_header() and store in result.formatted_header"
          - "Use format_completion_footer() and store in result.formatted_footer"
          - "Ensure ResponseFormatter wraps headers in code blocks for display"
          - "Add integration test verifying header presence in formatted response"
        
        evidence_template: |
          Operation '{operation_name}' executed but headers not in Copilot response
          
          Expected in Copilot Chat:
          ```
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            CORTEX {operation_name} Orchestrator v{version}
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          
          Profile: {profile} ‚îÇ Mode: {mode} ‚îÇ Started: {timestamp}
          
          üìã Purpose: {purpose}
          
          ¬© 2024-2025 Asif Hussain ‚îÇ Proprietary ‚îÇ github.com/asifhussain60/CORTEX
          ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
          ```
          
          Actual: Header missing or only in terminal
        
        rationale: |
          SKULL-006: Header/Footer in Copilot Response
          
          Real incident (2025-11-11):
          - User: "why is header not being displayed?"
          - Headers printing to terminal (stdout) correctly
          - But GitHub Copilot Chat response had NO header
          - ResponseFormatter suppressing headers after first operation
          - User specified they want header "in the copilot response in the chat window"
          
          Root Cause:
          1. Orchestrators print headers to stdout (terminal visibility)
          2. But stdout doesn't reach Copilot Chat window
          3. ResponseFormatter has _first_operation_shown flag (header suppression)
          4. User sees execution in terminal, but Chat response lacks context
          
          Why This Matters:
          - Copyright attribution must be visible to user
          - Purpose/profile provides context for what operation did
          - Accomplishments show value delivered
          - Headers make operations feel professional and informative
          - Chat is primary interface (terminal is secondary)
          
          Solution:
          1. Orchestrators generate formatted headers/footers
          2. Store in OperationResult.formatted_header/footer
          3. ResponseFormatter checks for stored headers (priority)
          4. Wraps headers in code blocks for proper display
          5. Also prints to terminal for immediate visibility
          
          SKULL-006 enforces this by:
          - Requiring formatted_header/footer in OperationResult
          - Integration tests verify headers in formatted response
          - Blocking completion if headers missing from Chat output
          - Ensuring copyright/attribution always visible
          
          Implementation:
          ```python
          # In orchestrator execute():
          formatted_header = format_minimalist_header(...)
          print(formatted_header)  # Terminal visibility
          
          # ... operation logic ...
          
          formatted_footer = format_completion_footer(...)
          print(formatted_footer)  # Terminal visibility
          
          return OperationResult(
              success=True,
              formatted_header=formatted_header,  # For Copilot Chat
              formatted_footer=formatted_footer   # For Copilot Chat
          )
          ```
      
      - rule_id: "SKULL_ALL_TESTS_MUST_PASS"
        name: "All Tests Must Pass (SKULL-007)"
        severity: "blocked"
        description: "Test suite MUST have 100% pass rate before claiming any work complete"
        
        detection:
          keywords:
            - "fixed ‚úÖ"
            - "complete ‚úÖ"
            - "done ‚úÖ"
            - "implemented ‚úÖ"
            - "ready for review"
            - "PR ready"
          with_test_failures:
            - "failed"
            - "FAILED"
            - "ERROR"
            - "test failures"
            - "X passed, Y failed"
          scope: ["response", "test_output"]
        
        verification_required:
          - type: "full_test_suite"
            description: "Run complete test suite (not just new tests)"
            requirement: "pytest exit code MUST be 0 (100% pass rate)"
          
          - type: "no_skipped_critical_tests"
            description: "Verify no critical tests are skipped"
            requirement: "Core functionality tests MUST run (not skipped)"
          
          - type: "test_count_validation"
            description: "Ensure test count doesn't decrease unexpectedly"
            requirement: "Total tests should increase or stay same (not decrease)"
        
        alternatives:
          - "Fix ALL failing tests before claiming completion"
          - "Mark work as 'IN PROGRESS' until tests pass"
          - "Revert changes if they break existing tests"
          - "Fix pre-existing failures first (clean baseline)"
        
        evidence_template: |
          Work claimed complete but tests failing!
          
          Test Results: {test_summary}
          - Passed: {passed_count}
          - Failed: {failed_count}  ‚ùå
          - Skipped: {skipped_count}
          
          SKULL-007 VIOLATION: Cannot claim "done ‚úÖ" with {failed_count} failures
        
        rationale: |
          SKULL-007: All Tests Must Pass
          
          Real incident (2025-11-11):
          - User: "are all tests passing?"
          - Agent: Ran tests, found 123 failed, 337 passed
          - Agent response: "No, not all tests are passing" (honest)
          - BUT: Agent claimed SKULL-006 work "complete ‚úÖ" earlier
          - Pre-existing failures create false confidence
          
          Why Pre-existing Failures Are Dangerous:
          1. Mask New Regressions:
             - Can't tell if new code broke something
             - "Already broken" becomes acceptable
             - Technical debt accumulates silently
          
          2. Create False Confidence:
             - "My tests pass" ‚â† "All tests pass"
             - Incomplete validation of changes
             - Integration issues hidden
          
          3. Compound Over Time:
             - Each feature adds more failures
             - "Just one more broken test" mentality
             - Eventually unmaintainable
          
          4. Undermine Trust:
             - Claims of completion ring hollow
             - Quality standards erode
             - Testing becomes performative
          
          Examples from Current Failures:
          - 123 failed tests (51% failure rate!)
          - Categories: Agent internals, platform issues, schema errors
          - Some tests testing wrong APIs (implementation changed)
          - Some tests have environmental dependencies
          - All must be fixed before claiming ANY work complete
          
          SKULL-007 Enforcement:
          1. BLOCKING severity - cannot proceed with failures
          2. Requires full test suite run (not just new tests)
          3. Exit code 0 mandatory (100% pass rate)
          4. No "works on my machine" exceptions
          5. No "will fix later" promises
          
          Allowed Exceptions:
          - Known flaky tests marked with @pytest.mark.flaky
          - Platform-specific tests properly skipped on other platforms
          - Optional feature tests when feature disabled in config
          
          Not Allowed:
          - "These failures are unrelated to my work"
          - "I'll fix them in next PR"
          - "Tests are broken, not my code"
          - "Only my new tests need to pass"
          
          Implementation Strategy:
          1. Fix critical blockers first (schema, imports)
          2. Fix by category (agents, ambient, tier1)
          3. Update tests if implementation changed
          4. Mark truly optional tests appropriately
          5. Achieve 100% pass rate
          6. Maintain 100% going forward
          
          This is NOT optional. This is core quality engineering.
      
      - rule_id: "SKULL_MULTI_TRACK_VALIDATION"
        name: "Multi-Track Configuration Validation (SKULL-008)"
        severity: "blocked"
        description: "Multi-track mode MUST have valid configuration with proper phase distribution"
        
        detection:
          keywords:
            - "enable multi-track"
            - "activate multi-track"
            - "create tracks"
            - "split tracks"
            - "multi-machine mode"
          scope: ["intent", "description"]
        
        verification_required:
          - type: "track_balance_check"
            description: "Verify workload balanced across tracks"
            requirement: "Track estimated hours MUST not differ by >30%"
          
          - type: "dependency_isolation_check"
            description: "Verify no cross-track dependencies"
            requirement: "Phase groups MUST be self-contained per track"
          
          - type: "machine_assignment_check"
            description: "Verify each machine assigned to exactly one track"
            requirement: "No machine overlap, no unassigned machines"
          
          - type: "fun_name_uniqueness"
            description: "Verify track names are unique and generated properly"
            requirement: "Track names MUST be deterministic and collision-free"
        
        alternatives:
          - "Run track distribution algorithm to validate balance"
          - "Use PhaseDistributor.distribute() to check dependency isolation"
          - "Verify machine count matches track count"
          - "Test track name generation for collision resistance"
        
        evidence_template: |
          Multi-track mode validation failed!
          
          Configuration Issues:
          - Track balance: {balance_check}
          - Dependencies: {dependency_check}
          - Machines: {machine_check}
          - Track names: {name_check}
          
          SKULL-008: Multi-track MUST be properly configured before use
        
        rationale: |
          SKULL-008: Multi-Track Configuration Validation
          
          Multi-track mode is powerful but requires careful setup:
          
          1. Workload Balance:
             - Tracks with vastly different hours ‚Üí bottlenecks
             - One machine idle while other overloaded
             - Race metrics meaningless if unfair
             Example: Track A (10h) vs Track B (40h) = broken
          
          2. Dependency Isolation:
             - Track A waiting on Track B output ‚Üí blocked
             - Cross-dependencies defeat parallel development
             - Must group dependent phases on same track
             Example: "setup" phases must complete before "processing"
          
          3. Machine Assignment:
             - Machine assigned to multiple tracks ‚Üí confusion
             - Unassigned machines ‚Üí wasted capacity
             - Clear 1:1 or 1:N mapping required
             Example: AHHOME on both tracks = which context?
          
          4. Track Name Uniqueness:
             - Collision-resistant generation
             - Deterministic (same input ‚Üí same name)
             - Human-memorable for commands
             Example: Hash collision ‚Üí wrong track loaded
          
          Why This Matters:
          - Prevents split-mode failures mid-development
          - Ensures race metrics are meaningful
          - Maintains track isolation guarantees
          - Makes "continue implementation for [track]" reliable
          
          Validation Points:
          
          Pre-Initialization:
          - Check machine count > 0
          - Verify operations.yaml accessible
          - Validate module definitions exist
          
          Post-Distribution:
          - Balance check: max_hours/min_hours < 1.3 (30% tolerance)
          - Dependency check: No phase in track requires other track's output
          - Machine check: Each machine in exactly one track
          - Name check: All track names unique and deterministic
          
          Integration Test Required:
          ```python
          def test_multi_track_validation():
              # Setup
              machines = ["AHHOME", "Mac"]
              config = create_multi_track_config(machines, modules)
              
              # Balance check
              hours = [t.estimated_hours for t in config.tracks.values()]
              assert max(hours) / min(hours) < 1.3, "Imbalanced tracks"
              
              # Dependency check
              for track in config.tracks.values():
                  deps = get_phase_dependencies(track.phases)
                  assert all(d in track.phases for d in deps), "Cross-track dep"
              
              # Machine check
              all_machines = [m for t in config.tracks.values() for m in t.machines]
              assert len(all_machines) == len(set(all_machines)), "Duplicate machine"
              
              # Name check
              names = [t.track_name for t in config.tracks.values()]
              assert len(names) == len(set(names)), "Duplicate track name"
          ```
          
          Enforcement:
          - CLI script validates before writing config
          - Design sync validates before split
          - Continue command validates track exists
          - Consolidation validates all tracks present
      
      - rule_id: "SKULL_TRACK_ISOLATION"
        name: "Track Work Isolation (SKULL-009)"
        severity: "blocked"
        description: "Work on Track A MUST NOT modify Track B's assigned modules"
        
        detection:
          combined_keywords:
            track_context:
              - "continue implementation for"
              - "working on track"
              - "active track"
            cross_modification:
              - "modified module"
              - "updated file"
              - "changed"
          scope: ["intent", "log_output"]
          logic: "AND"
        
        verification_required:
          - type: "module_ownership_check"
            description: "Verify modified modules belong to active track"
            requirement: "All changed files MUST be in active track's module list"
          
          - type: "phase_boundary_check"
            description: "Verify work stays within assigned phases"
            requirement: "Modified files MUST belong to active track's phases"
          
          - type: "git_diff_validation"
            description: "Verify git changes match track scope"
            requirement: "git diff MUST only show files from active track"
        
        alternatives:
          - "Filter implementation state by active track"
          - "Validate module ownership before allowing changes"
          - "Add pre-commit hook checking track boundaries"
          - "Switch to correct track before making changes"
        
        evidence_template: |
          Track isolation violation detected!
          
          Active Track: {active_track}
          Modified Files: {modified_files}
          Violations: {violations}
          
          Files belong to: {actual_track}
          
          SKULL-009: Tracks MUST NOT cross-modify each other's work
        
        rationale: |
          SKULL-009: Track Work Isolation
          
          Core principle: Each track is an isolated development context.
          
          Why Isolation Matters:
          
          1. Prevents Merge Conflicts:
             - Two machines editing same file ‚Üí disaster
             - Track A changes conflicting with Track B changes
             - Consolidation becomes manual merge nightmare
             Example: Both tracks fix same module differently
          
          2. Maintains Race Integrity:
             - Track A can't "cheat" by doing Track B's work
             - Progress metrics stay meaningful
             - Velocity calculations remain accurate
             Example: Track A does Track B's modules ‚Üí unfair race
          
          3. Enables True Parallel Development:
             - No coordination needed during work
             - No "wait for Track A to finish" scenarios
             - Maximum throughput achieved
             Example: Both machines working simultaneously without blocking
          
          4. Simplifies Context Management:
             - Each machine sees only relevant modules
             - Copilot context smaller and focused
             - Fewer tokens, faster responses
             Example: Track A context excludes Track B's 50 modules
          
          Enforcement Mechanism:
          
          1. Pre-Modification Check:
             ```python
             def validate_module_ownership(module_id, active_track):
                 if module_id not in active_track.modules:
                     raise TrackIsolationError(
                         f"Module {module_id} belongs to different track"
                     )
             ```
          
          2. Git Pre-Commit Hook:
             ```bash
             # Check if modified files belong to active track
             active_track=$(get_active_track)
             for file in $(git diff --cached --name-only); do
                 if ! track_owns_file "$active_track" "$file"; then
                     echo "Error: $file not in active track"
                     exit 1
                 fi
             done
             ```
          
          3. Design Sync Validation:
             - Compare git log with track assignments
             - Flag any cross-track modifications
             - Require explicit override with justification
          
          Allowed Cross-Track Work:
          - Shared files (cortex.config.json)
          - Documentation updates (README.md)
          - Test fixtures (tests/fixtures/)
          
          Not Allowed:
          - Modifying other track's modules
          - Changing other track's phase files
          - Updating other track's status in design doc
          
          Override Process:
          If cross-track work truly needed:
          1. Document why isolation must break
          2. Get explicit user approval
          3. Log violation for consolidation review
          4. Merge carefully during consolidation
          
          Integration Test:
          ```python
          def test_track_isolation():
              # Setup two tracks
              config = setup_multi_track(['AHHOME', 'Mac'])
              track_a = config.tracks['track_1']
              track_b = config.tracks['track_2']
              
              # Simulate Track A trying to modify Track B's module
              with pytest.raises(TrackIsolationError):
                  modify_module(track_b.modules[0], active_track=track_a)
              
              # Verify Track A can modify own modules
              modify_module(track_a.modules[0], active_track=track_a)  # OK
          ```
      
      - rule_id: "SKULL_CONSOLIDATION_INTEGRITY"
        name: "Track Consolidation Integrity (SKULL-010)"
        severity: "blocked"
        description: "Consolidation MUST merge all track progress accurately without data loss"
        
        detection:
          keywords:
            - "consolidate tracks"
            - "merge tracks"
            - "reset to single-track"
            - "design sync consolidation"
          scope: ["intent", "operation_name"]
        
        verification_required:
          - type: "progress_preservation_check"
            description: "Verify no completed modules lost in merge"
            requirement: "Consolidated count MUST equal sum of track counts"
          
          - type: "conflict_resolution_audit"
            description: "Log all conflict resolutions with justification"
            requirement: "Conflicts MUST be documented in archive"
          
          - type: "archive_completeness_check"
            description: "Verify split docs archived before deletion"
            requirement: "All split docs MUST exist in archive before removal"
          
          - type: "git_commit_validation"
            description: "Verify consolidation tracked in git history"
            requirement: "Git commit MUST reference both tracks and merge details"
        
        alternatives:
          - "Run consolidation with --verify flag"
          - "Review conflict resolution log before committing"
          - "Keep split docs until archive verified"
          - "Add integration test for consolidation accuracy"
        
        evidence_template: |
          Consolidation integrity check failed!
          
          Pre-Consolidation:
          - Track A: {track_a_completed}/{track_a_total} modules
          - Track B: {track_b_completed}/{track_b_total} modules
          - Total: {pre_total_completed} modules
          
          Post-Consolidation:
          - Unified: {post_total_completed} modules
          
          Discrepancy: {discrepancy} modules
          Conflicts Resolved: {conflicts}
          Archive Status: {archive_status}
          
          SKULL-010: Consolidation MUST preserve all progress
        
        rationale: |
          SKULL-010: Track Consolidation Integrity
          
          Consolidation is the critical merge operation - must be perfect.
          
          What Can Go Wrong:
          
          1. Progress Loss:
             - Track A shows module complete
             - Consolidation misses it
             - User loses work (demoralizing)
             Example: Track A completed 15 modules, only 12 appear in merge
          
          2. Conflict Mishandling:
             - Both tracks modified same module
             - Wrong version selected
             - Work overwritten silently
             Example: Track A's fix lost, Track B's bug remains
          
          3. Archive Failure:
             - Split docs deleted before archiving
             - No way to audit merge decisions
             - Can't roll back if issues found
             Example: User wants to see what Track A had, archive empty
          
          4. Git History Gaps:
             - Consolidation not committed properly
             - Can't trace what was merged when
             - Audit trail incomplete
             Example: Merge happened, git log says nothing
          
          Consolidation Algorithm:
          
          ```python
          def consolidate_tracks(track_config, impl_state):
              # Step 1: Collect all track progress
              all_modules = {}
              for track in track_config.tracks.values():
                  for module_id in track.modules:
                      status = get_module_status(module_id, impl_state)
                      
                      # Conflict detection
                      if module_id in all_modules:
                          conflict = resolve_conflict(
                              all_modules[module_id],
                              status,
                              strategy='latest_timestamp'
                          )
                          log_conflict_resolution(module_id, conflict)
                          all_modules[module_id] = conflict.winner
                      else:
                          all_modules[module_id] = status
              
              # Step 2: Validate counts
              pre_count = sum(t.metrics.modules_completed for t in track_config.tracks.values())
              post_count = sum(1 for s in all_modules.values() if s.completed)
              
              if pre_count != post_count:
                  raise ConsolidationError(
                      f"Progress mismatch: {pre_count} ‚Üí {post_count}"
                  )
              
              # Step 3: Archive split docs
              archive_dir = create_archive_directory()
              for status_file in get_split_design_docs():
                  archive_file(status_file, archive_dir)
              
              # Step 4: Generate consolidated doc
              consolidated = generate_consolidated_document(
                  all_modules,
                  track_config,
                  archive_reference=archive_dir
              )
              
              # Step 5: Git commit with full details
              commit_message = f"""design: consolidate multi-track progress
              
              Tracks merged:
              - {track_config.tracks['track_1'].track_name}: {track_config.tracks['track_1'].metrics.completion_percentage}%
              - {track_config.tracks['track_2'].track_name}: {track_config.tracks['track_2'].metrics.completion_percentage}%
              
              Total progress: {post_count}/{len(all_modules)} modules ({post_count/len(all_modules)*100:.0f}%)
              Conflicts resolved: {len(get_conflicts())}
              Archive: {archive_dir.name}
              
              [design_sync consolidation]
              """
              
              git_commit(consolidated, commit_message)
              
              return consolidated
          ```
          
          Conflict Resolution Strategy:
          
          Default: Latest Timestamp Wins
          - Simple, deterministic, predictable
          - Assumes most recent work is correct
          - Logged for audit
          
          Example:
          ```
          Module: platform_detection
          - Track A: marked complete 2025-11-11 14:00
          - Track B: marked complete 2025-11-11 15:00
          Winner: Track B (later timestamp)
          Logged: conflict-resolution.yaml
          ```
          
          Archive Structure:
          ```
          cortex-brain/archived-tracks/20251111-164530/
          ‚îú‚îÄ‚îÄ CORTEX2-STATUS-SPLIT.MD       # Original split doc
          ‚îú‚îÄ‚îÄ track-1-history.jsonl         # Track A progress log
          ‚îú‚îÄ‚îÄ track-2-history.jsonl         # Track B progress log
          ‚îú‚îÄ‚îÄ conflicts-resolved.yaml       # Conflict resolution log
          ‚îî‚îÄ‚îÄ consolidation-report.md       # Summary of merge
          ```
          
          Integration Test:
          ```python
          def test_consolidation_integrity():
              # Setup: Two tracks with overlapping work
              config = create_multi_track(['AHHOME', 'Mac'])
              track_a_complete = mark_modules_complete(config.tracks['track_1'], [0, 1, 2])
              track_b_complete = mark_modules_complete(config.tracks['track_2'], [3, 4, 5])
              
              # Introduce conflict: both complete module 2
              mark_complete(config.tracks['track_1'], 'module_2', timestamp='14:00')
              mark_complete(config.tracks['track_2'], 'module_2', timestamp='15:00')
              
              # Consolidate
              consolidated = consolidate_tracks(config, impl_state)
              
              # Verify counts
              assert consolidated.modules_completed == 6, "Progress lost"
              
              # Verify conflict handled
              conflicts = get_conflict_log()
              assert 'module_2' in conflicts, "Conflict not logged"
              assert conflicts['module_2']['winner'] == 'track_2', "Wrong winner"
              
              # Verify archive
              archive = get_latest_archive()
              assert archive.exists(), "Archive missing"
              assert (archive / 'CORTEX2-STATUS-SPLIT.MD').exists(), "Split doc not archived"
              
              # Verify git
              commit = get_latest_commit()
              assert 'consolidate multi-track' in commit.message
              assert 'track_1' in commit.message
              assert 'track_2' in commit.message
          ```
          
          User Experience:
          ```
          $ /CORTEX design sync
          
          üèÅ Multi-Track Mode: Running design sync consolidation
             Will merge all tracks into unified status
          
          [Phase 1/6] Discovering live implementation state...
          ‚úÖ Track A (Blazing Phoenix): 8/15 modules (53%)
          ‚úÖ Track B (Swift Falcon): 12/18 modules (67%)
          
          [Phase 5/6] Consolidating tracks...
          ‚öôÔ∏è  Merging progress from 2 tracks...
          ‚ö†Ô∏è  Conflict detected: platform_detection
              Track A: complete @ 14:00
              Track B: complete @ 15:00
              Resolution: Track B wins (latest timestamp)
          
          ‚úÖ Consolidated 2 tracks into unified document
             Combined: 20/33 modules (61%)
             Conflicts resolved: 1 (logged)
             Archive: cortex-brain/archived-tracks/20251111-164530/
          
          [Phase 6/6] Committing changes...
          üíæ Git commit: 7a3b9c2 "design: consolidate multi-track progress"
          
          Design Sync ‚úÖ COMPLETED in 4.2s
             ‚Ä¢ Merged 2 tracks: Blazing Phoenix (53%) + Swift Falcon (67%)
             ‚Ä¢ Combined progress: 20/33 modules (61%)
             ‚Ä¢ Conflicts resolved: 1
             ‚Ä¢ Archived split docs
             ‚Ä¢ Reset to single-track mode
          ```
  
  # Layer 6: Knowledge Quality
  - layer_id: "knowledge_quality"
    name: "Knowledge Quality"
    description: "Pattern validation and confidence thresholds"
    priority: 6
    
    rules:
      - rule_id: "MIN_OCCURRENCES"
        name: "Minimum Occurrences for High Confidence"
        severity: "warning"
        description: "High confidence (>0.50) with single occurrence"
        
        detection:
          combined_keywords:
            high_confidence:
              - "confidence: 1.0"
              - "confidence=1.0"
              - "confidence: 0.95"
            single_event:
              - "first occurrence"
              - "single event"
              - "occurrences: 1"
          scope: ["description"]
          logic: "AND"  # Both conditions must be true
        
        alternatives:
          - "Start with confidence ‚â§0.50 for single event"
          - "Wait for 3+ occurrences before high confidence"
          - "Mark as provisional pattern"
        
        evidence: "Require 3+ occurrences for confidence >0.50"
      
      - rule_id: "PATTERN_VALIDATION"
        name: "Pattern Validation"
        severity: "warning"
        description: "Pattern lacks validation evidence"
        
        detection:
          keywords:
            - "add pattern without validation"
            - "no evidence"
            - "unverified pattern"
          scope: ["description"]
        
        alternatives:
          - "Add validation test"
          - "Link to source documentation"
          - "Mark as hypothesis for validation"
        
        evidence: "Patterns require empirical validation"
  
  # Layer 7: Commit Integrity
  - layer_id: "commit_integrity"
    name: "Commit Integrity"
    description: "Brain state files excluded from commits"
    priority: 7
    
    rules:
      - rule_id: "BRAIN_STATE_GITIGNORE"
        name: "Brain State Files Not Committed"
        severity: "warning"
        description: "Brain state file should not be committed"
        
        detection:
          files: "{{brain_state_files}}"
          keywords:
            - "commit"
          scope: ["intent"]
        
        alternatives:
          - "Add to .gitignore"
          - "Keep local-only"
          - "Export as snapshot if needed for sharing"
        
        evidence: "Add to .gitignore to prevent pollution"
      
      - rule_id: "TEMP_FILES_COMMIT"
        name: "Temporary Files Not Committed"
        severity: "warning"
        description: "Temporary or generated files should not be committed"
        
        detection:
          path_patterns:
            - "**/*.tmp"
            - "**/temp_*"
            - "**/__pycache__/**"
            - "**/node_modules/**"
          keywords:
            - "commit"
          scope: ["intent"]
        
        alternatives:
          - "Update .gitignore"
          - "Clean before commit"
          - "Use .gitkeep for empty directories"
        
        evidence: "Temporary files pollute repository"
  
  # Layer 8: Git Isolation (CRITICAL)
  - layer_id: "git_isolation"
    name: "Git Isolation Enforcement"
    description: "CORTEX code MUST NEVER be committed to user application repositories"
    priority: 8
    
    rules:
      - rule_id: "GIT_ISOLATION_ENFORCEMENT"
        name: "CORTEX Code Isolation from User Repos"
        severity: "blocked"
        description: "CRITICAL: CORTEX source code, brain files, or internal components being committed to user application repository"
        
        detection:
          path_patterns:
            - "**/src/tier0/**"
            - "**/src/tier1/**"
            - "**/src/tier2/**"
            - "**/src/tier3/**"
            - "**/src/cortex_agents/**"
            - "**/src/plugins/**"
            - "**/src/crawlers/**"
            - "**/cortex-brain/**"
            - "**/prompts/**"
            - "**/scripts/cortex/**"
            - "**/CORTEX/**"
          in_repo: "user_application"  # Detection: not in CORTEX repo
          
        alternatives:
          - "Keep CORTEX as separate repository/package"
          - "Install CORTEX via pip/npm (when distributed)"
          - "Use git submodule if local development required"
          - "Ensure .gitignore excludes CORTEX directories"
        
        evidence_template: |
          üö® CRITICAL VIOLATION: Git Isolation Breach
          
          CORTEX code detected in user application repository!
          File: '{path}'
          
          CORTEX MUST remain isolated from user application code:
          - User App Repo: Application-specific code only
          - CORTEX Repo: Framework code (separate repository)
          - Knowledge Sharing: Via exported YAML (team-knowledge/)
          
          Brain knowledge (cortex-brain/) is LOCAL ONLY - never committed anywhere.
        
        rationale: |
          GIT_ISOLATION_ENFORCEMENT: Core CORTEX Principle
          
          CORTEX operates as a SEPARATE cognitive layer:
          
          ‚ùå NEVER DO THIS:
          UserApp/
          ‚îú‚îÄ‚îÄ src/                    # User's application code
          ‚îú‚îÄ‚îÄ cortex-brain/           # ‚ùå WRONG - Don't commit brain!
          ‚îú‚îÄ‚îÄ src/tier0/              # ‚ùå WRONG - Don't copy CORTEX code!
          ‚îú‚îÄ‚îÄ src/cortex_agents/      # ‚ùå WRONG - Keep CORTEX separate!
          ‚îî‚îÄ‚îÄ .git/
          
          ‚úÖ CORRECT SETUP:
          UserApp/
          ‚îú‚îÄ‚îÄ src/                    # User's application code
          ‚îú‚îÄ‚îÄ team-knowledge/         # ‚úÖ OK - Exported YAML patterns
          ‚îú‚îÄ‚îÄ .gitignore              # ‚úÖ Must include: cortex-brain/
          ‚îî‚îÄ‚îÄ .git/
          
          CORTEX/ (separate repo)
          ‚îú‚îÄ‚îÄ src/tier0/              # ‚úÖ CORTEX framework code
          ‚îú‚îÄ‚îÄ src/cortex_agents/      # ‚úÖ Agent system
          ‚îú‚îÄ‚îÄ cortex-brain/           # ‚úÖ Local brain (not in git)
          ‚îî‚îÄ‚îÄ .git/
          
          Why This Matters:
          1. Separation of Concerns: Framework vs. Application
          2. Licensing: CORTEX proprietary, user code their own license
          3. Updates: CORTEX updates don't pollute user repos
          4. Security: Brain knowledge stays local, never exposed
          5. Clarity: Clear boundary between "your code" and "framework"
          
          Git Hooks (setup during init):
          - pre-commit: Scans for CORTEX paths, blocks commit if found
          - pre-push: Double-check no CORTEX code being pushed
          
          Exception: team-knowledge/ YAML exports allowed (knowledge sharing)
      
      - rule_id: "GIT_HOOKS_INSTALLATION"
        name: "Git Hooks Must Be Installed During Setup"
        severity: "blocked"
        description: "Setup process must install git hooks to prevent accidental CORTEX code commits"
        
        detection:
          keywords:
            - "skip git hooks"
            - "disable hooks"
            - "bypass hook installation"
          scope: ["intent", "description"]
        
        alternatives:
          - "Run 'cortex init' to install hooks automatically"
          - "Manually run setup script with hooks enabled"
          - "Never bypass hook installation (critical protection)"
        
        evidence_template: "Git hooks are MANDATORY for CORTEX isolation protection"
        
        rationale: |
          Git hooks provide automatic enforcement:
          
          pre-commit hook:
          - Scans staged files for CORTEX paths
          - Blocks commit if any CORTEX code detected
          - Shows clear error message with alternatives
          
          pre-push hook:
          - Final safety check before push
          - Prevents accidental exposure of CORTEX code
          
          Installation: Automatic during 'cortex init'
          Location: UserApp/.git/hooks/ (user's repo, not CORTEX repo)

  # Layer 6: Namespace Protection (Knowledge Boundary Enforcement)
  - layer_id: "namespace_protection"
    name: "Knowledge Namespace Boundaries"
    description: "Enforce separation between CORTEX framework and workspace knowledge"
    priority: 6
    
    rules:
      - rule_id: "NAMESPACE-001"
        name: "Protected CORTEX Namespace"
        severity: "blocked"
        description: "Prevent user code from writing to cortex.* namespace"
        
        detection:
          combined_keywords:
            namespace_write:
              - "learn_pattern"
              - "store_pattern"
              - "add pattern"
            cortex_namespace:
              - "cortex.*"
              - "namespace='cortex."
              - 'namespace="cortex.'
            not_framework:
              - "is_cortex_internal=False"
              - "user code"
          scope: ["code", "intent"]
          logic: "AND"
        
        alternatives:
          - "Use workspace.* namespace for application patterns"
          - "workspace.myapp.* for your project-specific knowledge"
          - "Only CORTEX framework code can write to cortex.* namespace"
        
        evidence_template: |
          NAMESPACE PROTECTION VIOLATION
          
          Attempted write to protected cortex.* namespace from user code!
          
          Code: '{code_snippet}'
          
          CORTEX namespaces are PROTECTED:
          - cortex.* ‚Üí Framework knowledge only (read-only for users)
          - workspace.* ‚Üí Application knowledge (user read/write)
          
          Use: learn_pattern(..., namespace="workspace.myapp.*", is_cortex_internal=False)
        
        rationale: |
          NAMESPACE-001: Protected CORTEX Namespace
          
          Critical architectural boundary preventing knowledge contamination.
          
          Why This Matters:
          1. Framework Integrity: CORTEX patterns must remain pure
          2. Multi-Project Support: Each workspace isolated
          3. Knowledge Quality: No user app patterns in framework brain
          4. Upgradability: CORTEX can update without breaking user data
          
          Protected Namespaces:
          - cortex.tier_architecture (4-tier brain system)
          - cortex.agent_patterns (10 specialist agents)
          - cortex.operations (universal operations)
          - cortex.plugins (plugin system)
          
          Allowed Namespaces:
          - workspace.<project>.* (your application patterns)
          - workspace.myapp.security (JWT, OAuth patterns)
          - workspace.myapp.architecture (file structure, tech stack)
          
          This rule is BLOCKING severity - violations stop execution immediately.
      
      - rule_id: "NAMESPACE-002"
        name: "Workspace Isolation"
        severity: "warning"
        description: "Isolate workspace patterns by owner/project"
        
        detection:
          combined_keywords:
            cross_workspace:
              - "workspace.projectA"
              - "workspace.projectB"
            shared_pattern:
              - "pattern applies to both"
              - "copy to other project"
          scope: ["intent", "description"]
          logic: "AND"
        
        alternatives:
          - "Store pattern in each workspace separately"
          - "Use cortex.* for truly generic framework patterns"
          - "Create explicit cross-workspace link if needed"
        
        evidence_template: |
          CROSS-WORKSPACE CONTAMINATION RISK
          
          Pattern appears to span multiple workspaces: '{description}'
          
          Best Practices:
          - workspace.projectA.* ‚Üí Project A only
          - workspace.projectB.* ‚Üí Project B only
          - cortex.* ‚Üí Framework generic knowledge
          
          If truly shared, use explicit relationship links, not duplicate storage.
        
        rationale: |
          NAMESPACE-002: Workspace Isolation
          
          Each workspace (project) should have isolated knowledge.
          
          Benefits:
          1. Clean Separation: No cross-project contamination
          2. Parallel Development: Multiple projects on same machine
          3. Easier Cleanup: Delete workspace.projectA.* removes all traces
          4. Privacy: Project A can't see Project B patterns
          
          Example Structure:
          - workspace.ksessions.* ‚Üí KSESSIONS project patterns
          - workspace.noor.* ‚Üí NOOR Canvas project patterns
          - workspace.cortex.* ‚Üí CORTEX development patterns (meta!)
          
          This rule is WARNING severity - allowed but discouraged.
      
      - rule_id: "NAMESPACE-003"
        name: "No Namespace Mixing"
        severity: "blocked"
        description: "Prevent patterns from spanning multiple namespaces"
        
        detection:
          combined_keywords:
            multi_namespace:
              - "namespaces=['cortex."
              - "namespaces=['workspace.a', 'workspace.b']"
            pattern_storage:
              - "store_pattern"
              - "learn_pattern"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "Store pattern in single primary namespace"
          - "Use relationship links for cross-namespace references"
          - "Duplicate pattern if truly applicable to both (rare)"
        
        evidence_template: |
          NAMESPACE MIXING VIOLATION
          
          Pattern assigned to multiple namespaces: '{namespaces}'
          
          A pattern MUST belong to exactly ONE namespace.
          
          If pattern applies to multiple contexts, use explicit links:
          - Primary: workspace.myapp.auth_pattern
          - Link: cortex.security_patterns ‚Üí workspace.myapp.auth_pattern
          
          This maintains clear ownership and prevents ambiguity.
        
        rationale: |
          NAMESPACE-003: No Namespace Mixing
          
          Single Ownership Principle: Each pattern has ONE home.
          
          Why Single Namespace:
          1. Clear Ownership: No ambiguity about who maintains pattern
          2. Clean Deletion: Removing workspace.* removes all patterns
          3. No Orphans: Pattern lifecycle tied to single namespace
          4. Simpler Queries: No multi-namespace resolution logic
          
          Cross-Namespace References:
          Use relationship links instead of multi-namespace patterns:
          
          ‚ùå BAD:
          learn_pattern(
              ...,
              namespaces=["cortex.security", "workspace.myapp.security"]
          )
          
          ‚úÖ GOOD:
          # Store in primary namespace
          pattern_id = learn_pattern(
              ...,
              namespaces=["workspace.myapp.security"]
          )
          
          # Link to generic pattern
          create_relationship(
              from_pattern="cortex.security_best_practices",
              to_pattern=pattern_id,
              relationship_type="implements"
          )
          
          This rule is BLOCKING severity - multi-namespace patterns rejected.
  
  # Layer 9: Database Architecture Enforcement
  - layer_id: "database_architecture"
    name: "Distributed Database Architecture"
    description: "CORTEX uses tier-specific databases, never monolithic cortex-brain.db"
    priority: 9
    
    rules:
      - rule_id: "DISTRIBUTED_DATABASE_ARCHITECTURE"
        name: "Use Tier-Specific Databases (Never Monolithic)"
        severity: "blocked"
        description: "Code referencing monolithic cortex-brain.db instead of tier-specific databases"
        
        detection:
          combined_keywords:
            monolithic_reference:
              - "cortex-brain/cortex-brain.db"
              - "cortex-brain.db"
              - 'db_path: str = "cortex-brain.db"'
              - "default='cortex-brain/cortex-brain.db'"
            not_test_file:
              - "!**/tests/**"
              - "!**/test-*.py"
              - "!**/benchmark-*.ts"
          scope: ["code", "file_path"]
          logic: "AND"
        
        alternatives:
          - "Tier 1 (Conversations): Use cortex-brain/tier1/conversations.db"
          - "Tier 1 (Working Memory): Use cortex-brain/tier1/working_memory.db"
          - "Tier 2 (Knowledge Graph): Use cortex-brain/tier2/knowledge_graph.db"
          - "Tier 3 (Context): Use cortex-brain/tier3/context.db"
          - "Use ConfigManager to get correct tier-specific path"
        
        evidence_template: |
          üö® DATABASE ARCHITECTURE VIOLATION
          
          Monolithic database reference detected: '{path}'
          
          CORTEX uses distributed database architecture:
          ‚ùå WRONG: cortex-brain/cortex-brain.db (doesn't exist!)
          
          ‚úÖ CORRECT:
          - Tier 1: cortex-brain/tier1/conversations.db (chat history)
          - Tier 1: cortex-brain/tier1/working_memory.db (active context)
          - Tier 2: cortex-brain/tier2/knowledge_graph.db (learned patterns)
          - Tier 3: cortex-brain/tier3/context.db (development metrics)
          
          File: {file}
          Line: {line}
        
        rationale: |
          DISTRIBUTED_DATABASE_ARCHITECTURE: Core CORTEX 2.0 Design
          
          CORTEX 2.0 migrated from monolithic to distributed database architecture.
          
          ‚ùå OLD (CORTEX 1.0):
          cortex-brain/
          ‚îî‚îÄ‚îÄ cortex-brain.db  # Monolithic (conversations + knowledge + context)
          
          ‚úÖ NEW (CORTEX 2.0):
          cortex-brain/
          ‚îú‚îÄ‚îÄ tier1/
          ‚îÇ   ‚îú‚îÄ‚îÄ conversations.db       # Last 20 conversations
          ‚îÇ   ‚îî‚îÄ‚îÄ working_memory.db      # Active session context
          ‚îú‚îÄ‚îÄ tier2/
          ‚îÇ   ‚îî‚îÄ‚îÄ knowledge_graph.db     # Learned patterns + capabilities
          ‚îî‚îÄ‚îÄ tier3/
              ‚îî‚îÄ‚îÄ context.db             # Git metrics + test coverage + health
          
          Why Distributed?
          
          1. Separation of Concerns:
             - Tier 1: Conversational data (fast, frequently accessed)
             - Tier 2: Strategic knowledge (periodic reads, rare writes)
             - Tier 3: Development context (external data sources)
          
          2. Performance:
             - Smaller databases = faster queries
             - No lock contention between tiers
             - Independent backup/restore per tier
          
          3. Scalability:
             - Each tier can scale independently
             - Can distribute across different storage
             - Clear upgrade paths per tier
          
          4. Maintainability:
             - Schema changes isolated to tier
             - Migrations simpler (per-tier)
             - Clear ownership boundaries
          
          Common Violations:
          
          1. Hardcoded Default Paths:
             ```python
             # ‚ùå WRONG
             def __init__(self, db_path: str = "cortex-brain.db"):
                 pass
             
             # ‚úÖ CORRECT
             def __init__(self, db_path: str = None):
                 if db_path is None:
                     db_path = ConfigManager.get_tier1_conversations_path()
             ```
          
          2. Migration Scripts Still Using Old Path:
             ```python
             # ‚ùå WRONG
             parser.add_argument('--db-path', default='cortex-brain/cortex-brain.db')
             
             # ‚úÖ CORRECT
             parser.add_argument('--tier', choices=['tier1', 'tier2', 'tier3'])
             db_path = get_tier_database_path(args.tier)
             ```
          
          3. Documentation References:
             ```markdown
             ‚ùå WRONG: "Creates cortex-brain.db in KSESSIONS"
             ‚úÖ CORRECT: "Creates tier-specific databases: tier1/conversations.db, tier2/knowledge_graph.db, tier3/context.db"
             ```
          
          Files Commonly Affected:
          - src/router.py (routing logic)
          - src/context_injector.py (context management)
          - src/brain/tier1/request_logger.py (conversation logging)
          - src/brain/tier1/tier1_api.py (Tier 1 API)
          - src/brain/tier1/__init__.py (Tier 1 initialization)
          - scripts/cortex/migrate-*.py (migration scripts)
          
          How to Fix:
          
          1. Identify which tier the code needs:
             - Conversations/history ‚Üí Tier 1 (conversations.db)
             - Working memory/session ‚Üí Tier 1 (working_memory.db)
             - Patterns/capabilities ‚Üí Tier 2 (knowledge_graph.db)
             - Git/tests/health ‚Üí Tier 3 (context.db)
          
          2. Use ConfigManager for paths:
             ```python
             from src.config import ConfigManager
             
             config = ConfigManager()
             conversations_db = config.get_tier1_conversations_path()
             knowledge_db = config.get_tier2_knowledge_path()
             context_db = config.get_tier3_context_path()
             ```
          
          3. Update tests to use tier-specific paths:
             ```python
             # Test fixtures should mirror production structure
             @pytest.fixture
             def tier1_db(tmp_path):
                 return tmp_path / "tier1" / "conversations.db"
             ```
          
          4. Update documentation references:
             - README.md
             - Architecture docs
             - Setup guides
             - Migration instructions
          
          Exception: Test Files
          - Test files (tests/**, test-*.py, benchmark-*.ts) can use custom paths
          - Use clear naming: test-cortex-brain.db (not cortex-brain.db)
          
          Integration Test Required:
          ```python
          def test_no_monolithic_references():
              """Ensure no production code references cortex-brain.db"""
              violations = []
              
              for file in Path('src').rglob('*.py'):
                  content = file.read_text()
                  if 'cortex-brain.db' in content:
                      violations.append(str(file))
              
              assert not violations, (
                  f"Monolithic DB references found: {violations}\n"
                  f"Use tier-specific paths instead!"
              )
          ```
          
          Optimize Operation Validation:
          The optimize_cortex_orchestrator should validate:
          - ‚úÖ All tier databases exist
          - ‚úÖ No references to cortex-brain.db in src/
          - ‚úÖ ConfigManager returns correct paths
          - ‚úÖ Migration scripts use tier-specific args
          - ‚úÖ Documentation reflects distributed architecture

  # Layer 11: Template Architecture Protection
  - layer_id: "template_architecture"
    name: "Template Architecture Protection"
    description: "Protect URT v3.0 intelligent template system architectural invariants"
    priority: 11
    
    rules:
      - rule_id: "TEMPLATE_TOKEN_BUDGET_ENFORCEMENT"
        name: "Token Budget Limits Enforced"
        severity: "blocked"
        description: "Token budget limits must remain: CONCISE=400, SUMMARIZED=600, DETAILED=800, VISUAL=500"
        
        detection:
          path_patterns:
            - "src/core/template_renderer.py"
            - "cortex-brain/response-templates/*.yaml"
          combined_keywords:
            token_budget_change:
              - "token_budget"
              - "budget ="
            invalid_value:
              - "token_budget: int = 1200"
              - "token_budget: int = 300"
              - "budget = 1000"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "CONCISE format: 400 tokens (quick info, status checks)"
          - "SUMMARIZED format: 600 tokens (summary + collapsible details)"
          - "DETAILED format: 800 tokens (full subsections)"
          - "VISUAL format: 500 tokens (tables, progress bars)"
          - "Document rationale for any budget changes with performance data"
        
        evidence_template: |
          Token budget modification detected: '{code_snippet}'
          
          CRITICAL: URT v3.0 token budgets are optimized values
          - 42% token reduction achieved (800 ‚Üí 466 avg)
          - Validated with 21 integration tests
          
          Changes require performance analysis validation.
        
        rationale: |
          URT v3.0 token budgets are carefully optimized:
          - CONCISE: 400 tokens (simple help/status queries)
          - SUMMARIZED: 600 tokens (implementation summaries)
          - DETAILED: 800 tokens (complex planning/analysis)
          - VISUAL: 500 tokens (metrics/diagrams)
          
          Changes must be validated with performance metrics.
      
      - rule_id: "CHALLENGE_MODE_ROUTING_INTEGRITY"
        name: "Challenge Mode Routing Logic"
        severity: "blocked"
        description: "Challenge mode routing (SKIP/ACCEPT_ONLY/CHALLENGE_ONLY/MIXED/INTELLIGENT) must remain valid"
        
        detection:
          path_patterns:
            - "src/core/template_renderer.py"
          combined_keywords:
            challenge_logic:
              - "ChallengeMode"
              - "_determine_challenge_mode"
            bypass_pattern:
              - "return ChallengeMode.SKIP"
              - "# Bypass all validation"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "Preserve intelligent routing based on context analysis"
          - "SKIP mode only for simple info requests (help, status)"
          - "INTELLIGENT mode for complex planning/validation"
          - "Document any routing logic changes with test cases"
        
        evidence_template: "Challenge routing bypass detected: '{code_snippet}'"
        
        rationale: |
          Challenge mode routing is core to URT v3.0:
          - Eliminates forced challenge display when unnecessary
          - Intelligent validation for complex requests
          - User feedback: "No more forced challenge when nothing to validate"
      
      - rule_id: "TEMPLATE_SCHEMA_VALIDATION"
        name: "Template Orchestration Metadata Schema"
        severity: "warning"
        description: "Templates must include orchestration metadata: relevance_keywords, priority, composability"
        
        detection:
          path_patterns:
            - "cortex-brain/response-templates/*.yaml"
          keywords:
            - "new template"
            - "template_id:"
          without_keywords:
            - "orchestration:"
            - "relevance_keywords:"
          scope: ["code"]
        
        alternatives:
          - "Add orchestration metadata block with relevance_keywords, priority, category"
          - "Include composability rules: compatible_with, conflicts_with"
          - "See cortex-brain/response-templates/base-template-v2.yaml for schema"
        
        evidence_template: "Template missing orchestration metadata: '{template_id}'"

  # Layer 12: Context Management Architecture Protection
  - layer_id: "context_management_architecture"
    name: "Context Management Architecture Protection"
    description: "Protect unified context management system (Phase 1-2) architectural invariants"
    priority: 12
    
    rules:
      - rule_id: "TIER_BOUNDARY_ISOLATION"
        name: "Tier Boundary Isolation Enforcement"
        severity: "blocked"
        description: "T1/T2/T3 must be accessed through UnifiedContextManager, no direct cross-tier database access"
        
        detection:
          path_patterns:
            - "src/tier1/**/*.py"
            - "src/tier2/**/*.py"
            - "src/tier3/**/*.py"
            - "src/cortex_agents/**/*.py"
          combined_keywords:
            cross_tier_access:
              - "from src.tier2"
              - "from src.tier3"
              - "tier2.query_directly"
              - "tier3.get_insights"
            not_unified_context:
              - "!UnifiedContextManager"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "Use UnifiedContextManager for all cross-tier context access"
          - "UnifiedContextManager orchestrates T1/T2/T3 loading with relevance scoring"
          - "Agents receive unified_context from UnifiedContextManager"
        
        evidence_template: |
          Direct cross-tier access detected: '{code_snippet}'
          
          VIOLATION: Bypassing UnifiedContextManager
          
          ‚úÖ CORRECT:
          context = unified_context_manager.build_context(...)
          # UnifiedContextManager handles T1/T2/T3 coordination
        
        rationale: |
          UnifiedContextManager enforces architectural boundaries:
          - Tier isolation (T1/T2/T3 separation)
          - Relevance-based loading prioritization
          - Token budget allocation fairness
          - Context deduplication and merging
      
      - rule_id: "TOKEN_BUDGET_ALLOCATION_FAIRNESS"
        name: "Token Budget Allocation Must Be Proportional"
        severity: "blocked"
        description: "Token budget allocation must be proportional to relevance scores, not fixed percentages"
        
        detection:
          path_patterns:
            - "src/core/context_management/token_budget_manager.py"
          combined_keywords:
            unfair_allocation:
              - "tier1: total_budget * 0.9"
              - "fixed allocation"
              - "/ len(tier_relevance)"
            not_proportional:
              - "!tier_relevance"
              - "!relevance_score"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "Allocate budget proportionally to tier relevance scores"
          - "High relevance = more tokens, low relevance = fewer tokens"
          - "Ensure total allocation ‚â§ total_budget (no overflow)"
        
        evidence_template: "Unfair token allocation detected: '{code_snippet}'"
        
        rationale: |
          Token budget allocation algorithm ensures fairness:
          - Proportional to relevance (high relevance ‚Üí more tokens)
          - Dynamic allocation (not fixed percentages)
          - Sum constraint enforced (total ‚â§ budget)
      
      - rule_id: "STALENESS_THRESHOLD_ENFORCEMENT"
        name: "Staleness Thresholds Must Be Enforced"
        severity: "blocked"
        description: "Staleness thresholds: T1=24h, T2=90d, T3=7d must be enforced"
        
        detection:
          path_patterns:
            - "src/core/context_management/context_quality_monitor.py"
          combined_keywords:
            staleness_bypass:
              - "def check_staleness"
              - "return False"
            or_threshold_change:
              - "tier1': 168"
              - "tier2': 180"
              - "tier3': 14"
          scope: ["code"]
          logic: "OR"
        
        alternatives:
          - "T1 staleness: 24 hours (conversations)"
          - "T2 staleness: 90 days (learned patterns)"
          - "T3 staleness: 7 days (git metrics/insights)"
          - "Document rationale for any threshold changes"
        
        evidence_template: "Staleness detection compromised: '{code_snippet}'"
        
        rationale: |
          Staleness thresholds prevent using outdated context:
          - T1: 24h (recent conversations relevant)
          - T2: 90d (patterns valid longer term)
          - T3: 7d (metrics need frequent refresh)
      
      - rule_id: "CROSS_TIER_LINKING_SCHEMA"
        name: "Cross-Tier Linking Schema Integrity"
        severity: "warning"
        description: "Cross-tier linking fields (used_patterns, used_metrics) must be maintained"
        
        detection:
          path_patterns:
            - "src/core/context_management/migrate_cross_tier_linking.py"
            - "src/tier1/**/*.py"
            - "src/tier2/**/*.py"
          keywords:
            - "used_patterns"
            - "used_metrics"
            - "applied_in_conversations"
          without_keywords:
            - "JSON serialization"
            - "json.dumps"
          scope: ["code"]
        
        alternatives:
          - "Store linking data as JSON-serialized lists"
          - "Update bidirectionally (T1 ‚Üí T2 and T2 ‚Üí T1)"
          - "Include context_quality_score for monitoring"
        
        evidence_template: "Cross-tier linking schema violation: '{code_snippet}'"

  # Layer 13: Multi-Template Composition Protection
  - layer_id: "multi_template_composition"
    name: "Multi-Template Composition Rules Protection"
    description: "Protect Multi-Template Orchestrator architectural invariants"
    priority: 13
    
    rules:
      - rule_id: "RELEVANCE_SCORING_WEIGHTS"
        name: "Relevance Scoring Weight Immutability"
        severity: "warning"
        description: "Relevance scoring weights: keyword=0.4, trigger=0.3, context=0.2, category=0.1 (sum=1.0)"
        
        detection:
          path_patterns:
            - "src/response_templates/multi_template_orchestrator.py"
          combined_keywords:
            weight_modification:
              - "keyword_weight"
              - "trigger_weight"
              - "context_weight"
              - "category_weight"
            invalid_values:
              - "keyword_weight = 0.6"
              - "trigger_weight = 0.5"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "Preserve validated weights: keyword=0.4, trigger=0.3, context=0.2, category=0.1"
          - "Changes require A/B testing with 1000+ queries for validation"
          - "Ensure weights sum to 1.0 (mathematical constraint)"
        
        evidence_template: |
          Relevance weight modification detected: '{code_snippet}'
          
          Weights are validated through extensive testing.
          Changes require performance analysis.
        
        rationale: |
          Relevance scoring weights are empirically validated:
          - keyword=0.4 (strongest signal)
          - trigger=0.3 (explicit phrases)
          - context=0.2 (metadata match)
          - category=0.1 (general relevance)
      
      - rule_id: "CONFLICT_RESOLUTION_PRIORITY"
        name: "Conflict Resolution Priority Map"
        severity: "blocked"
        description: "Priority map: error=100, security=90, planning=80, execution=70, validation=60, help=50, status=40, general=30"
        
        detection:
          path_patterns:
            - "src/response_templates/multi_template_orchestrator.py"
          combined_keywords:
            priority_change:
              - "PRIORITY_MAP"
              - "'error':"
            invalid_priority:
              - "'error': 50"
              - "'general': 100"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "error=100 (highest priority - user blocked)"
          - "security=90 (critical security issues)"
          - "planning=80 (important strategic work)"
          - "Preserve priority hierarchy for conflict resolution"
        
        evidence_template: "Priority map violation detected: '{code_snippet}'"
        
        rationale: |
          Priority map ensures critical templates win conflicts:
          - Errors block users ‚Üí highest priority
          - Security issues ‚Üí second highest
          - General info ‚Üí lowest priority
      
      - rule_id: "MAX_TEMPLATES_LIMIT"
        name: "Max Templates Composition Limit"
        severity: "warning"
        description: "max_templates=3 limit enforced (composition complexity management)"
        
        detection:
          path_patterns:
            - "src/response_templates/multi_template_orchestrator.py"
            - "cortex-brain/response-templates/*.yaml"
          combined_keywords:
            limit_increase:
              - "max_templates"
              - "= 5"
              - "= 10"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "max_templates=3 (validated composition complexity limit)"
          - "Performance target: < 500ms composition time"
          - "Changes require composition time benchmarking"
        
        evidence_template: "Max templates limit increase detected: '{code_snippet}'"
        
        rationale: |
          max_templates=3 limit prevents:
          - Excessive composition complexity
          - Response token bloat
          - Performance degradation (target: < 500ms)
      
      - rule_id: "TEMPLATE_COMPATIBILITY_MATRIX"
        name: "Template Compatibility Matrix Integrity"
        severity: "warning"
        description: "Template compatibility declarations must be reciprocal and conflict-free"
        
        detection:
          path_patterns:
            - "cortex-brain/response-templates-enhanced.yaml"
          combined_keywords:
            compatibility_violation:
              - "compatible_with"
              - "conflicts_with"
            circular_logic:
              - "compatible_with: [template_b]"
              - "conflicts_with: [template_b]"
          scope: ["code"]
          logic: "AND"
        
        alternatives:
          - "Ensure compatible_with declarations are reciprocal"
          - "Prevent circular conflicts (A compatible with B, A conflicts with B)"
          - "Validate compatibility matrix with automated tests"
        
        evidence_template: "Template compatibility violation: '{code_snippet}'"

# Severity levels
severity_levels:
  safe:
    decision: "ALLOW"
    override_required: false
    message: "‚úÖ Modification appears safe. No violations detected."
  
  warning:
    decision: "WARN"
    override_required: false
    message: "‚ö†Ô∏è WARNING: Risky patterns detected. Proceed with caution."
  
  blocked:
    decision: "BLOCK"
    override_required: true
    message: "üõ°Ô∏è BLOCKED: Critical architectural violations detected."

# Challenge templates
challenge_template: |
  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  üß† BRAIN PROTECTION CHALLENGE
  
  Timestamp: {timestamp}
  Severity: {severity}
  Decision: {decision}
  
  {message}
  
  SAFE ALTERNATIVES:
  {alternatives}
  
  ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  
  Options:
    1. Accept recommended alternative (SAFE)
    2. Provide different approach (REVIEW)
    3. Type 'OVERRIDE' with justification (RISKY)
  
  Your choice:

  # Layer 10: Implementation Velocity Protection (Anti-Inefficiency Patterns)
  # Based on Phase 0 conversation analysis - prevent time waste patterns that caused 3x development time
  layer_10_efficiency_protection:
    layer_name: "Implementation Velocity Protection"
    description: "Prevent inefficiency patterns that waste development time"
    scope: "implementation_approach"
    
    rules:
      - rule_id: "EFFICIENCY_REALITY_CHECK_FIRST"
        name: "Reality Check Before Documentation Trust (Anti-Pattern #1)"
        severity: "warning"
        description: "Prevent trusting status documents without verifying current reality"
        
        detection:
          keywords:
            - "according to documentation"
            - "status document says" 
            - "documented as complete"
            - "based on the specs"
          scope: ["intent", "description", "approach"]
        
        acceleration_pattern: "Check current reality ‚Üí Update understanding ‚Üí Take action"
        time_saved_per_instance: "10-15 minutes of incorrect baseline"
        
        alternatives:
          - "Check actual current state first"
          - "Verify implementation reality before planning"
          - "Update documentation after reality check"
        
        evidence_template: "Documentation trust pattern: '{description}'"
      
      - rule_id: "EFFICIENCY_BATCH_FIXES_OVER_MICRO_CYCLES"
        name: "Batch Operations Over Micro-Cycles (Anti-Pattern #2)"
        severity: "warning"
        description: "Prevent inefficient micro-cycles of fix‚Üítest‚Üífix‚Üítest"
        
        detection:
          keywords:
            - "fix one test"
            - "run full suite"
            - "individual test fix"
            - "micro-optimization"
          scope: ["intent", "description", "approach"]
        
        acceleration_pattern: "Group related operations ‚Üí Execute batch ‚Üí Verify once"
        time_saved_per_instance: "30-60 seconds per micro-cycle overhead"
        
        alternatives:
          - "Fix all integration tests ‚Üí Run integration suite"
          - "Fix all template tests ‚Üí Run template suite"
          - "Batch related fixes by category"
        
        evidence_template: "Micro-cycle pattern detected: '{intent}'"
      
      - rule_id: "EFFICIENCY_ROOT_CAUSE_BEFORE_SYMPTOMS"
        name: "Systematic Debugging (Anti-Pattern #3)"
        severity: "warning"
        description: "Prevent symptom-chasing when root cause analysis would be faster"
        
        detection:
          keywords:
            - "debug individual"
            - "investigate symptoms"
            - "try different approach"
            - "troubleshoot each"
          scope: ["intent", "description", "approach"]
        
        acceleration_pattern: "Check fundamental assumptions ‚Üí Fix root cause ‚Üí Validate fix"
        time_saved_per_instance: "15-45 minutes of symptom investigation"
        
        systematic_checks:
          - "UTC vs local time assumptions"
          - "Schema/column name mismatches"  
          - "Relative vs absolute imports"
          - "Test expectations vs implementation reality"
        
        alternatives:
          - "Check UTC timestamp handling first"
          - "Verify schema consistency first"
          - "Validate fundamental assumptions first"
        
        evidence_template: "Symptom debugging pattern: '{description}'"
      
      - rule_id: "EFFICIENCY_EXISTING_TOOLS_DURING_CRISIS"
        name: "Use Existing Tools During Active Work (Anti-Pattern #4)"
        severity: "warning"
        description: "Prevent creating debug tools during active troubleshooting"
        
        detection:
          keywords:
            - "create debug script"
            - "write tool to"
            - "build utility for"
            - "make script to analyze"
          context_indicators:
            - "failing tests"
            - "active debugging"
            - "urgent fix"
          scope: ["intent", "description", "approach"]
        
        acceleration_pattern: "Use existing methods ‚Üí Create tools for future use only"
        time_saved_per_instance: "5-10 minutes per debug tool creation"
        
        alternatives:
          - "Use existing debugging methods for now"
          - "Create optimization tools after crisis resolved"
          - "Focus on immediate problem resolution"
        
        evidence_template: "Tool creation during crisis: '{intent}'"
      
      - rule_id: "EFFICIENCY_ACTION_OVER_EXCESSIVE_SEARCH"
        name: "Direct Action Over Search-Driven Development (Anti-Pattern #5)"
        severity: "warning"
        description: "Prevent excessive searching when direct action would be faster"
        
        detection:
          keywords:
            - "search for existing"
            - "look for similar"
            - "find reference implementation"
            - "research how others"
          threshold_indicators:
            - "simple implementation request"
            - "straightforward task"
            - "basic functionality"
          scope: ["intent", "description", "approach"]
        
        acceleration_pattern: "Check if X exists ‚Üí Create X if not ‚Üí No extended search required"
        time_saved_per_instance: "5-10 minutes per search session"
        
        alternatives:
          - "Implement directly if task is clear"
          - "Search only for complex architectural decisions"
          - "Use existing patterns for simple tasks"
        
        evidence_template: "Excessive search pattern: '{intent}'"
  
  # Layer 7: Deployment Architecture Protection
  - layer_id: "deployment_architecture"
    name: "Deployment Architecture Protection"
    description: "Enforce upgrade/setup separation and brain preservation during deployments"
    priority: 7
    
    rules:
      - rule_id: "DEPLOYMENT_VERSION_TRACKING"
        name: "Version Tracking File Required"
        severity: "blocked"
        description: ".cortex-version file MUST exist in all CORTEX deployments for upgrade detection"
        
        detection:
          combined_keywords:
            deployment_operation:
              - "deploy cortex"
              - "install cortex"
              - "setup cortex"
              - "build package"
            missing_version:
              - "no .cortex-version"
              - "version file missing"
              - "cannot detect version"
          scope: ["operation", "validation"]
          logic: "AND"
        
        alternatives:
          - "Create .cortex-version file with version metadata"
          - "Include version in all publish packages"
          - "Validate version file presence in setup/upgrade workflows"
        
        evidence_template: |
          Deployment without version tracking detected!
          
          Operation: '{operation}'
          
          CRITICAL: .cortex-version file is MANDATORY for:
          - Upgrade detection (setup vs upgrade)
          - Schema migration tracking
          - Rollback capability
          - Version compatibility checks
          
          Required file structure:
          ```json
          {
            "cortex_version": "5.2.0",
            "schema_version": "1.0",
            "installed_date": "2025-11-23T14:30:00Z",
            "last_upgrade": "2025-11-23T14:30:00Z",
            "workspace_id": "a7b3c4d5e6f7"
          }
          ```
          
          File location: CORTEX/.cortex-version
        
        rationale: |
          Version tracking enables intelligent upgrade/setup detection:
          - NO version file ‚Üí Initial setup (create empty brain)
          - Version file exists ‚Üí Upgrade (preserve brain, update core)
          - Version comparison ‚Üí Determine upgrade path & migrations needed
      
      - rule_id: "UPGRADE_BRAIN_PRESERVATION"
        name: "Brain Data Preservation During Upgrades"
        severity: "blocked"
        description: "Upgrade operations MUST NEVER overwrite brain data (conversations, patterns, documents)"
        
        detection:
          combined_keywords:
            upgrade_operation:
              - "upgrade cortex"
              - "update cortex"
              - "replace cortex"
            and_overwrite_risk:
              - "overwrite all"
              - "replace cortex-brain"
              - "delete CORTEX"
              - "rm -rf CORTEX"
              - "Remove-Item CORTEX"
          scope: ["operation", "command"]
          logic: "AND"
        
        alternatives:
          - "Use selective file replacement (core only, preserve brain)"
          - "Backup brain data before any upgrade"
          - "Use upgrade package (not full package)"
          - "Verify brain preservation in validation gates"
        
        evidence_template: |
          Brain data overwrite risk detected!
          
          Operation: '{operation}'
          Command: '{command}'
          
          CRITICAL: NEVER overwrite these directories during upgrades:
          - cortex-brain/tier1/*.db (conversations)
          - cortex-brain/tier2/*.db (patterns)
          - cortex-brain/tier3/*.db (development context)
          - cortex-brain/documents/ (user documents)
          - cortex-brain/user-dictionary.yaml (learned terms)
          - cortex.config.json (workspace configuration)
          
          SAFE upgrade procedure:
          1. Backup brain data first
          2. Replace ONLY: src/, scripts/, .github/
          3. Merge configs: response-templates.yaml, capabilities.yaml
          4. Preserve brain: All .db files, documents/, user-dictionary.yaml
          5. Apply schema migrations if needed
          6. Validate brain integrity
        
        rationale: |
          Brain data represents workspace-specific learning that cannot be regenerated:
          - Conversation history (127+ conversations typical)
          - Learned patterns (43+ patterns typical)
          - User documents (89+ files typical)
          - Custom configurations
          
          Loss of brain data = loss of CORTEX intelligence for that workspace.
      
      - rule_id: "SCHEMA_MIGRATION_ENFORCEMENT"
        name: "Database Schema Migration Required"
        severity: "blocked"
        description: "Database schema changes MUST include migration files, never destructive updates"
        
        detection:
          combined_keywords:
            schema_change:
              - "alter table"
              - "drop table"
              - "change schema"
              - "modify database"
            without_migration:
              - "no migration"
              - "skip migration"
              - "direct schema change"
          scope: ["operation", "sql"]
          logic: "AND"
        
        alternatives:
          - "Create migration file in cortex-brain/migrations/"
          - "Use migration runner to apply changes"
          - "Write rollback SQL in migration"
          - "Test migration on backup database first"
        
        evidence_template: |
          Database schema change without migration detected!
          
          Operation: '{operation}'
          Schema change: '{schema_change}'
          
          CRITICAL: Schema changes MUST use migration files!
          
          Required migration structure:
          ```sql
          -- Migration: 003_add_deployment_tracking.sql
          -- Version: 5.3.0
          -- Date: 2025-11-23
          -- Description: Add deployment tracking to tier2
          
          BEGIN TRANSACTION;
          
          -- Forward migration
          CREATE TABLE IF NOT EXISTS tier2_deployments (
              deployment_id TEXT PRIMARY KEY,
              version TEXT NOT NULL,
              deployed_at TEXT NOT NULL
          );
          
          -- Update schema version
          PRAGMA user_version = 3;
          
          COMMIT;
          
          -- Rollback migration (commented)
          -- DROP TABLE IF EXISTS tier2_deployments;
          -- PRAGMA user_version = 2;
          ```
          
          Migration location: cortex-brain/migrations/XXX_description.sql
          
          Why migrations matter:
          - Preserves data during schema changes
          - Provides rollback capability
          - Documents schema evolution
          - Enables automated upgrades
        
        rationale: |
          Schema migrations are MANDATORY for:
          1. Data preservation: No data loss during schema changes
          2. Upgrade automation: Apply migrations during upgrade workflow
          3. Rollback safety: Revert schema if upgrade fails
          4. Version tracking: PRAGMA user_version marks schema state
          5. Team coordination: All deployments use same schema version
      
      - rule_id: "DEPLOYMENT_TYPE_DETECTION"
        name: "Intelligent Deployment Type Detection"
        severity: "warning"
        description: "Deployment operations should auto-detect setup vs upgrade mode"
        
        detection:
          keywords:
            - "manual deployment"
            - "copy folder"
            - "replace directory"
            - "user must choose"
          scope: ["operation", "workflow"]
        
        alternatives:
          - "Auto-detect: No CORTEX/ ‚Üí Setup mode"
          - "Auto-detect: CORTEX/ exists ‚Üí Upgrade mode"
          - "Check .cortex-version for version comparison"
          - "Provide dry-run mode for preview"
        
        evidence_template: |
          Manual deployment workflow detected: '{operation}'
          
          CORTEX should auto-detect deployment type:
          
          Detection Logic:
          ```python
          if not cortex_dir.exists():
              return DeploymentType.INITIAL_SETUP
          elif not version_file.exists():
              return DeploymentType.LEGACY_UPGRADE
          elif current_version < latest_version:
              return DeploymentType.UPGRADE
          else:
              return DeploymentType.UP_TO_DATE
          ```
          
          User Experience:
          - "setup cortex" ‚Üí Auto-detects mode
          - "upgrade cortex" ‚Üí Auto-detects mode
          - No manual decision needed
        
        rationale: |
          Intelligent detection improves user experience:
          - No "am I upgrading or setting up?" confusion
          - Automatic brain preservation in upgrade mode
          - Correct package download (full vs upgrade)
          - Reduced human error
      
      - rule_id: "CONFIG_MERGE_INTELLIGENCE"
        name: "Config Files Require 3-Way Merge"
        severity: "blocked"
        description: "Config files (YAML) must use 3-way merge during upgrades, not overwrite"
        
        detection:
          combined_keywords:
            upgrade_operation:
              - "upgrade"
              - "update config"
            and_overwrite:
              - "overwrite yaml"
              - "replace config"
              - "copy config"
          scope: ["operation"]
          logic: "AND"
        
        alternatives:
          - "Use 3-way merge: Base + Local + Upgrade ‚Üí Merged"
          - "Preserve user customizations (local changes)"
          - "Add new features from upgrade"
          - "Detect conflicts and offer resolution"
        
        evidence_template: |
          Config overwrite detected during upgrade!
          
          File: '{file_path}'
          Operation: '{operation}'
          
          CRITICAL: Config files need intelligent merging!
          
          3-Way Merge Process:
          ```
          Base Config (v5.1.0 - original)
              +
          Local Config (user customizations)
              +
          Upgrade Config (v5.2.0 - new features)
              =
          Merged Config (customizations + new features)
          ```
          
          Example - response-templates.yaml:
          Base: triggers: ["help"]
          Local: triggers: ["help", "cortex help"] ‚Üê User added
          Upgrade: triggers: ["help"], new_template: admin_help
          Merged: triggers: ["help", "cortex help"], new_template: admin_help
          
          Preserve:
          - User-added triggers
          - Custom templates
          - Modified capabilities
          
          Add:
          - New templates from upgrade
          - New capabilities
          - New operations
        
        rationale: |
          Config merging prevents loss of user customizations:
          - User spent time configuring CORTEX
          - Custom triggers, templates, operations
          - Should be preserved during upgrades
          - New features added alongside customizations
      
      - rule_id: "PUBLISH_PACKAGE_VALIDATION"
        name: "Publish Package Content Validation"
        severity: "blocked"
        description: "Published packages MUST NOT contain brain data or machine-specific files"
        
        detection:
          combined_keywords:
            publish_operation:
              - "build package"
              - "create publish"
              - "generate release"
            privacy_risk:
              - "*.db included"
              - "brain data included"
              - "machine paths included"
          scope: ["operation", "validation"]
          logic: "AND"
        
        alternatives:
          - "Exclude all .db files from publish"
          - "Exclude cortex.config.json (machine-specific)"
          - "Exclude .platform_state.json"
          - "Use publish-config.yaml validation rules"
        
        evidence_template: |
          Privacy/data leak risk in publish package!
          
          Package: '{package_path}'
          Risk: '{risk_type}'
          
          CRITICAL: Published packages MUST NOT contain:
          - *.db files (brain data)
          - cortex.config.json (machine-specific paths)
          - .platform_state.json (hostname)
          - .coverage.* (machine names)
          - conversation-history.jsonl (private conversations)
          
          Validation checklist:
          ‚úÖ No .db files
          ‚úÖ No machine-specific configs
          ‚úÖ No hostnames/usernames
          ‚úÖ No conversation data
          ‚úÖ Only core CORTEX code
          
          Use forbidden_patterns from publish-config.yaml
        
        rationale: |
          Privacy protection in published packages:
          - Brain data is workspace-specific (never shared)
          - Machine paths reveal private directory structure
          - Conversation history is confidential
          - Published packages = clean core only

  # Layer 8: Test Location Isolation
  - layer_id: "test_location_isolation"
    name: "Test Location Isolation"
    description: "Application tests in user repo, CORTEX tests in CORTEX folder - maintain brain knowledge from both"
    priority: 8
    
    rules:
      - rule_id: "TEST_LOCATION_SEPARATION"
        name: "Test Location Separation"
        severity: "blocked"
        description: "When running in development environments, application tests MUST be created in user repo (NOT CORTEX folder). Only CORTEX-related tests reside in CORTEX. CORTEX MUST use user's existing test framework and feed brain with knowledge learned from application tests."
        
        detection:
          combined_keywords:
            test_generation:
              - "generate test"
              - "create test"
              - "write test"
              - "tdd workflow"
            application_code:
              - "user application"
              - "user code"
              - "application feature"
              - "business logic"
            wrong_location:
              - "tests/fixtures"
              - "CORTEX/tests"
              - "cortex-brain/tests"
          scope: ["intent", "target_path"]
          logic: "AND"
        
        alternatives:
          - "STEP 1: Detect user repository root (outside CORTEX folder)"
          - "STEP 2: Discover user's existing test framework (pytest, jest, xunit, etc.)"
          - "STEP 3: Create tests in user repo following their conventions"
          - "STEP 4: Run tests using user's framework"
          - "STEP 5: Capture test patterns, framework usage, and insights"
          - "STEP 6: Store learned patterns in CORTEX brain (Tier 1/Tier 2)"
          - "EXAMPLE: User app at /Users/user/myapp ‚Üí Tests at /Users/user/myapp/tests/"
          - "EXAMPLE: CORTEX tests ‚Üí Stay in /CORTEX/tests/"
        
        evidence_template: |
          Test location violation detected!
          
          Target: '{target_path}'
          Test Type: '{test_type}'
          
          ‚ùå WRONG: Application test being created in CORTEX folder
          - Path: CORTEX/tests/test_user_feature.py
          - Reason: Application tests don't belong in CORTEX
          - Impact: Pollutes CORTEX with application-specific code
          
          ‚úÖ CORRECT: Application test in user repository
          - Path: /Users/user/myapp/tests/test_user_feature.py
          - Framework: {user's framework} (pytest/jest/xunit/etc.)
          - Conventions: {user's naming patterns}
          - Brain Learning: Test patterns stored in cortex-brain/tier2/
          
          **CORTEX Tests (stay in CORTEX folder):**
          - Brain protection tests
          - Agent functionality tests
          - Workflow orchestration tests
          - CORTEX infrastructure tests
          
          **Application Tests (go in user repo):**
          - Business logic tests
          - Feature tests
          - Integration tests
          - E2E tests
          
          **Knowledge Capture (automatic):**
          - Test framework detection ‚Üí Tier 2
          - Naming conventions ‚Üí Tier 2
          - Test patterns ‚Üí Tier 2
          - Code coverage insights ‚Üí Tier 1
          - Common failure patterns ‚Üí Tier 2
        
        rationale: |
          TEST_LOCATION_SEPARATION: Architectural Clarity & Brain Intelligence
          
          Problem Scenario:
          - User developing "payment processing" feature
          - CORTEX generates test_payment.py in CORTEX/tests/
          - Test contains application-specific logic
          - CORTEX repo polluted with user's business logic
          - User can't find tests in their own repo
          - Tests don't use user's existing framework
          
          Correct Approach:
          1. **Detect Context:**
             - Working directory: /Users/user/myapp
             - Test framework: pytest (detected from requirements.txt)
             - Convention: tests/ folder with test_*.py pattern
          
          2. **Generate Tests in User Repo:**
             - Path: /Users/user/myapp/tests/test_payment.py
             - Framework: pytest (user's choice)
             - Style: Matches user's existing tests
          
          3. **Capture Knowledge for Brain:**
             - Pattern: "Payment tests use mock stripe API"
             - Pattern: "User prefers parametrized fixtures"
             - Insight: "Payment tests fail when DB not seeded"
             - Store: cortex-brain/tier2/knowledge-graph.yaml
          
          4. **Benefits:**
             - User repo self-contained with its tests
             - CORTEX stays focused on CORTEX functionality
             - Brain learns from user patterns (not polluted with user code)
             - User's test framework honored
             - Proper separation of concerns
          
          Brain Learning Mechanism:
          - Monitor test execution results
          - Track framework usage patterns
          - Identify common anti-patterns
          - Store generalized insights (not specific code)
          - Use insights to improve future test generation
          
          Exception:
          Only CORTEX-related tests (brain, agents, workflows) stay in CORTEX folder.

# Logging configuration
logging:
  enabled: true
  path: "cortex-brain/corpus-callosum/protection-events.jsonl"
  format: "jsonl"
  include_fields:
    - "timestamp"
    - "event"
    - "request"
    - "violations"
    - "decision"
    - "severity"
    - "alternatives_suggested"
    - "user_decision"
    - "override_justification"
    - "override_required"

# Validation rules
validation:
  - check: "rules_complete"
    scope: "protection_layers"
    required: true
    params:
      all_layers_have_rules: true
      all_rules_have_detection: true
      all_rules_have_alternatives: true
  
  - check: "severity_valid"
    scope: "rules"
    required: true
    params:
      valid_severities: ["safe", "warning", "blocked"]
  
  - check: "templates_valid"
    scope: "challenge_template"
    required: true
    params:
      has_placeholders: true
      has_options: true
