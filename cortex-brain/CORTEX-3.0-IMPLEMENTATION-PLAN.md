# CORTEX 3.0 Implementation Plan

**Date:** 2025-11-14  
**Status:** üöÄ APPROVED - Ready to Begin  
**Timeline:** 30 weeks (~7.5 months) with parallel execution  
**Decision Score:** 4.20/5.00 (84%) - STRONG GO

**Author:** Asif Hussain  
**Copyright:** ¬© 2024-2025 Asif Hussain. All rights reserved.  
**License:** Proprietary - See LICENSE file for terms  
**Repository:** https://github.com/asifhussain60/CORTEX

---

## üéØ Executive Summary

CORTEX 3.0 represents a major architectural evolution with 6 core improvements:

1. **Dual-Channel Memory** - Fuse conversations + executions = complete development narratives
2. **Simplified Operations** - Ship working MVPs fast using monolithic-then-modular pattern
3. **Intelligent Context** - ML-powered code analysis with proactive warnings
4. **Template Integration** - Zero-execution help responses
5. **Interactive Tutorial System** - Holistic + modular feature education
6. **Enhanced Agents** - Multi-agent workflows with specialized sub-agents

**Critical Success Factor:** Phase 0 (test stabilization) is BLOCKING - achieve 100% test pass rate before any new feature work.

---

## üìã Implementation Phases

### Phase 0: Test Stabilization (BLOCKING - 2 weeks)

**Objective:** Achieve 100% non-skipped test pass rate before any CORTEX 3.0 work

**Current Status:**
```
‚úÖ Passing:  834 tests (100% of non-skipped)
‚è≠Ô∏è Skipped:   63 tests (pragmatic - future work)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   Total:    897 tests
```

**Tasks:**

#### Week 1: Address Skipped Tests
```yaml
task_1_categorize_skipped:
  description: "Review all 63 skipped tests, categorize as BLOCKING/WARNING/PRAGMATIC"
  acceptance:
    - "Each skipped test has clear categorization"
    - "BLOCKING tests identified (must fix)"
    - "WARNING tests documented (defer with reason)"
    - "PRAGMATIC tests adjusted (MVP thresholds)"
  
task_2_fix_blocking:
  description: "Fix BLOCKING skipped tests (estimate 15-20 tests)"
  approach: "Apply Phase 0 optimization principles (pragmatic MVP)"
  categories:
    - "SKULL protection tests (critical)"
    - "Integration wiring tests (core functionality)"
    - "Security/privacy tests (redaction, encryption)"
  
target: "850+/897 passing (95%+)"
```

#### Week 2: Fix Remaining + Validation
```yaml
task_1_fix_warnings:
  description: "Fix WARNING skipped tests OR document deferral reasons"
  approach: "Balance MVP delivery with quality gates"
  
task_2_final_validation:
  description: "Full test suite validation + CI/CD pipeline green"
  acceptance:
    - "100% non-skipped test pass rate"
    - "All skips documented in test-strategy.yaml"
    - "CI/CD pipeline green"
    - "SKULL-007 compliance achieved"

target: "897/897 passing OR skips justified with documentation"
```

**Success Criteria:**
- ‚úÖ 100% non-skipped test pass rate (897/897 or justified skips)
- ‚úÖ SKULL-007 compliance (no status inflation)
- ‚úÖ Green CI/CD pipeline
- ‚úÖ All skips documented with deferral reasons

**Blockers:** None - Phase 0 optimization principles proven effective

---

### Phase 1: Foundation (6 weeks)

**Objective:** Ship all operations, integrate templates, add tutorial system

#### Milestone 1.1: Simplified Operations System (3 weeks)

**Approach:** Monolithic-then-modular (validated in Phase 0)

**Principles:**
1. Ship working end-to-end operation first (single script)
2. Refactor into modules only when complexity warrants (>500 lines)
3. Deliver user value early, optimize later
4. Apply optimization-principles.yaml patterns

**Operations:**

**Week 1:**
```yaml
environment_setup:
  current: "36% (4/11 modules)"
  new_approach: "Single setup.py script (~350 lines)"
  timeline: "3 days"
  deliverable: "Working cross-platform environment setup"
  
workspace_cleanup:
  current: "0% (0/6 modules)"
  new_approach: "Single cleanup.py script (~250 lines)"
  timeline: "2 days"
  deliverable: "Safe cleanup with temp file detection"
```

**Week 2:**
```yaml
update_documentation:
  current: "0% (0/6 modules)"
  new_approach: "Single doc_generator.py script (~300 lines)"
  timeline: "3 days"
  deliverable: "Auto-generate docs from code/YAML"
  
brain_protection_check:
  current: "0% (0/6 modules)"
  new_approach: "Single brain_check.py script (~200 lines)"
  timeline: "2 days"
  deliverable: "SKULL rule validation + health report"
```

**Week 3:**
```yaml
run_tests:
  current: "0% (0/5 modules)"
  new_approach: "Single test_runner.py script (~150 lines)"
  timeline: "1 day"
  deliverable: "Unified test execution with coverage"
  
comprehensive_self_review:
  current: "0% (0/20 modules)"
  new_approach: "Single self_review.py script (~400 lines)"
  timeline: "4 days"
  deliverable: "Full health check across all tiers"
  
refresh_cortex_story:
  current: "100% (6/6 modules)"
  action: "Validate end-to-end functionality"
  timeline: "1 day"
```

**Success Criteria:**
- ‚úÖ All 7 operations working end-to-end
- ‚úÖ User can invoke via natural language
- ‚úÖ Comprehensive tests for each operation
- ‚úÖ Documentation complete

#### Milestone 1.2: Template Integration (1 week)

**Objective:** Wire response-templates.yaml to entry point and all agents

**Tasks:**

**Days 1-2: Entry Point Integration**
```yaml
task_1:
  description: "Wire template_loader to CORTEX.prompt.md"
  implementation:
    - "Load response-templates.yaml on startup"
    - "Implement help/status/quick_start rendering"
    - "Test: 'help' command <10ms response"
  
task_2:
  description: "Zero-execution responses"
  tests:
    - "'help' ‚Üí Instant table (no Python execution)"
    - "'status' ‚Üí Template + Tier 3 metrics injection"
    - "'quick start' ‚Üí Instant guide"
```

**Days 3-4: Agent Integration**
```yaml
task_1:
  description: "Integrate template_registry with all 10 agents"
  agents:
    - "Executor, Tester, Validator, Work Planner, Documenter"
    - "Intent Detector, Architect, Health Validator, Pattern Matcher, Learner"
  
task_2:
  description: "Add success/error/progress templates"
  templates:
    - "agent_success_{{agent_name}}"
    - "agent_error_{{agent_name}}"
    - "agent_progress_{{agent_name}}"
  
test: "Consistent formatting across all agent responses"
```

**Day 5: Operation Integration**
```yaml
task_1:
  description: "Add operation templates"
  templates:
    - "operation_header_{{operation_name}}"
    - "operation_progress_{{operation_name}}_phase_{{phase}}"
    - "operation_completion_{{operation_name}}"
  
task_2:
  description: "End-to-end testing"
  validation:
    - "All operations use templates for status reporting"
    - "Verbosity control works (concise/detailed/expert)"
    - "User experience consistent across CORTEX"
```

**Success Criteria:**
- ‚úÖ 'help' command zero-execution (<10ms)
- ‚úÖ All agents use templates for responses
- ‚úÖ Consistent UX across CORTEX
- ‚úÖ Comprehensive tests (template rendering, edge cases)

#### Milestone 1.3: Interactive Tutorial System (2 weeks)

**Objective:** Three-tier tutorial architecture with user choice

**Architecture:**
```yaml
tier_1_first_contact:
  duration: "30 seconds"
  trigger: "First workspace OR 'introduce yourself'"
  content:
    - "CORTEX value proposition (memory, learning, intelligence)"
    - "3 quick demos (status, plan, continue)"
    - "Invitation to full tour OR jump right in"
  storage: "cortex-brain/tutorials/first-contact.yaml"
  
tier_2_guided_tour:
  duration: "5 minutes"
  trigger: "'give me the tour' OR 'show me what CORTEX can do'"
  structure:
    - "Part 1: Memory & Learning (90s)"
    - "Part 2: Feature Planning (90s)"
    - "Part 3: Operations (90s)"
    - "Part 4: Advanced Features (90s)"
  features:
    - "Interactive (Yes/No/Skip navigation)"
    - "Progress tracking (‚óè‚óè‚óã‚óã‚óã)"
    - "Skip/Resume capability"
  storage: "cortex-brain/tutorials/guided-tour.yaml"
  
tier_3_feature_tutorials:
  duration: "2-3 minutes each"
  triggers:
    - "'explain token optimization'"
    - "'how does dual-channel memory work?'"
    - "'teach me about agents'"
  storage: "cortex-brain/tutorials/features/"
  files:
    - "conversation-import.yaml"
    - "dual-channel-memory.yaml"
    - "agent-system.yaml"
    - "token-optimization.yaml"
    - "brain-architecture.yaml"
    - "operations-overview.yaml"
```

**Week 1 Tasks:**

**Days 1-2: Tutorial Agent + State Tracking**
```yaml
task_1:
  description: "Create tutorial_agent.py (~250 lines)"
  location: "src/cortex_agents/tutorial_agent.py"
  role: "Right brain (strategic) - guides learning"
  capabilities:
    - "Detect first-time users (auto-trigger first-contact)"
    - "Route tutorial requests via Intent Detector"
    - "Track completion state in Tier 1"
    - "Validate user actions during tutorials"
    - "Recommend relevant tutorials based on context"
  
task_2:
  description: "Add tutorial_state_tracker to Tier 1 schema"
  schema_additions:
    - "first_run_completed BOOLEAN DEFAULT 0"
    - "last_tutorial_accessed TEXT"
    - "completed_tutorials TEXT (JSON array)"
    - "preferred_verbosity TEXT DEFAULT 'detailed'"
  
task_3:
  description: "Natural language trigger detection"
  examples:
    holistic: ["introduce yourself", "give me the tour", "what can you do?"]
    feature_specific: ["explain X", "how does X work?", "teach me about X"]
```

**Days 3-4: Tutorial Content Creation**
```yaml
task_1:
  description: "Create cortex-brain/tutorials/ directory structure"
  structure: |
    cortex-brain/tutorials/
    ‚îú‚îÄ‚îÄ first-contact.yaml          # 30 sec intro
    ‚îú‚îÄ‚îÄ guided-tour.yaml            # 5 min full tour
    ‚îî‚îÄ‚îÄ features/                   # Feature-specific deep dives
  
task_2:
  description: "Write first-contact.yaml"
  content: "30 second introduction with value proposition"
  
task_3:
  description: "Write guided-tour.yaml"
  content: "5 minute interactive tour (4 parts, 90s each)"
  
task_4:
  description: "Integrate with response_templates system"
  validation: "Consistent formatting across all tutorials"
```

**Day 5: Testing & Validation**
```yaml
task_1:
  description: "Test first-run experience"
  scenarios:
    - "First-contact auto-triggers"
    - "User chooses tour vs explore"
    - "State persists in Tier 1"
  
task_2:
  description: "Test holistic tour"
  scenarios:
    - "Manual trigger: 'give me the tour'"
    - "Interactive navigation (Yes/No/Skip/Back)"
    - "Progress tracking (‚óè‚óè‚óã‚óã‚óã)"
  
task_3:
  description: "End-to-end testing with real workspace"
```

**Week 2 Tasks:**

**Days 1-2: Feature Tutorials**
```yaml
task_1:
  description: "Write 6 feature tutorials"
  files:
    - "conversation-import.yaml"
    - "dual-channel-memory.yaml"
    - "agent-system.yaml"
    - "token-optimization.yaml"
    - "brain-architecture.yaml"
    - "operations-overview.yaml"
  
task_2:
  description: "Implement feature-specific trigger routing"
  examples:
    - "'explain token optimization' ‚Üí tutorials/features/token-optimization.yaml"
    - "'how does dual-channel memory work?' ‚Üí tutorials/features/dual-channel-memory.yaml"
```

**Days 3-4: Advanced Features**
```yaml
task_1:
  description: "Contextual recommendations"
  examples:
    - "Executor suggests 'token optimization' tutorial when user asks about performance"
    - "Health Validator recommends 'brain architecture' tutorial when explaining diagnostics"
  
task_2:
  description: "Skip/Resume/Back functionality"
  implementation: "State machine for tutorial navigation"
  
task_3:
  description: "Progress tracking visualization (‚óè‚óè‚óã‚óã‚óã)"
  
task_4:
  description: "Test all feature tutorials end-to-end"
```

**Day 5: User Testing & Refinement**
```yaml
task_1:
  description: "User testing with first-time users"
  testers: "5 beta testers (mixed experience levels)"
  
task_2:
  description: "Refinement based on feedback"
  focus: "Clarity, pacing, navigation ease"
  
task_3:
  description: "Documentation update"
  deliverable: "How to create new tutorials (guide for future)"
  
task_4:
  description: "Integration testing"
  validation: "Tutorial ‚Üí actual feature execution works"
```

**Success Criteria:**
- ‚úÖ First-time users complete first-contact in <1 minute
- ‚úÖ Guided tour completion rate >80%
- ‚úÖ Feature tutorials completion rate >70%
- ‚úÖ User satisfaction >4.5/5.0 (post-tutorial survey)
- ‚úÖ Tutorial state persistence 100% accurate
- ‚úÖ All natural language triggers working
- ‚úÖ Contextual recommendations appropriate

---

### Phase 2: Dual-Channel Memory MVP (14 weeks)

**Objective:** Fuse conversations + executions = complete development narratives

**Target:** 85% "continue" command success rate (vs 60% baseline)

#### Milestone 2.1: Conversational Import (2 weeks)

**Status:** Prototype exists (`conversation_import_plugin.py`)

**Week 1:**
```yaml
task_1:
  description: "Integrate conversation_import_plugin with Tier 1"
  implementation:
    - "Add conversation type to SQLite schema"
    - "Add metadata fields (quality, semantic_elements, files_mentioned)"
    - "Wire plugin to natural language commands"
  
task_2:
  description: "Quality scoring implementation"
  levels: ["EXCELLENT (8-10)", "GOOD (6-7)", "FAIR (4-5)", "LOW (0-3)"]
  criteria:
    - "Challenge/Accept reasoning"
    - "Multi-phase planning"
    - "Design trade-offs"
    - "Actionable next steps"
  
task_3:
  description: "Storage directory creation"
  location: "cortex-brain/imported-conversations/"
  organization: "YYYY-MM-DD-description.md"
```

**Week 2:**
```yaml
task_1:
  description: "User documentation (manual import workflow)"
  content:
    - "How to export CopilotChats.md from VS Code"
    - "How to import to CORTEX"
    - "How to verify import success"
  
task_2:
  description: "Tutorial system (first-time user guide)"
  deliverable: "Interactive tutorial for conversation import"
  
task_3:
  description: "End-to-end testing with real exported conversations"
  validation: "Import 10 conversations, verify metadata accuracy"
```

**Success Criteria:**
- ‚úÖ Users can import conversations manually (CopilotChats.md ‚Üí CORTEX)
- ‚úÖ Quality scoring working (manual validation vs CORTEX scores)
- ‚úÖ Documentation complete and user-friendly
- ‚úÖ Tutorial guides first-time users successfully

#### Milestone 2.2: Fusion Layer - Basics (3 weeks)

**Objective:** Cross-reference conversations with daemon events

**Week 1: Temporal Correlation**
```yaml
task_1:
  description: "Implement temporal correlation algorithm"
  window: "¬±1 hour (configurable)"
  implementation: |
    def correlate_events(conversation_turn, daemon_events, time_window=3600):
        # Match conversation turn with daemon events within time window
  
task_2:
  description: "Add correlation table to Tier 1 SQLite"
  schema:
    - "conversation_id (FK)"
    - "event_id (FK)"
    - "time_diff_seconds (INTEGER)"
    - "match_type (TEXT): temporal|file_mention|plan_verification"
    - "confidence (INTEGER 0-100)"
  
task_3:
  description: "Test: Match conversation timestamps with file changes"
  validation: "90%+ accuracy on test dataset"
```

**Week 2: File Mention Matching**
```yaml
task_1:
  description: "Extract file paths from conversation text"
  pattern: "Backtick code blocks: `cleanup_temp_files.py`"
  implementation: "Regex + AST parsing for accurate extraction"
  
task_2:
  description: "Match with daemon file change events"
  algorithm: |
    for mentioned_file in conversation_turn.files_mentioned:
        if mentioned_file in daemon_event.file_path:
            return MATCH
  
task_3:
  description: "Test: File mention matching accuracy"
  validation: "90%+ accuracy on test dataset"
```

**Week 3: Basic Visualization + Integration Tests**
```yaml
task_1:
  description: "Text-based timeline visualization"
  format: |
    10:00 AM | Conversation | "Let's plan cleanup system"
    10:15 AM | Daemon       | Created cleanup-detection-patterns.yaml
    10:45 AM | Daemon       | Created analyze_temp_patterns.py
    11:30 AM | Daemon       | Created cleanup_temp_files.py
  
task_2:
  description: "Correlation confidence scores (0-100)"
  factors:
    - "Time proximity (closer = higher confidence)"
    - "File mention match (exact path = 100, partial = 50-80)"
    - "Event type relevance (file creation > file read)"
  
task_3:
  description: "Integration tests: End-to-end correlation flow"
  scenarios:
    - "Import conversation ‚Üí correlate with daemon ‚Üí verify timeline"
    - "Test edge cases (no events, multiple matches, ambiguous matches)"
```

**Success Criteria:**
- ‚úÖ Conversations matched with daemon events within time window
- ‚úÖ File mentions matched with file changes (90%+ accuracy)
- ‚úÖ Correlation confidence scores calculated
- ‚úÖ Timeline shows conversation WHY + daemon WHAT together

#### Milestone 2.3: User Enablement (2 weeks)

**Objective:** Make dual-channel memory easy to use

**Week 1: Tutorials + Examples**
```yaml
task_1:
  description: "Interactive tutorial (step-by-step conversation import)"
  content:
    - "Part 1: Export from VS Code"
    - "Part 2: Import to CORTEX"
    - "Part 3: View correlations"
    - "Part 4: Understand narratives"
  
task_2:
  description: "Example conversations for testing"
  deliverables:
    - "5 pre-built example conversations"
    - "Cover different scenarios (planning, debugging, refactoring)"
    - "Show good vs poor conversation quality"
  
task_3:
  description: "Best practices guide"
  content:
    - "When to import (valuable conversations only)"
    - "What makes good conversations (challenges, phases, decisions)"
    - "How to improve conversation quality (CORTEX template usage)"
```

**Week 2: Troubleshooting + FAQ**
```yaml
task_1:
  description: "Troubleshooting documentation"
  common_issues:
    - "Import fails (file format, permissions)"
    - "Low correlation scores (timestamp issues)"
    - "Missing file mentions (parsing errors)"
  
task_2:
  description: "FAQ (common questions answered)"
  questions:
    - "How often should I import conversations?"
    - "What happens to sensitive data?"
    - "Can I delete imported conversations?"
    - "How does this improve 'continue' command?"
  
task_3:
  description: "Video walkthrough (optional screen recording)"
  content: "5 minute demo of full workflow"
```

**Success Criteria:**
- ‚úÖ First-time users successful within 5 minutes
- ‚úÖ Tutorial completion rate >80%
- ‚úÖ User satisfaction >4.0/5.0
- ‚úÖ FAQ addresses 90%+ of common questions

#### Milestone 2.4: Fusion Advanced (3 weeks)

**Objective:** Plan verification + pattern learning fusion

**Week 1: Plan Verification**
```yaml
task_1:
  description: "Parse multi-phase plans from conversations"
  patterns:
    - "Phase 1:", "Phase 2:", "Phase 3:" (numbered)
    - "Milestone A:", "Milestone B:" (lettered)
    - "Track A:", "Track B:" (parallel tracks)
  
task_2:
  description: "Match phases with daemon execution events"
  algorithm: |
    for phase in conversation.parsed_phases:
        matched_events = find_events_after_phase_mention(phase)
        if matched_events:
            phase.status = "STARTED"
            if phase_complete(phase, matched_events):
                phase.status = "COMPLETED"
  
task_3:
  description: "Calculate phase completion percentages"
  output: "Phase 1: 100% | Phase 2: 80% | Phase 3: 0%"
```

**Week 2: Incomplete Phase Detection + Execution Proof**
```yaml
task_1:
  description: "Flag incomplete phases (discussed but not executed)"
  output: "‚ö†Ô∏è Phase 3 discussed at 2pm but no file changes detected"
  
task_2:
  description: "Generate execution proof reports"
  format: |
    ## Phase 2: Core Implementation
    ‚úÖ Discussed: 10:15 AM
    ‚úÖ Started: 10:45 AM (created cleanup_temp_files.py)
    ‚úÖ Completed: 11:30 AM (tests passed)
    Proof: 3 files created, 2 tests passed, 1 terminal command executed
```

**Week 3: Pattern Learning Fusion**
```yaml
task_1:
  description: "Correlate conversation patterns with daemon patterns"
  examples:
    - "Challenge ‚Üí Better design ‚Üí More REFACTOR events"
    - "Multi-phase plans ‚Üí Incremental file creation"
    - "Design trade-offs ‚Üí Higher test coverage"
  
task_2:
  description: "Learn from correlations"
  output: "Pattern: Conversations with Challenge/Accept ‚Üí 30% more REFACTORs"
  
task_3:
  description: "Store learned patterns in Tier 2 Knowledge Graph"
  integration: "Feed patterns back to Pattern Matcher agent"
```

**Success Criteria:**
- ‚úÖ Multi-phase plans tracked automatically
- ‚úÖ Completion percentages accurate
- ‚úÖ Incomplete phases flagged with context
- ‚úÖ Patterns learned from conversation + daemon correlations
- ‚úÖ Tier 2 Knowledge Graph enriched with learned patterns

#### Milestone 2.5: Narrative Generation (2 weeks)

**Objective:** Generate complete development narratives

**Week 1: Narrative Engine**
```yaml
task_1:
  description: "Narrative generation engine"
  algorithm: |
    def generate_narrative(conversation, correlated_events):
        intent = extract_user_intent(conversation)
        discussion = extract_key_decisions(conversation)
        implementation = summarize_daemon_events(correlated_events)
        verification = extract_test_results(correlated_events)
        return Story(intent, discussion, implementation, verification)
  
task_2:
  description: "Story template system"
  sections:
    - "Intent (User request)"
    - "Discussion (Conversation WHY)"
    - "Implementation (Daemon WHAT)"
    - "Verification (Test results)"
  
task_3:
  description: "Timeline visualization with narrative context"
  format: |
    ‚îÅ‚îÅ‚îÅ Development Narrative ‚îÅ‚îÅ‚îÅ
    Intent: Create cleanup system
    ‚îÉ
    ‚îú‚îÄ 10:00 AM ‚îÇ Conversation ‚îÇ Challenge marker-based approach
    ‚îú‚îÄ 10:15 AM ‚îÇ Daemon       ‚îÇ Created cleanup-detection-patterns.yaml
    ‚îú‚îÄ 10:45 AM ‚îÇ Daemon       ‚îÇ Created analyze_temp_patterns.py
    ‚îú‚îÄ 11:30 AM ‚îÇ Daemon       ‚îÇ Created cleanup_temp_files.py
    ‚îú‚îÄ 11:45 AM ‚îÇ Daemon       ‚îÇ Terminal: python scripts/cleanup_temp_files.py
    ‚îî‚îÄ Result: ‚úÖ 3/3 phases complete, system operational
```

**Week 2: Integration + 'Continue' Enhancement**
```yaml
task_1:
  description: "Tier 2 Knowledge Graph integration"
  action: "Store narratives for future learning"
  schema_additions:
    - "narrative_id"
    - "intent_text"
    - "key_decisions (JSON)"
    - "phases_completed (JSON)"
    - "verification_results (JSON)"
  
task_2:
  description: "'Continue' command enhancement"
  old_behavior: "I see you created cleanup_temp_files.py. What next?"
  new_behavior: "Phase 2 complete (interactive cleanup). Ready for Phase 3 (automatic tagging)?"
  
task_3:
  description: "Validation: 'Continue' success rate"
  baseline: "60% (CORTEX 2.0)"
  target: "85% (CORTEX 3.0)"
  measurement: "User satisfaction survey + A/B testing"
```

**Success Criteria:**
- ‚úÖ Complete narratives auto-generated for all imported conversations
- ‚úÖ Stories include all 4 sections (Intent, Discussion, Implementation, Verification)
- ‚úÖ "Continue" command success rate 60% ‚Üí 85%
- ‚úÖ Knowledge Graph learns patterns from narratives
- ‚úÖ Timeline visualization enhanced with narrative context

#### Milestone 2.6: Optimization (2 weeks)

**Objective:** Performance + UX polish

**Week 1: Performance Optimization**
```yaml
task_1:
  description: "Fusion algorithm performance"
  target: "<1s for 100+ events"
  approach:
    - "Batch processing (not event-by-event)"
    - "Index optimization (SQLite queries)"
    - "Caching (reuse correlation results)"
  
task_2:
  description: "Background processing"
  implementation: "Non-blocking imports (don't freeze UI)"
  
task_3:
  description: "Memory optimization"
  target: "Handle large workspaces (1000+ files, 500+ conversations)"
  approach: "Streaming processing (don't load everything in memory)"
```

**Week 2: UI/UX Improvements + Beta Feedback**
```yaml
task_1:
  description: "UI/UX improvements"
  focus:
    - "Clearer feedback (import progress, correlation status)"
    - "Progress bars (visual feedback during long operations)"
    - "Better error messages (actionable guidance)"
  
task_2:
  description: "Advanced visualizations"
  deliverables:
    - "Correlation graphs (show connections between conversations + events)"
    - "Enhanced timeline (interactive, zoomable)"
    - "Dashboard (statistics, quality scores, completion rates)"
  
task_3:
  description: "User feedback integration"
  source: "Beta tester comments from Milestone 2.3"
  action: "Address top 5 user pain points"
```

**Success Criteria:**
- ‚úÖ Fast performance on large workspaces (<1s fusion time)
- ‚úÖ Responsive, intuitive UI (non-blocking operations)
- ‚úÖ 90%+ user satisfaction (post-beta survey)
- ‚úÖ Advanced visualizations enhance understanding

---

### Phase 3: Intelligent Context Layer (6 weeks, parallel with Phase 2)

**Objective:** ML-powered code analysis with proactive warnings

**Can run in parallel with Phase 2 (Weeks 9-14)**

#### Milestone 3.1: Complexity Analyzer (2 weeks)

**Week 1: Metrics Implementation**
```yaml
task_1:
  description: "Integrate radon (cyclomatic complexity)"
  installation: "pip install radon"
  usage: "radon cc <file> -a"
  
task_2:
  description: "Integrate mccabe (cognitive complexity)"
  installation: "pip install mccabe"
  usage: "python -m mccabe --min 10 <file>"
  
task_3:
  description: "Calculate Halstead metrics"
  metrics:
    - "Program length (N)"
    - "Vocabulary size (n)"
    - "Volume (V)"
    - "Difficulty (D)"
    - "Effort (E)"
  
task_4:
  description: "Maintainability index calculation"
  formula: "MI = 171 - 5.2 √ó ln(V) - 0.23 √ó G - 16.2 √ó ln(LOC)"
```

**Week 2: Warning System + Trend Tracking**
```yaml
task_1:
  description: "Warning thresholds"
  rules:
    - "Cyclomatic complexity >15 ‚Üí ‚ö†Ô∏è High complexity, refactor recommended"
    - "Cognitive complexity >15 ‚Üí ‚ö†Ô∏è Hard to understand, simplify"
    - "Maintainability index <65 ‚Üí ‚ö†Ô∏è Low maintainability, needs improvement"
  
task_2:
  description: "Suggest refactoring candidates"
  output: |
    üîç Refactoring Candidates:
    1. cleanup_temp_files.py::scan_directory() (CC=18, MI=58)
       Recommendation: Extract file filtering logic to separate function
    2. brain_protector.py::validate_rules() (CC=16, MI=62)
       Recommendation: Use strategy pattern for rule validation
  
task_3:
  description: "Store complexity trends in Tier 3"
  tracking: "Monitor complexity over time (detect regressions)"
  
task_4:
  description: "Test: Analyze CORTEX codebase, identify hotspots"
```

**Success Criteria:**
- ‚úÖ Complexity warnings operational (accurate thresholds)
- ‚úÖ Refactoring suggestions actionable
- ‚úÖ Trends tracked over time (detect regressions)
- ‚úÖ Hotspots identified in CORTEX codebase

#### Milestone 3.2: Debt Detector (2 weeks)

**Week 1: Comment Scanning + Duplication Detection**
```yaml
task_1:
  description: "TODO/FIXME/HACK comment scanner"
  patterns:
    - "# TODO: <message>"
    - "# FIXME: <message>"
    - "# HACK: <message>"
    - "# XXX: <message>"
  output: "List of technical debt markers with location + message"
  
task_2:
  description: "Code duplication detection"
  tool: "pylint or radon"
  threshold: ">30 lines duplicated"
  output: "List of duplicated code blocks with similarity score"
  
task_3:
  description: "Dead code identification"
  tool: "vulture"
  usage: "vulture <directory> --min-confidence 80"
  output: "List of unused functions/classes/variables"
```

**Week 2: Dependency Scanning + Debt Scoring**
```yaml
task_1:
  description: "Outdated dependency detection"
  tools:
    - "pip-audit (security vulnerabilities)"
    - "pip list --outdated (version checks)"
  output: "List of outdated/vulnerable dependencies with CVE IDs"
  
task_2:
  description: "Debt score calculation (0-100)"
  formula: |
    debt_score = 100 - (
      (todo_count √ó 5) +
      (fixme_count √ó 10) +
      (hack_count √ó 15) +
      (duplication_lines / 10) +
      (dead_code_count √ó 3) +
      (vulnerable_deps √ó 20)
    )
  
task_3:
  description: "Prioritized fix list generation"
  sorting: "By impact (vulnerable deps > HACKs > TODOs)"
  
task_4:
  description: "Test: Scan CORTEX, generate debt report"
```

**Success Criteria:**
- ‚úÖ Debt score accurate (validated against manual assessment)
- ‚úÖ Prioritized fix list actionable
- ‚úÖ Security vulnerabilities flagged (CVE IDs shown)
- ‚úÖ Debt report generated for CORTEX codebase

#### Milestone 3.3: Change Impact Predictor (2 weeks)

**Week 1: Relationship Graph + Test Coverage Correlation**
```yaml
task_1:
  description: "File relationship graph from Tier 2 co-change patterns"
  data_source: "Git history (files changed together in commits)"
  algorithm: |
    for commit in git_log:
        files_in_commit = commit.files
        for file_a, file_b in combinations(files_in_commit, 2):
            relationship_graph.add_edge(file_a, file_b, weight+=1)
  
task_2:
  description: "Test coverage correlation analysis"
  algorithm: |
    if file_a changed and file_b frequently changed together:
        if file_b has tests and file_a doesn't:
            warning: "Consider adding tests to file_a"
  
task_3:
  description: "Historical change pattern extraction"
  examples:
    - "Change to plugins/base_plugin.py ‚Üí 80% chance plugins/* also change"
    - "Change to cortex_agents/executor.py ‚Üí 60% chance tests/agents/test_executor.py needs update"
```

**Week 2: ML Model Training + Integration**
```yaml
task_1:
  description: "ML model training (predict affected files)"
  approach:
    - "Features: file relationships, co-change frequency, test coverage"
    - "Model: Random Forest or Gradient Boosting"
    - "Training data: Tier 2 historical patterns (1000+ commits)"
  
task_2:
  description: "Confidence scoring (0-100%)"
  interpretation:
    - "90-100%: Very likely (show warning)"
    - "70-89%: Likely (show suggestion)"
    - "<70%: Possible (don't show)"
  
task_3:
  description: "Integration with Executor agent"
  behavior: "Before file change, show impact prediction"
  example: |
    ‚ö†Ô∏è Impact Prediction:
    Changing src/plugins/base_plugin.py may affect:
    ‚Ä¢ src/plugins/conversation_import_plugin.py (85% confidence)
    ‚Ä¢ src/plugins/platform_switch_plugin.py (78% confidence)
    ‚Ä¢ tests/plugins/test_base_plugin.py (92% confidence)
  
task_4:
  description: "Test: Predict impact, validate accuracy"
  target: "80%+ prediction accuracy"
```

**Success Criteria:**
- ‚úÖ Impact predictions 80%+ accurate
- ‚úÖ Proactive warnings before breaking changes
- ‚úÖ Integration with agent workflows (Executor agent)
- ‚úÖ Confidence scores guide decision-making

---

### Phase 4: Enhanced Agent System (4 weeks)

**Objective:** Multi-agent workflows + specialized sub-agents

#### Milestone 4.1: Multi-Agent Workflows (2 weeks)

**Week 1: Workflow Orchestration**
```yaml
task_1:
  description: "Corpus Callosum workflow orchestration"
  capability: "Coordinate handoffs between agents"
  
workflow_1_feature_implementation:
  sequence:
    - "Work Planner: Break down into phases"
    - "Architect: Review design, suggest patterns"
    - "Executor: Implement Phase 1"
    - "Tester: Generate comprehensive tests"
    - "Validator: Review code + tests, ensure quality"
    - "Documenter: Auto-generate docs"
  
workflow_2_code_refactoring:
  sequence:
    - "Health Validator: Identify technical debt"
    - "Pattern Matcher: Find similar successful refactors"
    - "Architect: Propose refactoring approach"
    - "Executor: Implement refactor"
    - "Tester: Update tests"
    - "Validator: Verify improvement"
  
workflow_3_debugging:
  sequence:
    - "Health Validator: Analyze error patterns"
    - "Pattern Matcher: Find similar past bugs"
    - "Executor: Implement fix"
    - "Tester: Add regression test"
    - "Validator: Verify fix resolves issue"
```

**Week 2: Context Passing + State Persistence**
```yaml
task_1:
  description: "Agent context passing"
  mechanism: "Output of agent N = input to agent N+1"
  implementation: |
    workflow_state = {}
    for agent in workflow.sequence:
        result = agent.execute(workflow_state)
        workflow_state.update(result.context)
  
task_2:
  description: "Workflow state persistence"
  purpose: "Resume interrupted workflows"
  storage: "Tier 1 SQLite (workflow_state table)"
  
task_3:
  description: "Test: End-to-end workflow execution"
  validation: "All 3 workflows complete without manual intervention"
```

**Success Criteria:**
- ‚úÖ 3 multi-agent workflows operational
- ‚úÖ Automatic handoffs working (no manual intervention)
- ‚úÖ State persistence enables resume after interruption
- ‚úÖ Workflows complete 90%+ of tasks without errors

#### Milestone 4.2: Agent Sub-Specialization (2 weeks)

**Week 1: Sub-Agent Implementation**
```yaml
code_reviewer:
  parent: "Validator"
  specialization: "Pull request reviews"
  capabilities:
    - "SOLID principle checks"
    - "Test coverage analysis"
    - "Design pattern recognition"
    - "Security vulnerability detection"
  integration: "Azure DevOps/GitHub APIs"
  
performance_optimizer:
  parent: "Executor"
  specialization: "Performance optimization"
  capabilities:
    - "cProfile integration (profiling)"
    - "memory_profiler integration"
    - "Algorithmic optimization suggestions"
    - "Caching recommendations"
  tools: ["cProfile", "memory_profiler", "line_profiler"]
  
security_auditor:
  parent: "Validator"
  specialization: "Security scanning"
  capabilities:
    - "Bandit integration (security linting)"
    - "CVE database queries"
    - "OWASP Top 10 checks"
    - "Dependency vulnerability scanning"
  tools: ["bandit", "safety", "pip-audit"]
```

**Week 2: Registration + Testing**
```yaml
task_1:
  description: "Register with agent system"
  action: "Add to Corpus Callosum agent registry"
  
task_2:
  description: "Add to appropriate workflows"
  examples:
    - "code_reviewer ‚Üí feature_implementation workflow (after Validator)"
    - "performance_optimizer ‚Üí refactoring workflow (optional step)"
    - "security_auditor ‚Üí feature_implementation workflow (before deployment)"
  
task_3:
  description: "Comprehensive testing"
  tests:
    - "Unit tests for each sub-agent"
    - "Integration tests with parent agents"
    - "Workflow tests with sub-agents included"
  
task_4:
  description: "Documentation updates"
  content: "Sub-agent capabilities, usage examples, configuration"
```

**Success Criteria:**
- ‚úÖ 3 sub-agents operational
- ‚úÖ Tests passing (100%)
- ‚úÖ Documentation complete (usage guides)
- ‚úÖ Integration with workflows seamless

---

### Phase 5: Polish & Documentation (4 weeks)

**Objective:** Production-ready CORTEX 3.0 release

#### Milestone 5.1: User Documentation (2 weeks)

**Week 1: User Guide + Migration Guide**
```yaml
task_1:
  description: "CORTEX 3.0 User Guide (comprehensive)"
  sections:
    - "Getting Started (installation, first run)"
    - "Core Features (operations, planning, memory)"
    - "Advanced Features (dual-channel, agents, context)"
    - "Troubleshooting (common issues, solutions)"
    - "Best Practices (optimize CORTEX usage)"
  format: "Markdown + MkDocs"
  
task_2:
  description: "Migration guide (2.0 ‚Üí 3.0)"
  content:
    - "What's new in 3.0 (feature summary)"
    - "Breaking changes (if any)"
    - "Migration steps (data, configuration)"
    - "FAQ (migration questions)"
```

**Week 2: Video Tutorials + FAQ**
```yaml
task_1:
  description: "Video tutorials (optional)"
  topics:
    - "Quick start (5 min)"
    - "Conversation import (10 min)"
    - "Feature planning (15 min)"
    - "Agent workflows (20 min)"
  tools: "Screen recording (OBS, Camtasia)"
  
task_2:
  description: "FAQ updates"
  sources:
    - "Beta tester questions"
    - "Slack/Discord community questions"
    - "GitHub issues"
  
task_3:
  description: "Troubleshooting guide"
  format: "Symptom ‚Üí Diagnosis ‚Üí Solution"
```

**Success Criteria:**
- ‚úÖ User guide comprehensive (all features documented)
- ‚úÖ Migration guide clear (smooth 2.0 ‚Üí 3.0 transition)
- ‚úÖ Video tutorials helpful (90%+ satisfaction)
- ‚úÖ FAQ addresses 95%+ of questions

#### Milestone 5.2: Performance Optimization (1 week)

```yaml
task_1:
  description: "Profile all operations"
  target: "<2s for typical usage"
  tools: "cProfile, line_profiler"
  
task_2:
  description: "Optimize database queries (Tier 1-3)"
  approach:
    - "Add indexes to frequently queried columns"
    - "Use batch queries (not row-by-row)"
    - "Cache query results (avoid repeated queries)"
  
task_3:
  description: "Memory optimization"
  target: "Handle large workspaces (1000+ files, 500+ conversations)"
  approach:
    - "Streaming processing (don't load everything)"
    - "Lazy loading (load on demand)"
    - "Garbage collection tuning"
  
task_4:
  description: "Background processing"
  implementation: "Non-blocking operations (don't freeze UI)"
  examples:
    - "Conversation import (background task)"
    - "Fusion layer correlation (background task)"
    - "Context layer analysis (background task)"
```

**Success Criteria:**
- ‚úÖ All operations <2s for typical usage
- ‚úÖ Database queries optimized (50%+ faster)
- ‚úÖ Large workspaces handled efficiently
- ‚úÖ Non-blocking UI (responsive)

#### Milestone 5.3: Final Testing & Release (1 week)

```yaml
task_1:
  description: "Integration testing"
  scope: "All components together (end-to-end)"
  scenarios:
    - "New user onboarding (first run ‚Üí productive)"
    - "Feature planning ‚Üí Implementation ‚Üí Verification"
    - "Conversation import ‚Üí Fusion ‚Üí Narrative ‚Üí Continue command"
    - "Multi-agent workflows (feature, refactor, debug)"
  
task_2:
  description: "User acceptance testing (beta users)"
  testers: "10-20 beta users (mixed experience levels)"
  duration: "3-5 days"
  feedback: "Surveys + interviews"
  
task_3:
  description: "Performance benchmarking"
  metrics:
    - "Startup time (<2s)"
    - "Operation execution time (<2s)"
    - "Fusion correlation time (<1s for 100+ events)"
    - "Memory usage (<500MB for typical workspace)"
  
task_4:
  description: "Security audit"
  focus:
    - "Sensitive data redaction (passwords, tokens)"
    - "Local-only storage (no data exfiltration)"
    - "Encryption at rest (SQLite)"
    - "Dependency vulnerabilities (pip-audit)"
  
task_5:
  description: "Release notes preparation"
  content:
    - "What's new in 3.0"
    - "Breaking changes"
    - "Migration guide"
    - "Known issues"
    - "Roadmap for 3.1"
  
task_6:
  description: "CORTEX 3.0 RELEASE! üéâ"
  actions:
    - "Tag release (git tag v3.0.0)"
    - "Publish to GitHub"
    - "Announce to community"
    - "Update documentation site"
```

**Success Criteria:**
- ‚úÖ All integration tests passing
- ‚úÖ User acceptance testing >90% satisfaction
- ‚úÖ Performance benchmarks met
- ‚úÖ Security audit clean (no critical issues)
- ‚úÖ Release notes complete
- ‚úÖ CORTEX 3.0 RELEASED!

---

## üìä Timeline Summary

| Phase | Description | Duration | Start Week | End Week |
|-------|-------------|----------|------------|----------|
| **Phase 0** | Test Stabilization (BLOCKING) | 2 weeks | Week 1 | Week 2 |
| **Phase 1.1** | Simplified Operations | 3 weeks | Week 3 | Week 5 |
| **Phase 1.2** | Template Integration | 1 week | Week 6 | Week 6 |
| **Phase 1.3** | Interactive Tutorials | 2 weeks | Week 7 | Week 8 |
| **Phase 2** | Dual-Channel Memory | 14 weeks | Week 9 | Week 22 |
| **Phase 3** | Intelligent Context | 6 weeks | Week 9 | Week 14 |
| **Phase 4** | Enhanced Agents | 4 weeks | Week 23 | Week 26 |
| **Phase 5** | Polish & Release | 4 weeks | Week 27 | Week 30 |

**Total Duration:** 30 weeks (~7.5 months)

**Parallel Execution:**
- Phase 2 + Phase 3 run in parallel (Weeks 9-22, but Phase 3 ends Week 14)
- Saves 8 weeks compared to sequential execution

---

## üéØ Success Metrics

### Key Performance Indicators

| Metric | Baseline (2.0) | Target (3.0) | Measurement |
|--------|---------------|--------------|-------------|
| **Test Pass Rate** | 93.0% | 100% | pytest output |
| **"Continue" Success** | 60% | 85% | User survey + A/B test |
| **Context Completeness** | 70% | 95% | Audit of context quality |
| **Plan Execution Tracking** | 0% | 90% | Automated verification |
| **Narrative Quality** | 50% | 90% | Manual review |
| **User Adoption** | N/A | 80% | % users import ‚â•1 conversation |
| **Performance (Operations)** | 5s | <2s | Benchmark |
| **Performance (Fusion)** | N/A | <1s | Benchmark (100+ events) |

### Quality Gates

**Phase 0 (BLOCKING):**
- ‚úÖ 100% non-skipped test pass rate
- ‚úÖ SKULL-007 compliance (no status inflation)
- ‚úÖ Green CI/CD pipeline

**Phase 1:**
- ‚úÖ All 7 operations working end-to-end
- ‚úÖ Template integration complete (zero-execution help)
- ‚úÖ Tutorial completion rate >80%

**Phase 2:**
- ‚úÖ "Continue" success rate 60% ‚Üí 85%
- ‚úÖ Conversation import working (manual path)
- ‚úÖ Narratives generated automatically

**Phase 3:**
- ‚úÖ Complexity warnings operational
- ‚úÖ Debt score accurate (validated)
- ‚úÖ Impact predictions 80%+ accurate

**Phase 4:**
- ‚úÖ 3 multi-agent workflows operational
- ‚úÖ 3 sub-agents implemented and tested

**Phase 5:**
- ‚úÖ User acceptance testing >90% satisfaction
- ‚úÖ Performance benchmarks met
- ‚úÖ Security audit clean

---

## üö® Risk Management

### High-Risk Items

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Phase 0 test failures persist** | Medium | CRITICAL | Apply optimization-principles.yaml, allocate extra week if needed |
| **Dual-channel fusion accuracy low** | Medium | High | Extensive testing with real data, adjust algorithms iteratively |
| **User adoption of conversation import low** | Medium | High | Excellent tutorials, example conversations, clear value proposition |
| **Performance issues on large workspaces** | Medium | Medium | Early performance testing, optimization in Phase 5 |
| **Scope creep (feature requests)** | High | Medium | Strict phase gates, defer non-MVP features to 3.1 |

### Medium-Risk Items

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **ML model accuracy below target** | Medium | Medium | Fallback to rule-based predictions, iterative training |
| **Agent coordination complexity** | Low | Medium | Start simple (3 workflows), expand incrementally |
| **Documentation falls behind** | Medium | Low | Dedicate Phase 5 to documentation, involve technical writer |

---

## üìö References & Dependencies

### Design Documents
- **Architecture Planning:** `cortex-brain/CORTEX-3.0-ARCHITECTURE-PLANNING.md`
- **Executive Summary:** `cortex-brain/CORTEX-3.0-EXECUTIVE-SUMMARY.md`
- **Dual-Channel Memory Design:** `cortex-brain/CORTEX-3.0-DUAL-CHANNEL-MEMORY-DESIGN.md`
- **Optimization Principles:** `cortex-brain/optimization-principles.yaml`
- **Test Strategy:** `cortex-brain/test-strategy.yaml`

### Existing Code
- **Conversation Import Plugin:** `src/plugins/conversation_import_plugin.py` (prototype)
- **Ambient Daemon:** `scripts/cortex/auto_capture_daemon.py` (production)
- **Template System:** `cortex-brain/response-templates.yaml` (86+ templates)
- **Agent System:** `src/cortex_agents/` (10 agents operational)

### External Dependencies
- **Complexity Analysis:** `radon`, `mccabe`, `cognitive-complexity`
- **Debt Detection:** `vulture`, `pylint`, `bandit`
- **Security Scanning:** `pip-audit`, `safety`
- **Performance Profiling:** `cProfile`, `memory_profiler`, `line_profiler`

---

## üîç Next Steps

### Immediate (This Week - Week 1)

‚òê **Approval Confirmation**
   - Review this implementation plan
   - Approve timeline and phases
   - Confirm resource allocation

‚òê **Phase 0 Kickoff**
   - Begin test stabilization work
   - Categorize 63 skipped tests (BLOCKING/WARNING/PRAGMATIC)
   - Start fixing BLOCKING tests

### Week 2

‚òê **Phase 0 Completion**
   - Fix remaining BLOCKING tests
   - Document deferral reasons for WARNING tests
   - Achieve 100% non-skipped test pass rate
   - Green CI/CD pipeline

### Week 3-5 (Phase 1.1)

‚òê **Simplified Operations Implementation**
   - Ship 7 operations as monolithic MVPs
   - Apply optimization-principles.yaml patterns
   - Comprehensive testing

### Week 6-8 (Phase 1.2-1.3)

‚òê **Template Integration + Interactive Tutorials**
   - Wire templates to entry point and agents
   - Create three-tier tutorial system
   - User testing and refinement

### Week 9-22 (Phase 2 + Phase 3 parallel)

‚òê **Dual-Channel Memory + Intelligent Context**
   - Conversation import, fusion, narratives (Phase 2)
   - Complexity analysis, debt detection, impact prediction (Phase 3)
   - Parallel execution saves 8 weeks

### Week 23-26 (Phase 4)

‚òê **Enhanced Agent System**
   - Multi-agent workflows
   - Agent sub-specialization
   - Integration testing

### Week 27-30 (Phase 5)

‚òê **Polish & Release**
   - User documentation
   - Performance optimization
   - Final testing
   - CORTEX 3.0 RELEASE! üéâ

---

## ‚úÖ Approval & Sign-off

**Implementation Plan Reviewed By:** [Pending]  
**Approved By:** [Pending]  
**Approval Date:** [Pending]

**Decision Points:**
- ‚òê Approve 30-week timeline with parallel execution
- ‚òê Approve Phase 0 as BLOCKING prerequisite
- ‚òê Approve simplified operations approach (monolithic-then-modular)
- ‚òê Approve dual-channel memory as highest priority
- ‚òê Approve resource allocation (developer time, tools, infrastructure)

**Next Action:** Begin Phase 0 (Test Stabilization) immediately upon approval.

---

**Implementation Plan Date:** 2025-11-14  
**CORTEX Version:** 3.0.0-plan  
**Status:** Awaiting Approval  
**Timeline:** 30 weeks (~7.5 months)  
**Estimated Completion:** July 2026

---

*"From vision to reality: The roadmap to CORTEX 3.0"*
