# CORTEX 3.1 Token Optimization Plan
# Goal: Reduce "Summarizing conversation history..." interruptions
# Author: Asif Hussain | © 2024-2025
# Date: 2025-11-16
# Status: IMPLEMENTATION READY

metadata:
  version: "3.1.0"
  type: "optimization_plan"
  target: "token_reduction"
  priority: "high"
  estimated_impact: "60-80% reduction in context size"

# ==============================================================================
# PHASE 1: Discovery & Measurement (Analysis)
# ==============================================================================
phase_1_discovery:
  objective: "Establish baseline and identify optimization targets"
  duration: "30 minutes"
  status: "COMPLETE"
  
  baseline_measurements:
    entry_point:
      file: ".github/prompts/CORTEX.prompt.md"
      lines: 482
      words: 3396
      characters: 26205
      estimated_tokens: 6800  # ~3.9 chars per token average
    
    response_templates:
      file: "cortex-brain/response-templates.yaml"
      lines: 3048
      estimated_tokens: 7900  # Large file, high token count
    
    shared_modules:
      directory: "prompts/shared"
      file_count: 32
      total_size_kb: 346.67
      estimated_tokens: 90000  # All modules combined
    
    file_references_in_entry:
      total_count: 15  # Unique #file: directives
      files:
        - "response-templates.yaml"
        - "story.md"
        - "setup-guide.md"
        - "help_plan_feature.md"
        - "technical-reference.md"
        - "agents-guide.md"
        - "tracking-guide.md"
        - "configuration-reference.md"
        - "operations-reference.md"
        - "plugin-system.md"
        - "limitations-and-status.md"
        - "test-strategy.yaml"
        - "optimization-principles.yaml"
        - "platform-switch-plugin.md"
  
  conversation_analysis:
    # Typical conversation flow
    turn_1:
      user_request: 200  # tokens
      entry_point_loaded: 6800
      total_input: 7000
      
    turn_2:
      previous_context: 7000
      new_user_request: 200
      total_input: 7200
      
    turn_3:
      previous_context: 14200
      new_user_request: 200
      total_input: 14400  # Approaching summarization threshold
      
    turn_4:
      # SUMMARIZATION TRIGGERED (typically around 15k-20k tokens)
      previous_summarized: 8000
      new_user_request: 200
      total_input: 8200
  
  findings:
    high_token_operations:
      - operation: "help"
        reason: "Loads entire response-templates.yaml (7900 tokens)"
      - operation: "documentation reference"
        reason: "Loads full module files (5k-15k tokens each)"
      - operation: "multi-turn conversations"
        reason: "Context accumulation compounds"
    
    optimization_targets:
      - target: "response-templates.yaml"
        current: 7900
        potential: 1500  # 81% reduction with lazy loading
      - target: "entry point"
        current: 6800
        potential: 2000  # 70% reduction with lite version
      - target: "module loading"
        current: "Full files"
        potential: "Section extraction (60% reduction)"

# ==============================================================================
# PHASE 2: Lazy Loading Implementation (Quick Win)
# ==============================================================================
phase_2_lazy_loading:
  objective: "Load content on-demand instead of upfront"
  duration: "2 hours"
  status: "READY_FOR_IMPLEMENTATION"
  estimated_impact: "70-80% token reduction"
  
  task_2_1_lite_entry_point:
    description: "Create minimal entry point with intent-based loading"
    file: ".github/prompts/CORTEX-lite.prompt.md"
    design:
      structure: |
        # Core Structure (2000 tokens vs 6800)
        - Mandatory response format (keep)
        - Intent detection table (keep)
        - File reference table (keep)
        - Module loader instructions (new)
        - Remove all examples (defer to templates)
        - Remove all embedded docs (use #file: only when needed)
      
      intent_based_loading:
        help_request:
          triggers: ["help", "what can cortex do"]
          loads: ["response-templates.yaml (help section only)"]
        
        documentation_request:
          triggers: ["story", "setup", "technical"]
          loads: ["Specific module only"]
        
        operation_request:
          triggers: ["plan", "cleanup", "sync"]
          loads: ["Operation-specific context only"]
    
    implementation:
      - Remove 8 embedded examples (save ~2000 tokens)
      - Remove duplicate documentation sections (save ~1500 tokens)
      - Add dynamic module loader instructions (add ~300 tokens)
      - Keep response format templates (mandatory)
    
    expected_result:
      before: 6800
      after: 2000
      reduction: "70.6%"
  
  task_2_2_split_response_templates:
    description: "Split response-templates.yaml into category files"
    directory: "cortex-brain/response-templates/"
    design:
      files:
        - "help.yaml" (200-300 tokens)
        - "agents.yaml" (800-1000 tokens)
        - "operations.yaml" (1200-1500 tokens)
        - "errors.yaml" (400-500 tokens)
        - "plugins.yaml" (300-400 tokens)
        - "planning.yaml" (600-800 tokens)
        - "brain-performance.yaml" (800-1000 tokens)
        - "questions.yaml" (1500-2000 tokens)
      
      loading_strategy: |
        Instead of:
          Load entire response-templates.yaml (7900 tokens)
        
        Use:
          Load category based on detected intent:
          - "help" → load help.yaml (300 tokens) 
          - "plan" → load planning.yaml (800 tokens)
          - "status" → load operations.yaml (1500 tokens)
    
    expected_result:
      before: 7900  # Full file
      after: 300-1500  # Category-specific
      avg_reduction: "81%"
  
  task_2_3_module_section_extraction:
    description: "Load specific sections of documentation modules"
    approach: |
      Instead of loading entire technical-reference.md (15k tokens),
      extract relevant section based on user query:
      
      User: "How does Tier 1 work?"
      Load: technical-reference.md#tier-1-api section only (2k tokens)
      
      User: "Show agent system"
      Load: agents-guide.md#agent-coordination section (3k tokens)
    
    implementation_note: |
      Requires Copilot to intelligently extract markdown sections.
      Uses heading-based navigation (## Section Name)
    
    expected_result:
      reduction: "60-70% per module load"

  task_2_4_token_budget_system:
    description: "Add token awareness to CORTEX responses"
    design:
      budget_limits:
        simple_question: 3000  # tokens max
        complex_task: 8000  # tokens max
        planning_session: 12000  # tokens max
      
      behavior:
        under_budget:
          action: "Load normally"
        approaching_budget:
          action: "Switch to lite mode automatically"
          threshold: 80  # % of budget
        over_budget:
          action: "Suggest conversation reset"
          message: "This conversation is getting long. Consider starting a new chat for better performance."
    
    implementation:
      location: "CORTEX-lite.prompt.md"
      instructions: |
        Monitor cumulative token usage in conversation.
        When approaching 80% of budget, switch to concise mode automatically.
        Warn user at 90% with reset suggestion.

# ==============================================================================
# PHASE 3: Context Compression (Medium Effort)
# ==============================================================================
phase_3_context_compression:
  objective: "Intelligent context reduction without accuracy loss"
  duration: "3 hours"
  status: "READY_FOR_IMPLEMENTATION"
  estimated_impact: "30-40% additional reduction"
  
  task_3_1_smart_yaml_truncation:
    description: "Load YAML schema + relevant sections only"
    example:
      full_load: |
        response-templates.yaml (3048 lines, 7900 tokens)
      
      smart_load: |
        # Load structure + requested section
        schema_version: "2.1.0"
        templates:
          help_table:  # Only requested template
            triggers: [...]
            content: [...]
        # ... other templates omitted
      
      result:
        before: 7900
        after: 1200
        reduction: "85%"
  
  task_3_2_response_verbosity_levels:
    description: "Add concise/standard/detailed modes"
    design:
      modes:
        concise:
          target_tokens: 500-1000
          use_case: "Quick questions, simple tasks"
          format: "Minimal examples, terse explanations"
        
        standard:
          target_tokens: 1500-3000
          use_case: "Normal operations (default)"
          format: "Balanced detail, key examples"
        
        detailed:
          target_tokens: 3000-6000
          use_case: "Complex planning, architecture discussions"
          format: "Comprehensive examples, full context"
      
      auto_detection:
        simple_query: "concise"
        multi_step_task: "standard"
        planning_request: "detailed"
      
      user_override:
        commands:
          - "be more concise"
          - "give me details"
          - "explain thoroughly"
    
    implementation:
      location: "Response format rules in CORTEX-lite.prompt.md"
      default: "standard"
  
  task_3_3_conversation_checkpoints:
    description: "Create explicit reset points to clear context"
    design:
      automatic_checkpoints:
        - after_major_task_completion
        - after_multi_phase_work
        - after_15_conversation_turns
      
      checkpoint_behavior:
        save_state:
          - Current task status
          - Key decisions made
          - Files modified
        
        clear_context:
          - Old conversation history
          - Loaded documentation
          - Unused references
        
        provide_summary:
          format: "YAML in cortex-brain/session-checkpoints/"
          content: |
            session_id: "checkpoint-2025-11-16-001"
            completed_tasks: [...]
            decisions: [...]
            next_steps: [...]
      
      user_trigger:
        command: "checkpoint" or "reset conversation"
        behavior: "Save state, clear context, start fresh"
  
  task_3_4_token_aware_formatter:
    description: "Dynamically adjust response format based on token budget"
    logic: |
      IF tokens_remaining < 5000:
        - Remove examples from responses
        - Use bullet points instead of paragraphs
        - Omit "Smart Hint" section
        - Reduce Next Steps to 3 items max
      
      IF tokens_remaining < 3000:
        - Switch to ultra-concise mode
        - One-line responses only
        - Suggest conversation reset
      
      IF tokens_remaining < 1000:
        - Force conversation reset
        - Save checkpoint automatically

# ==============================================================================
# PHASE 4: Validation & Rollback Safety (Critical)
# ==============================================================================
phase_4_validation:
  objective: "Ensure optimization doesn't degrade accuracy"
  duration: "2 hours"
  status: "READY_FOR_IMPLEMENTATION"
  
  task_4_1_ab_testing:
    description: "Compare original vs optimized side-by-side"
    test_scenarios:
      - scenario: "Simple help query"
        original_tokens: 6800
        optimized_tokens: 2000
        accuracy_check: "Both return same help table"
      
      - scenario: "Multi-step planning"
        original_tokens: 15000
        optimized_tokens: 8000
        accuracy_check: "Both create valid 4-phase plans"
      
      - scenario: "Documentation lookup"
        original_tokens: 12000
        optimized_tokens: 4000
        accuracy_check: "Both provide correct module section"
    
    acceptance_criteria:
      accuracy_threshold: "90%"  # Must match original 90%+ of the time
      token_reduction_target: "60%"
      user_satisfaction: "No complaints about missing info"
  
  task_4_2_accuracy_measurement:
    description: "Quantify information loss (if any)"
    metrics:
      - metric: "Response completeness"
        measurement: "Compare key facts included"
        threshold: ">90%"
      
      - metric: "Intent detection accuracy"
        measurement: "Same routing as original"
        threshold: "100%"
      
      - metric: "Module selection"
        measurement: "Correct file loaded"
        threshold: "100%"
  
  task_4_3_user_satisfaction_tracking:
    description: "Monitor real-world usage"
    indicators:
      positive:
        - Fewer "Summarizing..." interruptions
        - Faster response times
        - No complaints about missing information
      
      negative:
        - User says "you forgot..."
        - Repeated clarification requests
        - Complaints about terse responses
    
    rollback_trigger:
      condition: "Accuracy drops below 90% OR 3+ user complaints"
      action: "Revert to CORTEX.prompt.md original"
  
  task_4_4_rollback_mechanism:
    description: "Easy reversion if optimization fails"
    design:
      backup:
        file: ".github/prompts/CORTEX.prompt.md.backup"
        created: "Before applying Phase 2 changes"
      
      rollback_command:
        trigger: "User says 'rollback optimization' or accuracy fails"
        action: |
          1. Copy backup to CORTEX.prompt.md
          2. Delete CORTEX-lite.prompt.md
          3. Merge split response-templates back to single file
          4. Notify user: "Reverted to original (higher tokens, full context)"
      
      partial_rollback:
        option: "Keep some optimizations, revert others"
        example: "Keep lite entry point, restore full response templates"

# ==============================================================================
# IMPLEMENTATION TIMELINE
# ==============================================================================
timeline:
  phase_1: "30 minutes (COMPLETE)"
  phase_2: "2 hours"
  phase_3: "3 hours"
  phase_4: "2 hours"
  total: "7.5 hours"
  
  milestones:
    milestone_1:
      name: "Lite Entry Point Ready"
      deliverables:
        - "CORTEX-lite.prompt.md created"
        - "Token reduction: 70%"
      validation: "help command works with lite version"
    
    milestone_2:
      name: "Response Templates Split"
      deliverables:
        - "8 category-specific YAML files"
        - "Lazy loading logic implemented"
      validation: "help loads help.yaml only (300 tokens vs 7900)"
    
    milestone_3:
      name: "Context Compression Active"
      deliverables:
        - "Verbosity levels implemented"
        - "Smart YAML truncation working"
        - "Checkpoint system ready"
      validation: "Multi-turn conversations stay under threshold"
    
    milestone_4:
      name: "Validation Complete"
      deliverables:
        - "A/B testing passed"
        - "Accuracy ≥90%"
        - "Rollback mechanism tested"
      validation: "Production-ready, no regressions"

# ==============================================================================
# SUCCESS METRICS
# ==============================================================================
success_metrics:
  primary:
    - metric: "Turns before summarization"
      baseline: "4-5 turns"
      target: "12-15 turns"
      improvement: "200-300%"
    
    - metric: "Average tokens per turn"
      baseline: 7000
      target: 2500
      reduction: "64%"
    
    - metric: "User satisfaction"
      measurement: "No complaints about missing info"
      threshold: "95% satisfaction"
  
  secondary:
    - metric: "Response time"
      expected: "Slightly faster (less parsing)"
    
    - metric: "Accuracy"
      threshold: "≥90% vs original"
    
    - metric: "Context retention"
      measurement: "Still remembers key facts from early turns"

# ==============================================================================
# RISKS & MITIGATION
# ==============================================================================
risks:
  risk_1:
    description: "Lazy loading may miss necessary context"
    probability: "Medium"
    impact: "High"
    mitigation:
      - "Conservative default: Load standard mode on ambiguity"
      - "User can always say 'give me details' to override"
      - "A/B testing catches this before production"
  
  risk_2:
    description: "Split YAML files harder to maintain"
    probability: "Low"
    impact: "Medium"
    mitigation:
      - "Keep master template index with all categories"
      - "Validation script ensures consistency"
      - "Can merge back if maintenance burden too high"
  
  risk_3:
    description: "Users prefer full context (even with summarization)"
    probability: "Low"
    impact: "High"
    mitigation:
      - "Rollback mechanism ready"
      - "Make original available as CORTEX-full.prompt.md"
      - "User choice: lite vs full"

# ==============================================================================
# NEXT ACTIONS
# ==============================================================================
next_actions:
  immediate:
    - "Create CORTEX-lite.prompt.md (Phase 2, Task 1)"
    - "Split response-templates.yaml (Phase 2, Task 2)"
    - "Test lite version with help command"
  
  follow_up:
    - "Implement verbosity levels (Phase 3, Task 2)"
    - "Add checkpoint system (Phase 3, Task 3)"
    - "Run A/B testing (Phase 4, Task 1)"
  
  monitoring:
    - "Track turns-before-summarization metric"
    - "Collect user feedback on information completeness"
    - "Measure accuracy against original"

# ==============================================================================
# APPROVAL & SIGN-OFF
# ==============================================================================
approval:
  plan_reviewed: true
  accuracy_threshold_acceptable: true  # 90% is reasonable
  rollback_safety: true  # Mechanism in place
  ready_for_implementation: true

  concerns_addressed:
    - "Lazy loading won't sacrifice accuracy (90% threshold)"
    - "Rollback available if issues arise"
    - "Phased approach allows early validation"
    - "Token budget system prevents silent degradation"

  recommendation: "PROCEED WITH ALL PHASES"
  confidence: "HIGH"
  expected_outcome: "60-80% token reduction, 200-300% more turns before summarization"
