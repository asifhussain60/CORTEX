# TDD Mastery Phase 3 - M3.2 Production Optimization Plan

**Milestone:** M3.2 - Production Optimization  
**Status:** Ready to Implement  
**Estimated Time:** 7-9 days  
**Priority:** High (performance critical for production deployment)

---

## Overview

Optimize TDD workflow orchestrator for production performance through profiling, caching, batch processing, and memory management.

**Performance Targets:**
- Test generation: <500ms per function
- Smell detection: <200ms per file
- Session save/load: <100ms

---

## Phase 1: Performance Profiling (2-3 days)

### Objective
Identify bottlenecks in test generation, AST parsing, and smell detection.

### Tasks

#### Task 1.1: Profiling Infrastructure
**File:** `tests/performance/test_profiling.py`

```python
import cProfile
import pstats
from io import StringIO
from workflows.tdd_workflow_orchestrator import TDDWorkflowOrchestrator, TDDWorkflowConfig

def profile_test_generation():
    """Profile test generation performance."""
    profiler = cProfile.Profile()
    
    config = TDDWorkflowConfig(project_root=".")
    orchestrator = TDDWorkflowOrchestrator(config)
    orchestrator.start_session("profiling_test")
    
    # Profile test generation
    profiler.enable()
    result = orchestrator.generate_tests(
        source_file="src/auth/login.py",
        scenarios=["edge_cases", "domain_knowledge"]
    )
    profiler.disable()
    
    # Analyze results
    s = StringIO()
    stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
    stats.print_stats(20)  # Top 20 slowest functions
    
    print(s.getvalue())
    return stats

def profile_smell_detection():
    """Profile code smell detection."""
    # Similar profiling for RefactoringEngine
    pass

def profile_session_persistence():
    """Profile session save/load."""
    # Profile PageTracker operations
    pass
```

#### Task 1.2: Benchmark Suite
**File:** `tests/performance/benchmarks.py`

Create benchmarks for:
- Single function test generation
- Entire module test generation
- Code smell detection on various file sizes
- Session save/load with different context sizes

**Metrics to Track:**
- Execution time (mean, median, p95, p99)
- Memory usage (peak, average)
- CPU usage
- I/O operations (file reads, database queries)

#### Task 1.3: Bottleneck Analysis

Expected bottlenecks to investigate:
1. **AST Parsing:** `ast.parse()` called repeatedly on same files
2. **Pattern Matching:** Edge case analyzer scanning entire AST tree
3. **Database I/O:** SQLite queries in PageTracker
4. **File I/O:** Reading source files multiple times

---

## Phase 2: Caching Strategies (2-3 days)

### Objective
Implement intelligent caching to avoid redundant computations.

### Tasks

#### Task 2.1: AST Tree Caching
**File:** `src/workflows/ast_cache.py`

```python
from typing import Dict, Optional
import ast
import hashlib
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

@dataclass
class CachedAST:
    """Cached AST tree with metadata."""
    tree: ast.AST
    file_hash: str
    cached_at: datetime
    access_count: int = 0

class ASTCache:
    """Cache for parsed AST trees."""
    
    def __init__(self, max_size: int = 100):
        self.max_size = max_size
        self.cache: Dict[str, CachedAST] = {}
    
    def get(self, filepath: str) -> Optional[ast.AST]:
        """Get cached AST tree if file unchanged."""
        current_hash = self._compute_file_hash(filepath)
        
        if filepath in self.cache:
            cached = self.cache[filepath]
            if cached.file_hash == current_hash:
                cached.access_count += 1
                return cached.tree
            else:
                # File changed, invalidate cache
                del self.cache[filepath]
        
        return None
    
    def put(self, filepath: str, tree: ast.AST) -> None:
        """Cache AST tree."""
        if len(self.cache) >= self.max_size:
            self._evict_lru()
        
        file_hash = self._compute_file_hash(filepath)
        self.cache[filepath] = CachedAST(
            tree=tree,
            file_hash=file_hash,
            cached_at=datetime.now()
        )
    
    def _compute_file_hash(self, filepath: str) -> str:
        """Compute file content hash."""
        with open(filepath, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _evict_lru(self) -> None:
        """Evict least recently used entry."""
        if not self.cache:
            return
        
        lru_key = min(self.cache.items(), key=lambda x: x[1].access_count)[0]
        del self.cache[lru_key]
    
    def clear(self) -> None:
        """Clear cache."""
        self.cache.clear()
    
    def stats(self) -> Dict[str, int]:
        """Get cache statistics."""
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "total_accesses": sum(c.access_count for c in self.cache.values())
        }
```

**Integration:**
- Inject ASTCache into EdgeCaseAnalyzer
- Inject ASTCache into CodeSmellDetector
- Add cache statistics to session summary

#### Task 2.2: Pattern Cache
**File:** `src/workflows/pattern_cache.py`

Cache edge case patterns to avoid repeated regex matching:

```python
from typing import Dict, List, Optional
from dataclasses import dataclass
import ast

@dataclass
class PatternMatch:
    """Cached pattern match result."""
    patterns: List[str]  # Matched pattern names
    locations: List[int]  # Line numbers
    confidence: float

class PatternCache:
    """Cache for edge case pattern matches."""
    
    def __init__(self):
        self.cache: Dict[str, Dict[str, PatternMatch]] = {}
        # Key: filepath -> function_name -> PatternMatch
    
    def get(self, filepath: str, function_name: str) -> Optional[PatternMatch]:
        """Get cached pattern match."""
        if filepath in self.cache:
            return self.cache[filepath].get(function_name)
        return None
    
    def put(self, filepath: str, function_name: str, match: PatternMatch) -> None:
        """Cache pattern match."""
        if filepath not in self.cache:
            self.cache[filepath] = {}
        self.cache[filepath][function_name] = match
    
    def invalidate_file(self, filepath: str) -> None:
        """Invalidate all patterns for a file."""
        if filepath in self.cache:
            del self.cache[filepath]
```

#### Task 2.3: Smell Detection Cache
**File:** `src/workflows/smell_cache.py`

Cache smell detection results:

```python
from typing import Dict, List
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class SmellCacheEntry:
    """Cached smell detection result."""
    smells: List[dict]
    file_hash: str
    cached_at: datetime
    ttl: timedelta = timedelta(hours=1)
    
    def is_expired(self) -> bool:
        return datetime.now() - self.cached_at > self.ttl

class SmellCache:
    """Cache for code smell detection results."""
    
    def __init__(self):
        self.cache: Dict[str, SmellCacheEntry] = {}
    
    def get(self, filepath: str, file_hash: str) -> Optional[List[dict]]:
        """Get cached smells if valid."""
        if filepath in self.cache:
            entry = self.cache[filepath]
            if entry.file_hash == file_hash and not entry.is_expired():
                return entry.smells
            else:
                del self.cache[filepath]
        return None
    
    def put(self, filepath: str, file_hash: str, smells: List[dict]) -> None:
        """Cache smell detection results."""
        self.cache[filepath] = SmellCacheEntry(
            smells=smells,
            file_hash=file_hash,
            cached_at=datetime.now()
        )
```

---

## Phase 3: Batch Processing (2-3 days)

### Objective
Process multiple functions/files in parallel for efficiency.

### Tasks

#### Task 3.1: Batch Test Generation
**File:** `src/workflows/batch_processor.py`

```python
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import ast

class BatchTestGenerator:
    """Generate tests for multiple functions in parallel."""
    
    def __init__(self, orchestrator, max_workers: int = 4):
        self.orchestrator = orchestrator
        self.max_workers = max_workers
    
    def generate_tests_batch(
        self,
        source_files: List[str],
        scenarios: List[str] = None
    ) -> Dict[str, Any]:
        """Generate tests for multiple files in parallel."""
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {
                executor.submit(
                    self.orchestrator.generate_tests,
                    source_file,
                    None,
                    scenarios
                ): source_file
                for source_file in source_files
            }
            
            for future in as_completed(future_to_file):
                source_file = future_to_file[future]
                try:
                    result = future.result()
                    results[source_file] = result
                except Exception as e:
                    results[source_file] = {"error": str(e)}
        
        return results
    
    def generate_tests_module(
        self,
        module_path: str,
        scenarios: List[str] = None
    ) -> Dict[str, Any]:
        """Generate tests for entire module (all .py files)."""
        from pathlib import Path
        
        module_dir = Path(module_path)
        source_files = list(module_dir.glob("**/*.py"))
        
        return self.generate_tests_batch(
            [str(f) for f in source_files],
            scenarios
        )
```

#### Task 3.2: Parallel Smell Detection

```python
def analyze_files_batch(
    self,
    source_files: List[str]
) -> Dict[str, List[dict]]:
    """Analyze multiple files for code smells in parallel."""
    results = {}
    
    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
        future_to_file = {
            executor.submit(
                self.smell_detector.analyze_file,
                source_file
            ): source_file
            for source_file in source_files
        }
        
        for future in as_completed(future_to_file):
            source_file = future_to_file[future]
            try:
                smells = future.result()
                results[source_file] = smells
            except Exception as e:
                results[source_file] = []
    
    return results
```

---

## Phase 4: Memory Optimization (1-2 days)

### Objective
Reduce memory footprint for large codebases.

### Tasks

#### Task 4.1: Streaming File Reader

```python
class StreamingFileReader:
    """Stream large files line-by-line to reduce memory."""
    
    @staticmethod
    def read_function(filepath: str, function_name: str) -> str:
        """Read only specific function from file."""
        with open(filepath, 'r') as f:
            in_function = False
            function_lines = []
            indent_level = None
            
            for line in f:
                if f"def {function_name}(" in line:
                    in_function = True
                    indent_level = len(line) - len(line.lstrip())
                    function_lines.append(line)
                elif in_function:
                    current_indent = len(line) - len(line.lstrip())
                    if line.strip() and current_indent <= indent_level:
                        break
                    function_lines.append(line)
            
            return ''.join(function_lines)
```

#### Task 4.2: AST Tree Release

```python
def release_ast_tree(tree: ast.AST) -> None:
    """Release AST tree from memory after analysis."""
    # Clear node references
    for node in ast.walk(tree):
        node._fields = ()
        node._attributes = ()
    
    del tree
```

#### Task 4.3: Session History Limits

```python
class SessionHistoryManager:
    """Manage session history with retention limits."""
    
    def __init__(self, max_sessions: int = 100, max_age_days: int = 30):
        self.max_sessions = max_sessions
        self.max_age_days = max_age_days
    
    def cleanup_old_sessions(self, db_path: str) -> int:
        """Remove old sessions from database."""
        import sqlite3
        from datetime import datetime, timedelta
        
        cutoff_date = datetime.now() - timedelta(days=self.max_age_days)
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            DELETE FROM tdd_contexts
            WHERE last_updated < ?
        """, (cutoff_date.isoformat(),))
        
        deleted = cursor.rowcount
        conn.commit()
        conn.close()
        
        return deleted
```

---

## Integration Plan

### Step 1: Add Caching to Orchestrator

```python
class TDDWorkflowOrchestrator:
    def __init__(self, config: TDDWorkflowConfig):
        # ... existing initialization ...
        
        # Add caches
        self.ast_cache = ASTCache(max_size=100)
        self.pattern_cache = PatternCache()
        self.smell_cache = SmellCache()
        
        # Inject caches into components
        self.edge_case_analyzer.set_cache(self.ast_cache, self.pattern_cache)
        self.smell_detector.set_cache(self.ast_cache, self.smell_cache)
```

### Step 2: Add Batch Processing API

```python
    def generate_tests_batch(
        self,
        source_files: List[str],
        scenarios: List[str] = None
    ) -> Dict[str, Any]:
        """Generate tests for multiple files (batch mode)."""
        batch_processor = BatchTestGenerator(self, max_workers=4)
        return batch_processor.generate_tests_batch(source_files, scenarios)
```

### Step 3: Add Cache Management Methods

```python
    def clear_caches(self) -> None:
        """Clear all caches."""
        self.ast_cache.clear()
        self.pattern_cache.cache.clear()
        self.smell_cache.cache.clear()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        return {
            "ast_cache": self.ast_cache.stats(),
            "pattern_cache": {"size": len(self.pattern_cache.cache)},
            "smell_cache": {"size": len(self.smell_cache.cache)}
        }
```

---

## Testing Plan

### Performance Tests
**File:** `tests/performance/test_optimization.py`

```python
def test_ast_cache_speedup():
    """AST cache should improve performance on repeated parsing."""
    # Measure without cache
    # Measure with cache
    # Assert speedup >50%

def test_batch_processing_speedup():
    """Batch processing should be faster than sequential."""
    # Sequential: 10 files × 500ms = 5000ms
    # Batch (4 workers): ~1500ms
    # Assert speedup >2x

def test_memory_usage():
    """Memory usage should stay within bounds."""
    # Generate tests for 100 functions
    # Assert peak memory <500MB
```

---

## Performance Targets Validation

| Metric | Target | Measurement | Status |
|--------|--------|-------------|--------|
| Test generation (single function) | <500ms | TBD | Pending |
| Test generation (batch, 10 files) | <2000ms | TBD | Pending |
| Smell detection (single file) | <200ms | TBD | Pending |
| Session save | <100ms | TBD | Pending |
| Session load | <100ms | TBD | Pending |
| Memory usage (100 functions) | <500MB | TBD | Pending |
| Cache hit rate (AST) | >70% | TBD | Pending |

---

## Deliverables

1. **Caching System** (3 caches: AST, patterns, smells)
2. **Batch Processing** (parallel test generation and smell detection)
3. **Memory Optimization** (streaming reader, AST release, history limits)
4. **Performance Tests** (benchmarks, profiling, validation)
5. **Documentation** (optimization guide, cache usage, batch API)

---

## Success Criteria

- ✅ Test generation <500ms per function
- ✅ Smell detection <200ms per file
- ✅ Session save/load <100ms
- ✅ Cache hit rate >70%
- ✅ Batch processing >2x speedup
- ✅ Memory usage <500MB for 100 functions
- ✅ All existing tests still passing
- ✅ No performance regressions

---

## Timeline

**Week 1:**
- Days 1-2: Performance profiling infrastructure
- Day 3: Bottleneck analysis and optimization strategy

**Week 2:**
- Days 4-5: AST caching implementation
- Day 6: Pattern and smell caching

**Week 3:**
- Days 7-8: Batch processing implementation
- Day 9: Memory optimization

**Testing & Validation:**
- Continuous throughout implementation
- Final validation on Day 9-10

---

**Ready to Implement:** All tasks documented, architecture designed, tests planned.

**Next Action:** User approval to begin M3.2 implementation or proceed with M3.3 video demonstrations.
