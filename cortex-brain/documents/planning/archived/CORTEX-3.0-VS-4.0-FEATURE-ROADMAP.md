# CORTEX 3.0 vs 4.0 Feature Roadmap
## Based on Copilot-Instructions Pattern Analysis

**Date:** 2025-01-18  
**Source Analysis:** COPILOT-INSTRUCTIONS-PATTERN-ANALYSIS.md  
**Planning Horizon:** Q1 2025 (3.0) ‚Üí Q3 2025 (4.0)

---

## Executive Summary

This roadmap categorizes patterns from the `copilot-instructions` repository analysis into two implementation phases:

- **CORTEX 3.0** (Current Architecture): Enhancements that leverage existing modular template system, agent coordination, and governance structure
- **CORTEX 4.0** (Architectural Evolution): Fundamental changes requiring agent lifecycle redesign, hybrid knowledge architecture, and pluggable external dependencies

**Key Decision Criteria:**
- ‚úÖ **CORTEX 3.0:** Additive enhancements, no breaking changes, works within modular architecture
- ‚è∏Ô∏è **CORTEX 4.0:** Requires core redesign, introduces new architectural paradigms, breaking changes acceptable

---

## CORTEX 3.0: Modular Enhancement Phase

**Timeline:** Q1-Q2 2025 (8-12 weeks)  
**Philosophy:** Enhance existing architecture without breaking changes  
**Compatibility:** Fully backward compatible with CORTEX 2.1

### Feature 1: Meta-Template Documentation System

**Priority:** üî¥ **HIGH**  
**Effort:** 1-2 weeks  
**Status:** Not Started

**What It Is:**
Create self-referential documentation defining how to create response templates, governance rules, and agent configurations. Establishes consistent structure across all CORTEX documentation.

**Implementation:**
```yaml
# cortex-brain/templates/meta-template.yaml

meta_template:
  purpose: "Defines standard structure for all CORTEX response templates"
  
  required_sections:
    - name: "template_name"
      type: "string"
      description: "Unique identifier for template"
      example: "work_planner_success"
    
    - name: "trigger_patterns"
      type: "array<string>"
      description: "Natural language keywords that activate template"
      example: ["plan a feature", "let's plan", "interactive planning"]
    
    - name: "response_type"
      type: "enum"
      values: ["narrative", "table", "detailed", "structured"]
      description: "Output format for response"
    
    - name: "content"
      type: "string"
      description: "Template body with 5-part structure"
      required_placeholders: ["{{user_request}}", "{{challenge}}", "{{response}}"]
  
  validation_rules:
    - "All placeholders must be declared in required_fields"
    - "Decorative headers must use ‚îÅ (U+2501) character"
    - "No hardcoded counts (use placeholders or qualitative descriptions)"
    - "Template name must match file name pattern"
  
  validation_script: "src/validators/template_validator.py"
  
  examples:
    - "cortex-brain/response-templates.yaml (86+ templates)"
    - "cortex-brain/brain-protection-rules.yaml (protection layers)"
```

**Files to Create:**
- `cortex-brain/templates/meta-template.yaml` - Template about templates
- `governance/meta-rules.md` - Rules about rules
- `cortex-brain/agents/meta-agent.template.yaml` - Agent about agents
- `src/validators/meta_validator.py` - Automated validation

**Benefits:**
- ‚úÖ Consistent structure across all documentation
- ‚úÖ Self-validating system (templates check themselves)
- ‚úÖ Easier onboarding for contributors
- ‚úÖ Automated quality checks during `optimize_cortex`

**Risks:**
- ‚ö†Ô∏è Requires updating existing 86+ templates to conform to meta-template
- ‚ö†Ô∏è Validation overhead may slow template creation initially

**Success Criteria:**
- [ ] Meta-template defines all required sections for templates, rules, agents
- [ ] Automated validator catches structural violations
- [ ] All existing templates pass meta-template validation
- [ ] Documentation explains meta-pattern philosophy

**Dependencies:** None (independent feature)

---

### Feature 2: Explicit Agent Mode Selection

**Priority:** üü° **MEDIUM**  
**Effort:** 2-3 weeks  
**Status:** Not Started

**What It Is:**
Allow users to manually activate specific CORTEX agents using `@agent-name` syntax, overriding automatic intent routing. Provides explicit control over AI behavior for specific workflows.

**Current Limitation:**
CORTEX automatically routes requests to agents via Intent Router. Users cannot explicitly request a specific agent (e.g., "I want Work Planner, not Code Executor").

**Implementation:**
```python
# src/agents/agent_selector.py

class AgentSelector:
    """Handles explicit agent mode selection via @mentions"""
    
    AGENT_PATTERNS = {
        "@work-planner": "work-planner",
        "@code-executor": "code-executor",
        "@brain-protector": "brain-protector",
        "@screenshot-analyzer": "screenshot-analyzer",
        "@test-generator": "test-generator",
        "@health-validator": "health-validator",
        "@error-corrector": "error-corrector",
        "@commit-handler": "commit-handler",
        "@change-governor": "change-governor",
        "@intent-router": "intent-router"
    }
    
    def parse_agent_mention(self, user_message: str) -> tuple[str, str]:
        """
        Extract @agent-name from message
        Returns: (agent_name, cleaned_message)
        """
        for pattern, agent_name in self.AGENT_PATTERNS.items():
            if pattern in user_message:
                cleaned = user_message.replace(pattern, "").strip()
                return (agent_name, cleaned)
        return (None, user_message)
    
    def route_to_agent(self, agent_name: str, request: str, context: dict):
        """Execute request with explicitly selected agent"""
        agent = self.agent_registry.get_agent(agent_name)
        if not agent:
            raise ValueError(f"Unknown agent: {agent_name}")
        return agent.execute(request, context)

# Example usage
selector = AgentSelector()
agent, message = selector.parse_agent_mention("@work-planner Create authentication plan")
# Returns: ("work-planner", "Create authentication plan")
```

**User Experience:**
```
User: "@work-planner Create authentication plan"
      ‚Üì
CORTEX: "üß† CORTEX Work Planner (Explicit Mode)
         
         I'll focus on planning, not implementation.
         Let me ask some clarifying questions..."

User: "Continue"  # Without @mention
      ‚Üì
CORTEX: Stays in Work Planner mode for remainder of session
```

**Agent Configuration Files:**
```yaml
# cortex-brain/agents/work-planner.config.yaml

agent_name: "work-planner"
display_name: "Work Planner"
purpose: "Strategic feature planning and breakdown"

behavioral_constraints:
  never_generates_code: true
  output_format: "Markdown planning documents"
  focus: ["planning", "architecture", "requirements"]

priorities:
  - "Ask clarifying questions"
  - "Break work into phases"
  - "Identify risks and dependencies"
  - "Define acceptance criteria"

response_style:
  tone: "Strategic, forward-thinking"
  level_of_detail: "High-level architecture"
  format: "Phase-based roadmap"

validation:
  must_ask_questions: true
  confidence_threshold: 0.7
  requires_user_approval: true
```

**Files to Create:**
- `src/agents/agent_selector.py` - @mention parsing and routing
- `cortex-brain/agents/*.config.yaml` - Configuration for each agent (10 files)
- `cortex-brain/agents/meta-agent.template.yaml` - Standard agent config structure
- `docs/user-guides/explicit-agent-selection.md` - User documentation

**Benefits:**
- ‚úÖ User control over AI behavior
- ‚úÖ Explicit workflow mode selection
- ‚úÖ Prevents unwanted intent routing
- ‚úÖ Better for advanced users who know exact agent needed

**Risks:**
- ‚ö†Ô∏è Beginners may not know which agent to select
- ‚ö†Ô∏è Could bypass Intent Router's intelligence
- ‚ö†Ô∏è May create confusion if user selects wrong agent

**Success Criteria:**
- [ ] @mention syntax recognized in user messages
- [ ] Agent configurations loaded from YAML files
- [ ] Explicit mode overrides intent routing
- [ ] Agent stays active for session duration
- [ ] User can exit explicit mode with "auto" or "reset"

**Dependencies:**
- Requires Feature 1 (Meta-Agent Template) for configuration structure

---

### Feature 3: Confidence-Based Challenge Workflows

**Priority:** üü° **MEDIUM**  
**Effort:** 1 week  
**Status:** Not Started

**What It Is:**
Display explicit confidence percentages in response Challenge sections and adjust questioning depth based on confidence level.

**Current Limitation:**
CORTEX has binary Challenge behavior (Accept vs Challenge) without showing confidence scores to user.

**Implementation:**
```python
# src/agents/intent_router.py (enhancement)

class IntentRouter:
    def parse_with_confidence(self, user_message: str) -> dict:
        """Parse intent and calculate confidence score"""
        intent = self.detect_intent(user_message)
        confidence = self.calculate_confidence(user_message, intent)
        
        # Determine workflow based on confidence
        if confidence >= 0.8:
            workflow = "high_confidence"  # Minimal questions
        elif confidence >= 0.5:
            workflow = "medium_confidence"  # 1-2 confirming questions
        else:
            workflow = "low_confidence"  # Detailed clarifying questions
        
        return {
            "intent": intent,
            "confidence": confidence,
            "workflow": workflow,
            "questions": self.generate_questions(confidence, intent)
        }
```

**Response Template Enhancement:**
```yaml
# cortex-brain/response-templates.yaml (updated)

work_planner_success:
  trigger: ["plan a feature", "let's plan"]
  content: |
    üéØ My Understanding Of Your Request:
       {{user_request_summary}}
    
    ‚ö†Ô∏è Challenge: {{challenge_type}}
       Confidence: {{confidence}}% ({{confidence_label}})
       
       {{#if confidence < 80}}
       I need to ask some questions before planning:
       {{questions}}
       {{/if}}
       
       {{#if confidence >= 80}}
       High confidence. Proceeding with minimal clarification.
       {{/if}}
    
    üí¨ Response:
       {{response_content}}
```

**User Experience:**
```
High Confidence (85%):
  "üéØ My Understanding: You want JWT authentication
   ‚ö†Ô∏è Challenge: ‚úì Accept
   Confidence: 85% (high)
   
   I'm confident in the scope. Proceeding with planning."

Medium Confidence (65%):
  "üéØ My Understanding: You want authentication
   ‚ö†Ô∏è Challenge: ‚ö° Challenge
   Confidence: 65% (medium)
   
   Let me confirm: JWT or OAuth? Admin and user roles?"

Low Confidence (40%):
  "üéØ My Understanding: You want to add security
   ‚ö†Ô∏è Challenge: ‚ö° Challenge
   Confidence: 40% (low)
   
   I need detailed clarification:
   1. What authentication methods?
   2. Which user types?
   3. Integration requirements?
   4. Security constraints?"
```

**Files to Create:**
- `src/agents/confidence_calculator.py` - Confidence scoring logic
- `cortex-brain/templates/follow-up-questions/*.yaml` - Question banks by scenario
- `src/agents/question_generator.py` - Dynamic question generation

**Benefits:**
- ‚úÖ Transparency in AI decision-making
- ‚úÖ Adaptive questioning (fewer questions for clear requests)
- ‚úÖ Better user trust (explicit confidence display)
- ‚úÖ Matches copilot-instructions follow-up pattern

**Risks:**
- ‚ö†Ô∏è Low confidence scores may discourage users
- ‚ö†Ô∏è Calibration needed (what counts as "high" confidence?)

**Success Criteria:**
- [ ] Confidence percentage displayed in all Challenge sections
- [ ] High confidence (80%+) asks 0-1 questions
- [ ] Medium confidence (50-80%) asks 1-2 questions
- [ ] Low confidence (<50%) asks 3-5 detailed questions
- [ ] Confidence labels accurate ("low", "medium", "high")

**Dependencies:** None (independent feature)

---

### Feature 4: Technology-Specific Prompt Library

**Priority:** üü¢ **LOW**  
**Effort:** 2 weeks  
**Status:** Not Started

**What It Is:**
Create curated library of technology-specific setup prompts (Husky for .NET, pre-commit for Python, Jest for TypeScript) organized by language/framework.

**Current Limitation:**
CORTEX prompts are framework-agnostic. Users need technology-specific guidance (e.g., "How to set up Husky in .NET?").

**Implementation:**
```
cortex-brain/prompts/technology-specific/
‚îú‚îÄ‚îÄ dotnet/
‚îÇ   ‚îú‚îÄ‚îÄ husky-setup.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ xunit-testing.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ aspnet-api-design.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ entity-framework-setup.prompt.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ pre-commit-setup.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ pytest-setup.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ poetry-dependency-management.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ fastapi-setup.prompt.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ typescript/
‚îÇ   ‚îú‚îÄ‚îÄ jest-testing.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ eslint-setup.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ react-component-patterns.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ nextjs-setup.prompt.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ java/
‚îÇ   ‚îú‚îÄ‚îÄ maven-setup.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ junit-testing.prompt.md
‚îÇ   ‚îú‚îÄ‚îÄ spring-boot-setup.prompt.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ meta-technology-prompt.yaml (template for tech prompts)
```

**Example Prompt:**
```markdown
# Husky Setup for .NET Projects

## Purpose
Configure Husky git hooks for .NET projects to enforce code quality at commit time.

## Prerequisites
- .NET 6+ SDK
- Node.js 18+ (for Husky)
- Git repository initialized

## Step-by-Step Setup

### 1. Install Husky
```bash
npm install --save-dev husky
npx husky install
```

### 2. Create Pre-Commit Hook
```bash
npx husky add .husky/pre-commit "dotnet format --verify-no-changes"
```

### 3. Create Pre-Push Hook
```bash
npx husky add .husky/pre-push "dotnet test --no-restore"
```

### 4. Configure .NET Format
Create `.editorconfig` with coding style rules.

## Validation
- Commit should trigger format check
- Push should run tests
- Failed checks block commit/push

## Troubleshooting
- If hooks don't run: Check file permissions (`chmod +x .husky/*`)
- If format fails: Run `dotnet format` to auto-fix
- If tests fail: Fix failing tests before pushing

## Related Prompts
- `xunit-testing.prompt.md` - Setting up XUnit
- `aspnet-api-design.prompt.md` - API conventions
```

**Integration with Setup Operation:**
```python
# src/operations/setup.py (enhancement)

class SetupOperation:
    def detect_project_type(self, workspace_path: str) -> str:
        """Detect project type from file patterns"""
        if (workspace_path / "*.csproj").exists():
            return "dotnet"
        elif (workspace_path / "pyproject.toml").exists():
            return "python"
        elif (workspace_path / "package.json").exists():
            return "typescript"
        return "unknown"
    
    def suggest_technology_prompts(self, project_type: str):
        """Suggest relevant technology prompts during setup"""
        prompts_path = Path("cortex-brain/prompts/technology-specific")
        tech_prompts = list((prompts_path / project_type).glob("*.md"))
        
        print(f"üìö Available {project_type} setup guides:")
        for prompt in tech_prompts:
            print(f"  - {prompt.stem}")
        print("  Say 'show [prompt-name]' to view details")
```

**User Experience:**
```
User: "setup"
      ‚Üì
CORTEX: "Detected .NET project
         
         üìö Available .NET setup guides:
         - husky-setup (Git hooks)
         - xunit-testing (Test framework)
         - aspnet-api-design (API conventions)
         - entity-framework-setup (Database)
         
         Say 'show husky-setup' to view details"

User: "show husky-setup"
      ‚Üì
CORTEX: [Displays husky-setup.prompt.md with step-by-step instructions]
```

**Benefits:**
- ‚úÖ Technology-specific best practices
- ‚úÖ Faster project setup
- ‚úÖ Curated by CORTEX maintainers
- ‚úÖ Community can contribute prompts

**Risks:**
- ‚ö†Ô∏è Maintenance burden (keep prompts up-to-date)
- ‚ö†Ô∏è Scope creep (how many technologies to support?)

**Success Criteria:**
- [ ] 4+ technologies supported (.NET, Python, TypeScript, Java)
- [ ] 3-5 prompts per technology
- [ ] Meta-prompt template defines structure
- [ ] Setup operation suggests relevant prompts
- [ ] Community contribution guide published

**Dependencies:** None (independent content addition)

---

### Feature 5: User Instruction System

**Priority:** üü° **MEDIUM**  
**Effort:** 2 weeks  
**Status:** Not Started

**What It Is:**
Allow users to define domain-specific instructions in `workspace/instructions/` that CORTEX reads and enforces, similar to copilot-instructions repository pattern.

**Current Limitation:**
CORTEX governance rules (Tier 0) are universal. Users cannot define project-specific or domain-specific rules (DDD, API contracts, naming conventions).

**Implementation:**
```python
# src/tier0/user_instructions.py

class UserInstructionLoader:
    """Load and enforce user-defined instructions from workspace"""
    
    def __init__(self, workspace_path: Path):
        self.workspace_path = workspace_path
        self.instructions_path = workspace_path / "instructions"
        self.loaded_instructions = {}
    
    def discover_instructions(self) -> list[dict]:
        """Find all *.instructions.md files in workspace"""
        if not self.instructions_path.exists():
            return []
        
        instructions = []
        for file in self.instructions_path.glob("*.instructions.md"):
            content = file.read_text()
            instructions.append({
                "name": file.stem,
                "path": file,
                "content": content,
                "rules": self.parse_rules(content)
            })
        return instructions
    
    def parse_rules(self, content: str) -> list[str]:
        """Extract rules from instruction file"""
        # Look for "Rules:", "Standards:", "Requirements:" sections
        # Return list of rules
        pass
    
    def enforce_instruction(self, code: str, instruction_name: str) -> dict:
        """Validate code against user instruction rules"""
        instruction = self.loaded_instructions.get(instruction_name)
        violations = []
        
        for rule in instruction["rules"]:
            if not self.check_rule(code, rule):
                violations.append({
                    "rule": rule,
                    "instruction": instruction_name,
                    "severity": "warning"
                })
        
        return {
            "valid": len(violations) == 0,
            "violations": violations
        }
```

**User Workspace Structure:**
```
workspace/
‚îú‚îÄ‚îÄ instructions/
‚îÇ   ‚îú‚îÄ‚îÄ ddd-rules.instructions.md
‚îÇ   ‚îú‚îÄ‚îÄ api-contracts.instructions.md
‚îÇ   ‚îú‚îÄ‚îÄ naming-conventions.instructions.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md (user's instruction guide)
‚îú‚îÄ‚îÄ src/
‚îî‚îÄ‚îÄ tests/
```

**Example User Instruction:**
```markdown
# DDD Rules

## Purpose
Enforce Domain-Driven Design principles in this project.

## Rules

### 1. Bounded Context Separation
- Each bounded context has its own directory under `src/`
- No cross-context references (use published events)

### 2. Aggregate Root Enforcement
- Only aggregate roots are public
- Entities and value objects are internal to aggregate

### 3. Repository Pattern
- Repositories only access aggregate roots
- No direct database access outside repositories

### 4. Domain Events
- All state changes publish domain events
- Event names use past tense (UserCreated, OrderPlaced)

## Validation
- [ ] Each bounded context in separate directory
- [ ] Aggregate roots marked with `IAggregateRoot` interface
- [ ] Repository interfaces defined for each aggregate
- [ ] Domain events inherit from `DomainEvent` base class

## Examples
See `src/Ordering/` for reference implementation.
```

**Integration with Brain Protector:**
```python
# src/agents/brain_protector.py (enhancement)

class BrainProtector:
    def validate_change(self, change_request: dict) -> dict:
        """Validate change against Tier 0 rules AND user instructions"""
        
        # Existing Tier 0 validation
        tier0_result = self.validate_tier0_rules(change_request)
        
        # New: User instruction validation
        user_result = self.user_instruction_loader.validate_against_instructions(
            code=change_request["proposed_code"],
            workspace=change_request["workspace_path"]
        )
        
        # Merge results
        all_violations = tier0_result["violations"] + user_result["violations"]
        
        return {
            "allowed": len(all_violations) == 0,
            "violations": all_violations,
            "tier0_ok": tier0_result["valid"],
            "user_instructions_ok": user_result["valid"]
        }
```

**User Experience:**
```
User: "Create new Order entity"
      ‚Üì
CORTEX: "Checking against user instructions...
         
         ‚ö†Ô∏è User Instruction Violation (ddd-rules.instructions.md):
         Rule: 'Aggregate roots must implement IAggregateRoot'
         
         Should I add IAggregateRoot interface to Order entity?"

User: "Yes"
      ‚Üì
CORTEX: "‚úÖ Order entity now complies with DDD rules"
```

**Files to Create:**
- `src/tier0/user_instructions.py` - User instruction loader
- `src/validators/instruction_validator.py` - Rule checking
- `docs/user-guides/creating-instructions.md` - How to write instructions
- `workspace/instructions/README.template.md` - Template for user's instruction README

**Benefits:**
- ‚úÖ Domain-specific rules without modifying CORTEX core
- ‚úÖ Project-specific best practices enforcement
- ‚úÖ Team standards codified as instructions
- ‚úÖ Brain Protector validates both Tier 0 and user rules

**Risks:**
- ‚ö†Ô∏è Users may write conflicting instructions
- ‚ö†Ô∏è Instruction parsing may be brittle
- ‚ö†Ô∏è Validation overhead for every code change

**Success Criteria:**
- [ ] CORTEX discovers `workspace/instructions/*.instructions.md` files
- [ ] Brain Protector validates against user instructions
- [ ] Violations display instruction name and rule violated
- [ ] User can disable specific instructions via config
- [ ] Documentation explains instruction file format

**Dependencies:** None (independent feature)

---

## CORTEX 3.0 Summary

**Total Effort:** 8-13 weeks (parallel development)  
**Features:** 5 major enhancements  
**Backward Compatibility:** 100% (no breaking changes)

### Implementation Priority Matrix

| Feature | Effort | Impact | Priority | Quarter |
|---------|--------|--------|----------|---------|
| Meta-Template Documentation | 1-2 weeks | High | üî¥ High | Q1 2025 |
| Confidence-Based Workflows | 1 week | Medium | üü° Medium | Q1 2025 |
| Explicit Agent Selection | 2-3 weeks | Medium | üü° Medium | Q1-Q2 2025 |
| User Instruction System | 2 weeks | Medium | üü° Medium | Q2 2025 |
| Technology-Specific Prompts | 2 weeks | Low | üü¢ Low | Q2 2025 |

### Development Phases

**Phase 1: Foundation (Weeks 1-3)**
- Meta-template documentation
- Confidence-based workflows
- Agent configuration structure

**Phase 2: User Control (Weeks 4-7)**
- Explicit agent selection
- User instruction system
- Integration testing

**Phase 3: Content Expansion (Weeks 8-12)**
- Technology-specific prompt library
- Community contribution guidelines
- Documentation polish

**Phase 4: Polish & Release (Week 13)**
- Integration testing
- User acceptance testing
- Release notes and migration guide

---

## CORTEX 4.0: Architectural Evolution Phase

**Timeline:** Q3-Q4 2025 (16-24 weeks)  
**Philosophy:** Fundamental architectural redesign  
**Compatibility:** Breaking changes acceptable (migration path provided)

### Feature 1: Dynamic Agent Composition

**Priority:** üî¥ **HIGH**  
**Effort:** 6-8 weeks  
**Status:** Research Phase

**What It Is:**
Enable runtime creation of new agents from YAML configuration files without modifying CORTEX source code. Users define custom agents with specific behaviors, constraints, and priorities.

**Current Limitation:**
Agents are hardcoded in `src/agents/*.py`. Adding new agent requires code changes and CORTEX update.

**Architectural Change Required:**
```python
# CORTEX 3.0 (Current)
src/agents/
‚îú‚îÄ‚îÄ work_planner.py (hardcoded)
‚îú‚îÄ‚îÄ code_executor.py (hardcoded)
‚îî‚îÄ‚îÄ brain_protector.py (hardcoded)

# Agent behavior defined in Python code
# New agents require code changes

# CORTEX 4.0 (Future)
cortex-brain/agents/
‚îú‚îÄ‚îÄ work-planner.agent.yaml (data-driven)
‚îú‚îÄ‚îÄ code-executor.agent.yaml (data-driven)
‚îú‚îÄ‚îÄ user-defined-agent.agent.yaml (user-created!)

# Agent engine interprets YAML and creates agent at runtime
# No code changes needed for new agents
```

**Implementation Concept:**
```yaml
# cortex-brain/agents/api-reviewer.agent.yaml (user-created)

agent_definition:
  name: "api-reviewer"
  type: "reviewer"
  purpose: "Review API designs for RESTful best practices"
  
  capabilities:
    - "Read OpenAPI specifications"
    - "Validate HTTP method usage"
    - "Check naming conventions"
    - "Suggest improvements"
  
  constraints:
    never_generates_code: true
    output_format: "Review report with suggestions"
    requires_openapi_file: true
  
  priorities:
    - "RESTful conventions"
    - "Naming consistency"
    - "Error handling patterns"
    - "Documentation completeness"
  
  response_template:
    format: |
      ## API Review Report
      
      ### Endpoint: {{endpoint}}
      
      ‚úÖ **Strengths:**
      {{strengths}}
      
      ‚ö†Ô∏è **Issues:**
      {{issues}}
      
      üí° **Suggestions:**
      {{suggestions}}
  
  validation_rules:
    - rule: "HTTP GET should not have request body"
      severity: "error"
    - rule: "Resource names should be plural"
      severity: "warning"
    - rule: "Use 201 for resource creation"
      severity: "warning"
```

**Agent Engine:**
```python
# src/core/agent_engine.py (new in CORTEX 4.0)

class AgentEngine:
    """Dynamically creates agents from YAML definitions"""
    
    def load_agent(self, agent_yaml_path: Path) -> Agent:
        """Load agent definition and create agent instance"""
        definition = yaml.safe_load(agent_yaml_path.read_text())
        
        return DynamicAgent(
            name=definition["agent_definition"]["name"],
            purpose=definition["agent_definition"]["purpose"],
            capabilities=definition["agent_definition"]["capabilities"],
            constraints=definition["agent_definition"]["constraints"],
            priorities=definition["agent_definition"]["priorities"],
            response_template=definition["agent_definition"]["response_template"],
            validation_rules=definition["agent_definition"]["validation_rules"]
        )
    
    def register_agent(self, agent: Agent):
        """Register agent for intent routing"""
        self.agent_registry.register(agent)

class DynamicAgent(Agent):
    """Agent created from YAML configuration"""
    
    def __init__(self, name, purpose, capabilities, constraints, priorities, 
                 response_template, validation_rules):
        self.name = name
        self.purpose = purpose
        self.capabilities = capabilities
        self.constraints = constraints
        self.priorities = priorities
        self.response_template = response_template
        self.validation_rules = validation_rules
    
    def execute(self, request: str, context: dict) -> dict:
        """Execute agent logic based on YAML definition"""
        # Interpret YAML rules and execute
        # Generate response using response_template
        # Validate against validation_rules
        pass
```

**Why CORTEX 4.0:**
- Requires complete redesign of agent lifecycle
- Agent registry must support dynamic registration
- Intent Router must adapt to new agents
- Response template engine needs YAML interpretation
- Validation framework must be pluggable

**Benefits:**
- ‚úÖ Users create custom agents without code changes
- ‚úÖ Community can share agent definitions
- ‚úÖ Domain-specific agents (API Reviewer, DDD Validator, Security Auditor)
- ‚úÖ Faster innovation (no CORTEX release needed for new agents)

**Risks:**
- ‚ö†Ô∏è Complex YAML definitions may be error-prone
- ‚ö†Ô∏è Performance overhead of YAML interpretation
- ‚ö†Ô∏è Security concerns (malicious agent definitions)
- ‚ö†Ô∏è Breaking change (existing agents need migration)

**Migration Path:**
1. Convert existing hardcoded agents to YAML definitions
2. Provide migration tool: `cortex migrate-agents`
3. Deprecation period: Support both hardcoded and dynamic agents in 4.0.0
4. Full removal of hardcoded agents in 4.1.0

---

### Feature 2: Pluggable Knowledge Sources (Hybrid Architecture)

**Priority:** üü° **MEDIUM**  
**Effort:** 8-10 weeks  
**Status:** Research Phase

**What It Is:**
Redesign knowledge architecture to support optional external data sources (Microsoft MCP, Stack Overflow API, GitHub Docs) while maintaining local-first operation.

**Current Limitation:**
CORTEX has zero external dependencies. All knowledge stored locally in Tier 2. Cannot access real-time documentation or external best practices.

**Architectural Change Required:**
```python
# CORTEX 3.0 (Current)
Knowledge Source: Tier 2 Knowledge Graph (local only)
External Access: None
Dependency: Zero

# CORTEX 4.0 (Future)
Primary Source: Tier 2 Knowledge Graph (local, always available)
Optional Sources: MCP, Stack Overflow, GitHub Docs (when online)
Fallback: Always degrades to local patterns
Dependency: Optional, configurable
```

**Implementation Concept:**
```python
# src/tier2/hybrid_knowledge_graph.py (new in CORTEX 4.0)

class HybridKnowledgeGraph:
    """
    Multi-source knowledge system with intelligent fallback
    """
    
    def __init__(self, config: dict):
        self.local_kg = KnowledgeGraph()  # Tier 2 (existing)
        self.external_sources = self.load_external_sources(config)
        self.cache = KnowledgeCache()  # Cache external queries
    
    def load_external_sources(self, config: dict) -> list:
        """Load optional external knowledge sources from config"""
        sources = []
        
        if config.get("external_sources", {}).get("mcp_enabled", False):
            sources.append(MCPKnowledgeSource(
                api_key=config["mcp_api_key"],
                cache_duration=3600  # 1 hour cache
            ))
        
        if config.get("external_sources", {}).get("stackoverflow_enabled", False):
            sources.append(StackOverflowSource(
                cache_duration=86400  # 24 hour cache
            ))
        
        return sources
    
    def query_pattern(self, query: str, context: dict) -> dict:
        """
        Query knowledge with intelligent source selection
        """
        # 1. Always check local Tier 2 first (fast, reliable)
        local_results = self.local_kg.search_patterns(query)
        
        # 2. If high confidence local match, return immediately
        if local_results and local_results[0]["confidence"] > 0.9:
            return {
                "source": "local",
                "results": local_results,
                "external_queried": False
            }
        
        # 3. If external sources enabled and low local confidence, augment
        if self.external_sources and local_results[0]["confidence"] < 0.7:
            external_results = self.query_external_sources(query, context)
            combined = self.merge_results(local_results, external_results)
            return {
                "source": "hybrid",
                "results": combined,
                "external_queried": True
            }
        
        # 4. Return local results (external sources offline or disabled)
        return {
            "source": "local",
            "results": local_results,
            "external_queried": False
        }
    
    def query_external_sources(self, query: str, context: dict) -> list:
        """Query external sources with timeout and caching"""
        results = []
        
        for source in self.external_sources:
            try:
                # Check cache first
                cached = self.cache.get(source.name, query)
                if cached:
                    results.extend(cached)
                    continue
                
                # Query external source with timeout
                source_results = source.query(query, timeout=2.0)
                self.cache.set(source.name, query, source_results)
                results.extend(source_results)
            
            except TimeoutError:
                # External source unavailable, continue with local
                pass
            except Exception as e:
                # Log error, continue with local
                logger.warning(f"External source {source.name} failed: {e}")
        
        return results
    
    def merge_results(self, local: list, external: list) -> list:
        """
        Merge local and external results with conflict resolution
        - Local patterns always take precedence
        - External results augment, never replace
        - Confidence scores adjusted based on source
        """
        merged = []
        
        # Add local results with original confidence
        for result in local:
            result["source"] = "local"
            merged.append(result)
        
        # Add external results with adjusted confidence
        for result in external:
            result["source"] = "external"
            result["confidence"] *= 0.8  # External sources less confident
            merged.append(result)
        
        # Sort by confidence
        merged.sort(key=lambda r: r["confidence"], reverse=True)
        return merged
```

**Configuration:**
```json
// cortex.config.json (CORTEX 4.0)

{
  "tier2": {
    "knowledge_graph": {
      "local_first": true,
      "external_sources": {
        "mcp_enabled": false,
        "mcp_api_key": "${MCP_API_KEY}",
        "mcp_cache_duration": 3600,
        
        "stackoverflow_enabled": false,
        "stackoverflow_cache_duration": 86400,
        
        "github_docs_enabled": false,
        "github_docs_cache_duration": 3600
      },
      "fallback_behavior": {
        "offline_mode": "local_only",
        "timeout_mode": "local_only",
        "error_mode": "local_only"
      }
    }
  }
}
```

**Why CORTEX 4.0:**
- Requires fundamental change to knowledge architecture
- Cache layer needs sophisticated invalidation logic
- Conflict resolution between local and external sources
- Fallback mechanisms for offline operation
- Breaking change (Tier 2 API changes)

**Benefits:**
- ‚úÖ Access to real-time documentation (when online)
- ‚úÖ Augments local patterns with external best practices
- ‚úÖ Still works offline (degrades gracefully)
- ‚úÖ Configurable (users control what sources to enable)

**Risks:**
- ‚ö†Ô∏è External dependencies introduce failure modes
- ‚ö†Ô∏è Cache invalidation complexity
- ‚ö†Ô∏è Potential for conflicting advice (local vs external)
- ‚ö†Ô∏è Privacy concerns (queries sent to external services)

**Migration Path:**
1. All external sources disabled by default
2. Users opt-in via `cortex.config.json`
3. Existing Tier 2 API remains compatible
4. New hybrid API available for advanced users

---

### Feature 2.5: Model Context Protocol (MCP) Integration

**Priority:** üî¥ **HIGH**  
**Effort:** 16-20 weeks  
**Status:** Planning Complete (See MCP.md)  
**Document:** `.github/CopilotChats/MCP.md`

**What It Is:**
Integrate Model Context Protocol (MCP) to enable CORTEX to connect to external knowledge sources, databases, and specialized tools while maintaining local-first operation. MCP is Anthropic's open standard for AI systems to securely access external data sources.

**Current Status:**
- ‚úÖ **Pylance MCP Server Already Integrated!** (via GitHub Copilot)
  - Tools: `mcp_pylance_mcp_s_pylanceImports`, `mcp_pylance_mcp_s_pylanceWorkspaceUserFiles`, `mcp_pylance_mcp_s_pylanceRunCodeSnippet`
- ‚è∏Ô∏è **Planned MCP Servers:** MSSQL, Filesystem, GitHub, Stack Overflow

**Why MCP Matters:**
CORTEX 3.0 is completely local-first (no external dependencies). This is excellent for privacy and offline operation, but limits access to external knowledge (Stack Overflow solutions, API documentation, database schemas).

**MCP enables:**
- ‚úÖ Database introspection (SQL Server schemas for accurate query generation)
- ‚úÖ External knowledge (Stack Overflow error solutions)
- ‚úÖ Business tool integration (GitHub Issues, Jira, Slack)
- ‚úÖ Language-specific analysis (Pylance for Python, ESLint for JavaScript)

**Adaptive Local-First Architecture:**
```python
# CORTEX 4.0 Knowledge Resolution (Adaptive)

class AdaptiveKnowledgeResolver:
    """
    Tiered fallback for knowledge queries:
    1. Tier 1/2/3 (Local - Always Available) ‚Üê Priority
    2. MCP Servers (External - If enabled and online)
    3. MCP Cache (Previously fetched - Always available)
    """
    
    def resolve_query(self, query: str) -> dict:
        # Step 1: Check local tiers first (fast, always available)
        local_result = self.query_local_tiers(query)
        
        if local_result["confidence"] >= 0.8:
            return local_result  # High confidence from local data
        
        # Step 2: Optionally enhance with MCP (if enabled and online)
        if self.config.get("mcp_enabled") and self.is_online():
            mcp_result = self.query_mcp_servers(query)
            return self.merge_results(local_result, mcp_result)
        
        # Step 3: Check MCP cache (previously fetched)
        cached = self.get_cached_mcp_response(query)
        if cached:
            return self.merge_results(local_result, cached)
        
        # Step 4: Return local-only (offline mode)
        return local_result
```

**MCP Servers Roadmap:**

| Server | Priority | Purpose | Phase |
|--------|----------|---------|-------|
| Pylance | ‚úÖ **Already Integrated** | Python analysis, imports, refactoring | N/A |
| MSSQL | üî¥ **High** | Database schema introspection | Phase 3 |
| Stack Overflow | üü° **Medium** | Error solutions, best practices | Phase 4 |
| GitHub | üü° **Medium** | Issue integration, PR context | Phase 4 |
| Filesystem | üü¢ **Low** | Multi-repo analysis, safe file ops | Future |

**Implementation Phases:**

**Phase 1: MCP Foundation (Weeks 1-4)**
- [ ] `src/mcp/mcp_host.py` - MCP host infrastructure
- [ ] `src/mcp/server_registry.py` - MCP server registry
- [ ] `src/mcp/protocol.py` - MCP protocol handling
- [ ] Privacy enforcement layer (never send `.env`, keys, credentials)
- [ ] Configuration schema in `cortex.config.json`

**Phase 2: Pylance Deep Integration (Weeks 5-8)**
- [ ] Auto-enhance Tier 2 with Pylance-discovered dependencies
- [ ] Import relationship tracking (file A imports B)
- [ ] Module availability checking before suggesting imports
- [ ] Refactoring integration (fix imports, organize imports)

**Phase 3: Database Introspection (Weeks 9-12)**
- [ ] MSSQL MCP client implementation
- [ ] Schema caching in Tier 2
- [ ] SQL query generation using live schema
- [ ] Entity Framework model generation

**Phase 4: External Knowledge (Weeks 13-16)**
- [ ] Stack Overflow MCP client
- [ ] GitHub MCP client
- [ ] Error correction enhancement (Stack Overflow integration)
- [ ] GitHub Issue ‚Üî ADO synchronization

**Phase 5: Testing & Polish (Weeks 17-20)**
- [ ] Integration tests for all MCP servers
- [ ] Performance benchmarks (<500ms MCP query target)
- [ ] Privacy audit (verify no data leaks)
- [ ] User documentation (MCP setup guide)

**Configuration Example:**
```yaml
# cortex.config.json (CORTEX 4.0)

mcp:
  enabled: false  # Global toggle (opt-in)
  
  servers:
    pylance:
      enabled: true  # Already integrated
      auto_enhance_tier2: true
    
    mssql:
      enabled: false
      connections:
        - name: "Production DB"
          connection_string: "${MSSQL_CONNECTION_STRING}"
          read_only: true  # Safety: SELECT only
    
    stackoverflow:
      enabled: false
      cache_duration: 86400  # 24 hours
      rate_limit: 30  # requests per hour
    
    github:
      enabled: false
      auth_token: "${GITHUB_TOKEN}"
      cache_duration: 3600  # 1 hour
  
  privacy:
    never_send_to_external:
      - "*.env"
      - "*.key"
      - "*.pem"
      - "secrets/*"
    log_external_queries: true  # Audit trail
    require_user_approval: true  # Confirm before external query
```

**User Experience Example:**
```
User: "How do I fix asyncio.run() error in Python?"

CORTEX: "I found a local solution (Tier 2: 65% confidence)
         
         Would you like me to also check Stack Overflow?
         (This will send the error message to stackoverflow.com)
         
         [Yes, check Stack Overflow] [No, use local solution only]"

User: Clicks "Yes"

CORTEX: "‚úÖ Querying Stack Overflow...
         üìä Found 3 solutions (sorted by votes)
         üíæ Caching result locally (24h)
         üìù Logged query to cortex-brain/logs/mcp-queries.log
         
         Top solution (42 votes):
         [Shows Stack Overflow answer]"
```

**Why CORTEX 4.0:**
- Architectural complexity (MCP host infrastructure)
- Configuration overhaul (new MCP section)
- Privacy model change (from "never external" to "optionally external")
- Backward compatibility maintained but behavior changes significantly
- 16-20 week development timeline warrants major version

**Benefits:**
- ‚úÖ Smarter suggestions (external knowledge enhances local patterns)
- ‚úÖ Faster debugging (Stack Overflow integration)
- ‚úÖ Better SQL (database introspection prevents syntax errors)
- ‚úÖ Cross-tool workflow (GitHub Issues ‚Üî CORTEX ADO)
- ‚úÖ Competitive advantage (matches GitHub Copilot's external access)

**Risks & Mitigations:**

| Risk | Mitigation |
|------|------------|
| Privacy concerns | Opt-in only, explicit approval, audit trail |
| Performance degradation | Aggressive caching, local-first priority |
| External API rate limits | Per-server rate limits, quota monitoring |
| Breaking changes from 3.0 | MCP disabled = CORTEX 3.0 behavior (backward compatible) |

**Success Criteria:**
- [ ] MCP host can connect to 4+ MCP servers
- [ ] Privacy tests pass (no sensitive data leaked)
- [ ] MCP queries complete in <500ms average
- [ ] Works offline (degrades gracefully when MCP unavailable)
- [ ] 100% test pass rate for MCP integration
- [ ] User documentation complete (setup, privacy, troubleshooting)

**Dependencies:**
- Feature 2 (Pluggable Knowledge Sources) - MCP is the implementation mechanism
- Tier 2 Knowledge Graph - MCP results cached in Tier 2

**Reference Documentation:**
- Detailed MCP Plan: `.github/CopilotChats/MCP.md`
- MCP Specification: https://modelcontextprotocol.io
- Pylance MCP Already Used: `cortex-brain/documents/analysis/FILE-DEPENDENCY-ANALYSIS.md`

---

### Feature 2.6: Developer Performance Analytics System

**Priority:** üî¥ **HIGH**  
**Effort:** 10-14 weeks  
**Status:** Planned (CORTEX 4.0)

**What It Is:**
Comprehensive analytics system to track, measure, and visualize developer productivity improvements when using CORTEX. Quantifies CORTEX's impact with measurable metrics, ROI calculations, and team performance insights.

**Business Value:**
- ‚úÖ Quantify CORTEX's productivity impact (2-3x velocity improvements)
- ‚úÖ Identify top performers for mentorship/recognition
- ‚úÖ Data-driven CORTEX adoption decisions
- ‚úÖ Calculate ROI (hours saved √ó hourly rate)
- ‚úÖ Improve individual developer effectiveness

**Core Components:**

#### 1. Enhanced Tier 3 Analytics (Extend Existing)

**Current Tier 3 Capabilities (Already Built):**
- Git activity analysis (last 30 days)
- Commit velocity tracking per developer
- File hotspot identification (churn rate)
- Session analytics (best working times)
- Workflow effectiveness (TDD vs non-TDD)
- Code quality metrics (test coverage trends)

**New Tier 3 Enhancements:**

```python
# cortex-brain/tier3/productivity_dashboard.py

class ProductivityDashboard:
    """Real-time dashboard showing CORTEX effectiveness"""
    
    def get_developer_metrics(self, developer_id, date_range):
        return {
            "cortex_usage_hours": 42.5,
            "commits_with_cortex": 67,
            "commits_without_cortex": 23,
            
            "time_to_first_commit": {
                "with_cortex": "15 minutes",
                "without_cortex": "42 minutes",
                "improvement": "2.8x faster"
            },
            
            "time_to_merge": {
                "with_cortex": "1.8 hours",
                "without_cortex": "4.2 hours",
                "improvement": "2.3x faster"
            },
            
            "quality_metrics": {
                "test_pass_rate_with_cortex": 0.89,
                "test_pass_rate_without": 0.62,
                "improvement": "+27 points"
            },
            
            "rework_ratio": {
                "with_cortex": 0.15,  # 15% of commits are fixes
                "without_cortex": 0.42,
                "improvement": "64% less rework"
            }
        }
```

#### 2. CORTEX Impact Scoring System

**Automated scoring to quantify CORTEX effectiveness per developer:**

```python
# cortex-brain/tier3/impact_scoring.py

class CortexImpactScoring:
    """Score CORTEX's impact on developer productivity"""
    
    SCORING_WEIGHTS = {
        "velocity_improvement": 0.30,
        "quality_improvement": 0.25,
        "learning_curve": 0.20,
        "feature_adoption": 0.15,
        "code_health": 0.10
    }
    
    def calculate_impact_score(self, developer_id):
        """Returns score 0.0-10.0"""
        
        velocity = self._score_velocity(developer_id)      # 0-10
        quality = self._score_quality(developer_id)        # 0-10
        learning = self._score_learning_curve(developer_id) # 0-10
        adoption = self._score_feature_adoption(developer_id) # 0-10
        health = self._score_code_health(developer_id)     # 0-10
        
        weighted_score = (
            velocity * self.SCORING_WEIGHTS["velocity_improvement"] +
            quality * self.SCORING_WEIGHTS["quality_improvement"] +
            learning * self.SCORING_WEIGHTS["learning_curve"] +
            adoption * self.SCORING_WEIGHTS["feature_adoption"] +
            health * self.SCORING_WEIGHTS["code_health"]
        )
        
        return weighted_score
```

**Example Output:**
```
Developer: John Doe
CORTEX Impact Score: 8.7/10 (Highly Effective)

Breakdown:
‚îú‚îÄ Velocity: 9.2/10 (2.8x faster than pre-CORTEX)
‚îú‚îÄ Quality: 8.5/10 (89% test pass vs 65% baseline)
‚îú‚îÄ Learning: 7.8/10 (Proficient in 2 weeks)
‚îú‚îÄ Adoption: 9.0/10 (Uses all major features)
‚îî‚îÄ Code Health: 8.2/10 (Complexity reduced 35%)

Recommendations:
‚Ä¢ Leverage CORTEX planning more (currently 20%, target 40%)
‚Ä¢ Share best practices with team (top performer)
```

#### 3. A/B Testing Framework

**Compare performance with/without CORTEX:**

```python
# cortex-brain/tier3/ab_testing.py

class ABTestingFramework:
    """Compare developer performance with/without CORTEX"""
    
    def run_ab_test(self, developer_id, baseline_weeks=2, test_weeks=2):
        """
        Week 1-2: Baseline (no CORTEX)
        Week 3-4: Test (with CORTEX)
        """
        
        baseline = self._collect_metrics(developer_id, baseline_weeks, cortex=False)
        test = self._collect_metrics(developer_id, test_weeks, cortex=True)
        
        return {
            "velocity_metrics": {
                "time_to_first_commit": {
                    "baseline": baseline["avg_time_to_commit"],
                    "test": test["avg_time_to_commit"],
                    "improvement": self._calculate_improvement(
                        baseline["avg_time_to_commit"],
                        test["avg_time_to_commit"]
                    )
                },
                "commits_per_feature": {
                    "baseline": baseline["commits_per_feature"],
                    "test": test["commits_per_feature"],
                    "improvement": "51% reduction"
                }
            },
            
            "quality_metrics": {
                "test_coverage": {
                    "baseline": baseline["test_coverage"],
                    "test": test["test_coverage"],
                    "improvement": "+19 points"
                },
                "bug_density": {
                    "baseline": baseline["bugs_per_100loc"],
                    "test": test["bugs_per_100loc"],
                    "improvement": "65% reduction"
                }
            },
            
            "conclusion": "CORTEX improved productivity by 2.4x with 60% better quality"
        }
```

#### 4. Team Performance Leaderboard

**Gamified visualization to encourage best practices:**

```yaml
# cortex-brain/tier3/leaderboard_config.yaml

leaderboard_categories:
  - name: "CORTEX Power User"
    metric: "cortex_effectiveness_score"
    description: "Most effective CORTEX usage"
    icon: "‚ö°"
  
  - name: "Quality Champion"
    metric: "test_pass_rate"
    description: "Highest test pass rate"
    icon: "üèÜ"
  
  - name: "Velocity Leader"
    metric: "avg_time_to_commit"
    description: "Fastest time-to-commit"
    icon: "üöÄ"
  
  - name: "Learning Master"
    metric: "feature_adoption_rate"
    description: "Best CORTEX feature adoption"
    icon: "üéì"
  
  - name: "Code Health Hero"
    metric: "code_stability_score"
    description: "Most stable, maintainable code"
    icon: "üí™"

display_settings:
  refresh_frequency: "daily"
  show_top_n: 5
  anonymize: false  # Show names or "Developer A/B/C"
```

#### 5. Real-Time Performance Alerts

**Proactive notifications when patterns indicate issues:**

```python
# cortex-brain/tier3/performance_alerts.py

class PerformanceAlerts:
    """Notify developers of productivity issues"""
    
    ALERT_TRIGGERS = {
        "high_rework": {
            "condition": "rework_ratio > 0.50",
            "message": "‚ö†Ô∏è High rework detected (>50% commits are fixes)",
            "recommendation": "Use CORTEX planning feature before coding"
        },
        
        "low_test_coverage": {
            "condition": "test_coverage < 0.70 for 3+ commits",
            "message": "‚ö†Ô∏è Test coverage dropping (<70% for 3 commits)",
            "recommendation": "Enable CORTEX test generator"
        },
        
        "slow_velocity": {
            "condition": "time_to_commit > 2x baseline",
            "message": "‚ö†Ô∏è Slower velocity than usual (2x baseline)",
            "recommendation": "Check CORTEX session analytics for optimal times"
        },
        
        "context_switch_fatigue": {
            "condition": "context_switches > 5 in 1 hour",
            "message": "‚ö†Ô∏è High context switching (5+ in 1 hour)",
            "recommendation": "CORTEX recommends 45-min focused sessions"
        },
        
        "hotspot_warning": {
            "condition": "working_on_file with churn_rate > 0.30",
            "message": "‚ö†Ô∏è Editing high-churn file (28% churn rate)",
            "recommendation": "CORTEX suggests extra testing for this file"
        }
    }
```

#### 6. CORTEX ROI Calculator

**Automated return-on-investment calculation:**

```python
# cortex-brain/tier3/roi_calculator.py

class ROICalculator:
    """Calculate CORTEX return on investment"""
    
    def calculate_roi(self, team_size, avg_hourly_rate, weeks=12):
        """
        Calculate quarterly ROI
        """
        
        # Time savings from analytics
        dev_time_saved = self._calculate_time_saved("development")
        debug_time_saved = self._calculate_time_saved("debugging")
        rework_time_saved = self._calculate_time_saved("rework")
        
        total_hours_saved = (
            dev_time_saved + 
            debug_time_saved + 
            rework_time_saved
        ) * team_size * weeks
        
        cost_savings = total_hours_saved * avg_hourly_rate
        
        return {
            "time_savings": {
                "development": f"{dev_time_saved * weeks}h/dev",
                "debugging": f"{debug_time_saved * weeks}h/dev",
                "rework": f"{rework_time_saved * weeks}h/dev",
                "total": f"{total_hours_saved}h team-wide"
            },
            
            "cost_savings": {
                "weekly": f"${cost_savings / weeks:,.2f}",
                "quarterly": f"${cost_savings:,.2f}",
                "annual": f"${cost_savings * 4:,.2f}"
            },
            
            "cortex_investment": "$0 (built in-house)",
            
            "roi_metrics": {
                "break_even": "Immediate",
                "payback_period": "0 weeks",
                "roi_percentage": "‚àû% (zero investment cost)"
            }
        }
```

**Example ROI Report:**
```
CORTEX ROI Report (Q4 2025)

Team Size: 10 developers
Average Hourly Rate: $75/hour

Time Savings:
‚îú‚îÄ Development time saved: 120 hours/week
‚îú‚îÄ Debug time saved: 45 hours/week
‚îú‚îÄ Rework time saved: 30 hours/week
‚îî‚îÄ Total time saved: 195 hours/week

Cost Savings:
‚îú‚îÄ Weekly: $14,625
‚îú‚îÄ Quarterly: $190,125
‚îî‚îÄ Annual: $760,500

CORTEX Investment: $0 (built in-house)

ROI Metrics:
‚îú‚îÄ Break-even: Immediate
‚îú‚îÄ Payback Period: 0 weeks
‚îî‚îÄ ROI: ‚àû% (zero investment)
```

**Database Schema Extensions:**

```sql
-- New tables in tier3/context-intelligence.db

CREATE TABLE developer_metrics (
    developer_id TEXT PRIMARY KEY,
    name TEXT,
    email TEXT,
    team TEXT,
    
    -- CORTEX usage
    cortex_sessions_total INTEGER,
    cortex_usage_hours REAL,
    first_cortex_use DATETIME,
    last_cortex_use DATETIME,
    
    -- Productivity
    commits_with_cortex INTEGER,
    commits_without_cortex INTEGER,
    avg_time_to_commit_with REAL,
    avg_time_to_commit_without REAL,
    
    -- Quality
    test_pass_rate_with REAL,
    test_pass_rate_without REAL,
    rework_ratio_with REAL,
    rework_ratio_without REAL,
    
    -- Impact score
    impact_score REAL,
    last_calculated DATETIME
);

CREATE TABLE ab_test_results (
    test_id TEXT PRIMARY KEY,
    developer_id TEXT,
    start_date DATETIME,
    end_date DATETIME,
    
    baseline_weeks INTEGER,
    test_weeks INTEGER,
    
    velocity_improvement REAL,
    quality_improvement REAL,
    
    conclusion TEXT,
    
    FOREIGN KEY (developer_id) REFERENCES developer_metrics(developer_id)
);

CREATE TABLE performance_alerts (
    alert_id TEXT PRIMARY KEY,
    developer_id TEXT,
    alert_type TEXT,
    triggered_at DATETIME,
    message TEXT,
    recommendation TEXT,
    acknowledged BOOLEAN DEFAULT FALSE,
    
    FOREIGN KEY (developer_id) REFERENCES developer_metrics(developer_id)
);
```

**Why CORTEX 4.0:**
- Requires Tier 3 architectural enhancements (new tables, analytics engine)
- Complex metric calculation algorithms (velocity, quality scoring)
- A/B testing framework needs session isolation (compare CORTEX on/off)
- Breaking changes to Tier 3 API (new methods, new data structures)
- 10-14 week implementation timeline warrants major version

**Benefits:**
- ‚úÖ Quantifiable productivity improvements (2-3x typical)
- ‚úÖ Data-driven CORTEX adoption decisions
- ‚úÖ Identify top performers for recognition/mentorship
- ‚úÖ ROI justification for management/stakeholders
- ‚úÖ Individual developer coaching opportunities
- ‚úÖ Team-wide performance optimization

**Risks & Mitigations:**

| Risk | Mitigation |
|------|------------|
| Privacy concerns (tracking developers) | Opt-in system, anonymization option, transparent metrics |
| "Big Brother" perception | Position as coaching tool, not surveillance |
| Inaccurate metrics | A/B testing validation, manual review |
| Gaming the metrics | Focus on outcome metrics (quality + velocity), not activity |

**Success Criteria:**
- [ ] Dashboard shows real-time developer metrics
- [ ] Impact scoring algorithm validated with 10+ developers
- [ ] A/B testing shows statistically significant improvements
- [ ] ROI calculator matches manual calculations
- [ ] Privacy controls prevent misuse
- [ ] Team adoption >80% after 4 weeks

**Configuration:**

```json
// cortex.config.json (CORTEX 4.0)

{
  "tier3": {
    "developer_analytics": {
      "enabled": true,
      "tracking_mode": "opt_in",  // "opt_in", "opt_out", "mandatory"
      
      "privacy": {
        "anonymize_leaderboard": false,
        "share_with_team": true,
        "share_with_manager": true,
        "export_allowed": false
      },
      
      "alerts": {
        "enabled": true,
        "delivery": ["in_app", "email"],
        "frequency": "daily"
      },
      
      "roi_calculator": {
        "team_size": 10,
        "avg_hourly_rate": 75,
        "reporting_frequency": "weekly"
      }
    }
  }
}
```

**User Experience:**

```
# View your personal metrics
cortex analytics show

# Compare your metrics to team average
cortex analytics compare

# View team leaderboard
cortex analytics leaderboard

# Generate ROI report
cortex analytics roi

# Run A/B test (2 weeks baseline + 2 weeks test)
cortex analytics ab-test --weeks 4
```

**Dependencies:**
- Tier 3 Context Intelligence (existing)
- Git analysis (existing)
- Session analytics (existing)
- New: Performance scoring engine
- New: A/B testing framework
- New: Alert notification system

**Migration Path:**
1. Analytics disabled by default (opt-in)
2. Existing Tier 3 API remains compatible
3. New analytics API available for teams that opt-in
4. Dashboard requires explicit activation

**Related Documents:**
- Tier 3 API: `prompts/shared/technical-reference.md` (lines 682-800)
- Optimization principles: `cortex-brain/documents/analysis/optimization-principles.yaml`
- Session analytics: Already implemented in Tier 3

---

### Feature 3: Nested Agent Coordination (Hierarchical Agents)

**Priority:** üü¢ **LOW**  
**Effort:** 6-8 weeks  
**Status:** Research Phase

**What It Is:**
Support hierarchical agent specialization where "parent" agents delegate to "child" agents for specific sub-tasks.

**Current Limitation:**
CORTEX agents are flat (no hierarchy). Intent Router routes to exactly one agent. Agents cannot delegate to other agents.

**Architectural Change Required:**
```python
# CORTEX 3.0 (Current)
Intent Router
    ‚Üì
Single Agent (Work Planner, Code Executor, etc.)
    ‚Üì
Executes task directly

# CORTEX 4.0 (Future)
Intent Router
    ‚Üì
Parent Agent (e.g., Full-Stack Developer)
    ‚Üì
Delegates to Child Agents:
    ‚îú‚îÄ Frontend Specialist (React UI)
    ‚îú‚îÄ Backend Specialist (API design)
    ‚îú‚îÄ Database Specialist (Schema design)
    ‚îî‚îÄ Test Specialist (E2E testing)
```

**Implementation Concept:**
```yaml
# cortex-brain/agents/full-stack-developer.agent.yaml

agent_definition:
  name: "full-stack-developer"
  type: "coordinator"
  purpose: "Coordinate frontend, backend, and database work"
  
  child_agents:
    - name: "frontend-specialist"
      path: "cortex-brain/agents/frontend-specialist.agent.yaml"
      delegates_when: "Request involves UI components"
    
    - name: "backend-specialist"
      path: "cortex-brain/agents/backend-specialist.agent.yaml"
      delegates_when: "Request involves API endpoints"
    
    - name: "database-specialist"
      path: "cortex-brain/agents/database-specialist.agent.yaml"
      delegates_when: "Request involves database schema"
    
    - name: "test-specialist"
      path: "cortex-brain/agents/test-specialist.agent.yaml"
      delegates_when: "Request involves testing"
  
  delegation_strategy:
    type: "sequential"  # or "parallel"
    order:
      - "database-specialist"  # Schema first
      - "backend-specialist"   # API second
      - "frontend-specialist"  # UI third
      - "test-specialist"      # Tests last
  
  coordination:
    shares_context: true  # Child agents see parent context
    aggregates_results: true  # Parent collects child outputs
```

**Why CORTEX 4.0:**
- Requires complete redesign of agent coordination
- Corpus Callosum needs hierarchical message routing
- Context management across agent hierarchies
- Cycle detection (prevent infinite delegation)
- Breaking change (agent API changes)

**Benefits:**
- ‚úÖ Specialized agents for sub-domains
- ‚úÖ Better separation of concerns
- ‚úÖ Reusable child agents across parent agents

**Risks:**
- ‚ö†Ô∏è Complexity explosion (too many agents)
- ‚ö†Ô∏è Performance overhead (delegation latency)
- ‚ö†Ô∏è Debugging difficulty (which agent did what?)

---

### Feature 4: Session-Specific Agent State

**Priority:** üü¢ **LOW**  
**Effort:** 4-6 weeks  
**Status:** Research Phase

**What It Is:**
Support chatmode-style session-scoped agent behavior that resets after session ends, separate from persistent agent state in Tier 1/2.

**Current Limitation:**
CORTEX agents persist state across sessions (Tier 1/2 memory). Chatmodes are session-only (no persistence).

**Why CORTEX 4.0:**
- Requires new session management layer
- Distinction between session state and persistent state
- Memory lifecycle changes

---

## CORTEX 4.0 Summary

**Total Effort:** 40-52 weeks (10-13 months)  
**Features:** 5 major architectural changes  
**Backward Compatibility:** Breaking changes (migration provided)

### Implementation Priority Matrix

| Feature | Effort | Impact | Priority | Quarter | Status |
|---------|--------|--------|----------|---------|--------|
| Dynamic Agent Composition | 6-8 weeks | High | üî¥ High | Q3 2025 | Research |
| Pluggable Knowledge Sources | 8-10 weeks | Medium | üü° Medium | Q3-Q4 2025 | Research |
| **MCP Integration** | **16-20 weeks** | **High** | **üî¥ High** | **Q3 2026** | **‚úÖ Planning Complete** |
| Nested Agent Coordination | 6-8 weeks | Low | üü¢ Low | Q4 2025 | Research |
| Session-Specific Agent State | 4-6 weeks | Low | üü¢ Low | Q4 2025 | Research |

**Note:** MCP Integration (Feature 2.5) is the concrete implementation of Pluggable Knowledge Sources (Feature 2). MCP provides the standard protocol for external knowledge access.

### Development Phases

**Phase 1: Research & Design (Weeks 1-8)**
- Dynamic agent engine design
- Hybrid knowledge architecture design
- **MCP host architecture design** ‚úÖ (Planning complete)
- Migration planning
- Community feedback

**Phase 2: Core Implementation (Weeks 9-20)**
- Dynamic agent engine
- Pluggable knowledge sources
- **MCP host infrastructure (Weeks 9-12)**
- Integration testing

**Phase 3: MCP Deep Integration (Weeks 21-36)**
- **Pylance deep integration (Weeks 21-24)**
- **MSSQL MCP server (Weeks 25-28)**
- **External knowledge sources (Weeks 29-32)**
- **MCP testing & polish (Weeks 33-36)**

**Phase 4: Advanced Features (Weeks 37-44)**
- Nested agent coordination
- Session state management
- Performance optimization

**Phase 5: Migration & Release (Weeks 45-52)**
- Migration tools
- Breaking change documentation
- CORTEX 4.0.0 release

---

## Migration Strategy: 3.0 ‚Üí 4.0

### Breaking Changes

**Agent System:**
- Hardcoded agents removed ‚Üí YAML-defined agents
- Agent API changed ‚Üí New DynamicAgent base class
- Intent routing redesigned ‚Üí Supports dynamic agents

**Knowledge Graph:**
- Local-only ‚Üí Hybrid local + external sources
- Tier 2 API extended ‚Üí New query methods
- Configuration format changed ‚Üí New external_sources section

**Agent Coordination:**
- Flat agent system ‚Üí Hierarchical agent support
- Corpus Callosum redesigned ‚Üí Nested message routing

### Migration Tools

**1. Agent Conversion Tool**
```bash
cortex migrate-agents --from-version 3.0 --to-version 4.0

Output:
  ‚úÖ Converted work-planner ‚Üí work-planner.agent.yaml
  ‚úÖ Converted code-executor ‚Üí code-executor.agent.yaml
  ‚úÖ Converted brain-protector ‚Üí brain-protector.agent.yaml
  ...
  ‚úÖ 10 agents converted successfully
  
  Manual review required:
    - Custom agent logic may need adjustment
    - See migration guide: docs/migration/3.0-to-4.0.md
```

**2. Configuration Migrator**
```bash
cortex migrate-config --from-version 3.0 --to-version 4.0

Output:
  ‚úÖ Updated cortex.config.json
  ‚úÖ Added external_sources section (all disabled)
  ‚úÖ Preserved existing settings
  
  Review changes:
    - External sources disabled by default
    - Opt-in via external_sources.mcp_enabled = true
```

**3. Tier 2 Schema Upgrade**
```bash
cortex upgrade-tier2 --from-version 3.0 --to-version 4.0

Output:
  ‚úÖ Backed up tier2/knowledge-graph.db
  ‚úÖ Added cache table for external sources
  ‚úÖ Added source column to patterns table
  ‚úÖ Updated indexes
  
  Database upgraded: tier2/knowledge-graph.db (v4.0)
```

### Deprecation Timeline

**CORTEX 4.0.0 (Q3 2025):**
- Hardcoded agents: Deprecated (still work, warnings shown)
- Local-only knowledge: Deprecated (still default, hybrid available)
- Flat agent system: Deprecated (hierarchical agents optional)

**CORTEX 4.1.0 (Q4 2025):**
- Hardcoded agents: Removed (migration required)
- Local-only knowledge: Still supported (hybrid default)
- Flat agent system: Still supported (hierarchical default)

**CORTEX 4.2.0 (Q1 2026):**
- All CORTEX 3.0 APIs removed
- Full CORTEX 4.0 architecture mandatory

---

## Decision Matrix: When to Use 3.0 vs 4.0 Features

### Use CORTEX 3.0 Features When:
- ‚úÖ Enhancement fits within existing modular architecture
- ‚úÖ No external dependencies introduced
- ‚úÖ Backward compatibility maintained
- ‚úÖ Additive change (new files, not modified core)
- ‚úÖ User-facing improvement
- ‚úÖ Low implementation risk

**Examples:**
- Meta-template documentation
- Explicit agent selection
- User instruction system
- Technology-specific prompts

### Use CORTEX 4.0 Features When:
- ‚ùå Requires fundamental architectural redesign
- ‚ùå Introduces external dependencies (even optional)
- ‚ùå Breaking changes to agent system or knowledge graph
- ‚ùå Core API changes
- ‚ùå Runtime behavior changes (agent lifecycle, memory)
- ‚ùå High implementation complexity

**Examples:**
- Dynamic agent composition
- MCP server integration
- Nested agent coordination
- Session-specific state management

---

## Recommended Path Forward

### Immediate (Q1 2025):
1. ‚úÖ Implement Meta-Template Documentation (1-2 weeks)
2. ‚úÖ Implement Confidence-Based Workflows (1 week)
3. ‚úÖ Release CORTEX 3.0.1 (incremental update)

### Short-Term (Q1-Q2 2025):
1. ‚úÖ Implement Explicit Agent Selection (2-3 weeks)
2. ‚úÖ Implement User Instruction System (2 weeks)
3. ‚úÖ Build Technology-Specific Prompt Library (2 weeks)
4. ‚úÖ Release CORTEX 3.1.0 (major feature update)

### Long-Term (Q3 2025 - Q3 2026):
1. ‚è∏Ô∏è Research Dynamic Agent Composition (Q3 2025)
2. ‚è∏Ô∏è Research Pluggable Knowledge Sources (Q3 2025)
3. ‚úÖ **MCP Integration Planning Complete** (Q4 2025)
4. ‚è∏Ô∏è Implement Developer Performance Analytics System (Q1 2026)
5. ‚è∏Ô∏è Implement MCP Host Infrastructure (Q1 2026)
6. ‚è∏Ô∏è Implement Pylance Deep Integration (Q1 2026)
7. ‚è∏Ô∏è Implement MSSQL & External Knowledge MCP Servers (Q2 2026)
8. ‚è∏Ô∏è Implement CORTEX 4.0 Core (Q2-Q3 2026)
9. ‚è∏Ô∏è Beta testing and migration tooling (Q3 2026)
10. ‚è∏Ô∏è Release CORTEX 4.0.0 (Q3 2026)

**Target Release Date:** CORTEX 4.0.0 with MCP Integration + Developer Analytics - Q3 2026

---

## Conclusion

**CORTEX 3.0** focuses on enhancing the existing modular template architecture with user-friendly features that require no breaking changes. The 8-13 week implementation timeline delivers significant value through meta-templates, explicit agent control, domain-specific instructions, and technology-specific prompts.

**CORTEX 4.0** represents a fundamental architectural evolution enabling dynamic agent creation, hybrid knowledge sources, and hierarchical agent coordination. The 24-32 week timeline reflects the complexity of redesigning core systems while maintaining migration paths for existing users.

**Recommended Strategy:** Ship CORTEX 3.0 enhancements in Q1-Q2 2025 to deliver immediate value, then invest in CORTEX 4.0 architectural work in Q3-Q4 2025 for long-term platform evolution.

---

**Document Version:** 1.0  
**Planning Date:** 2025-01-18  
**Roadmap Owner:** CORTEX Work Planner Agent  
**Next Review:** Q2 2025 (after CORTEX 3.1 release)
