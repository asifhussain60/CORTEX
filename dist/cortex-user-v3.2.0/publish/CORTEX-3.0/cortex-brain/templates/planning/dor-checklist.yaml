# CORTEX Definition of Ready (DoR) Checklist
# Enhanced ambiguity detection and self-audit framework
# Author: Asif Hussain | ¬© 2024-2025 | github.com/asifhussain60/CORTEX

version: "1.0.0"
created: "2025-11-19"
purpose: "Detect ambiguous requirements and guide users toward clarity before development"

# Ambiguity Detection Rules
ambiguity_detection:
  vague_terms:
    - term: "improve"
      challenge: "Improve by how much? What specific metric?"
      example: "Improve performance ‚Üí Reduce page load time from 3s to <1s"
    
    - term: "enhance"
      challenge: "What specific capability are you adding?"
      example: "Enhance UI ‚Üí Add dark mode toggle with user preference storage"
    
    - term: "better"
      challenge: "Better than what? By what measure?"
      example: "Better error handling ‚Üí Display user-friendly messages with actionable next steps"
    
    - term: "optimize"
      challenge: "Optimize what specifically? What's the target?"
      example: "Optimize queries ‚Üí Reduce database query time from 500ms to <100ms"
    
    - term: "update"
      challenge: "What exactly needs updating? What changes?"
      example: "Update API ‚Üí Add pagination support with 50 items per page"
    
    - term: "fix"
      challenge: "What is broken? What should it do instead?"
      example: "Fix login ‚Üí Resolve null reference error when username is empty"
  
  missing_context:
    - question: "Who is the user?"
      rationale: "Features need clear target users (admin, customer, guest)"
      example: "User = authenticated customers with active subscriptions"
    
    - question: "What does success look like?"
      rationale: "Need measurable outcomes, not vague improvements"
      example: "Success = 95% of users complete checkout in <2 minutes"
    
    - question: "What problem does this solve?"
      rationale: "Features should address specific pain points"
      example: "Problem = Users abandon checkout due to confusing shipping options"
    
    - question: "What data is involved?"
      rationale: "Identify inputs, outputs, transformations, storage"
      example: "Data = User profile (name, email, prefs), stored in PostgreSQL users table"
    
    - question: "What are the boundaries?"
      rationale: "Define scope limits (what's IN scope vs OUT scope)"
      example: "IN: Password reset via email | OUT: SMS-based reset (future phase)"
  
  undefined_terms:
    - pattern: "the system"
      challenge: "Which system specifically? Which component/service?"
      example: "The system ‚Üí The authentication service (AuthService.cs)"
    
    - pattern: "the API"
      challenge: "Which API? Which endpoint?"
      example: "The API ‚Üí POST /api/v1/auth/login endpoint"
    
    - pattern: "the database"
      challenge: "Which database? Which table/collection?"
      example: "The database ‚Üí PostgreSQL production DB, users table"
    
    - pattern: "the user"
      challenge: "Which user role/type?"
      example: "The user ‚Üí Authenticated customer users (not admins or guests)"
    
    - pattern: "it"
      challenge: "What does 'it' refer to? Be explicit."
      example: "It ‚Üí The user session token stored in Redis"

# Self-Audit Prompts (5+ questions per feature)
self_audit_prompts:
  edge_cases:
    - "What happens if the input is empty?"
    - "What happens if the input exceeds maximum size?"
    - "What happens if the user is not authenticated?"
    - "What happens if the database is unavailable?"
    - "What happens if a third-party API times out?"
    - "What happens if two users modify the same record simultaneously?"
  
  access_control:
    - "Who can access this feature?"
    - "What permissions are required?"
    - "Can guests/unauthenticated users access this?"
    - "What happens if an unauthorized user tries to access?"
    - "Are there different access levels (read/write/admin)?"
  
  data_risk:
    - "What data is at risk if this feature is compromised?"
    - "Is sensitive data (PII, passwords, tokens) involved?"
    - "How is data validated before storage?"
    - "Is data encrypted in transit and at rest?"
    - "What happens if data validation fails?"
  
  testing:
    - "How do we test this feature?"
    - "What are the happy path test cases?"
    - "What are the error/edge case test cases?"
    - "Can this be tested automatically?"
    - "What test data is needed?"
  
  dependencies:
    - "What other systems/services does this depend on?"
    - "What happens if a dependency is unavailable?"
    - "Are there third-party APIs involved?"
    - "What database tables/collections are accessed?"
    - "Are there any external file systems or storage services?"

# Mandatory Planning Sections
mandatory_sections:
  acceptance_criteria:
    description: "Testable, specific success criteria"
    requirements:
      - "Use GIVEN-WHEN-THEN format for clarity"
      - "Include happy path scenarios"
      - "Include error/edge case scenarios"
      - "Define measurable outcomes (not subjective)"
      - "Cover all user roles/types"
    example: |
      GIVEN an authenticated user with valid credentials
      WHEN they submit the login form
      THEN they are redirected to dashboard within 2 seconds
      AND a session token is created with 24-hour expiry
      AND login attempt is logged to audit table
  
  risk_analysis:
    description: "Identify technical, security, and UX risks"
    categories:
      technical:
        - "Performance bottlenecks (database queries, API calls)"
        - "Scalability limits (concurrent users, data volume)"
        - "Integration failures (third-party APIs, services)"
        - "Data corruption or loss scenarios"
      security:
        - "Authentication/authorization bypass risks"
        - "Injection attacks (SQL, XSS, CSRF)"
        - "Sensitive data exposure"
        - "Denial of service vulnerabilities"
      ux:
        - "Confusing user flows or error messages"
        - "Accessibility barriers (screen readers, keyboard nav)"
        - "Mobile responsiveness issues"
        - "Performance perceived by users (loading times)"
    example: |
      TECHNICAL RISK: Database query may timeout with >10k users
      Mitigation: Add pagination (50 users per page), implement caching
      
      SECURITY RISK: Password reset tokens could be predictable
      Mitigation: Use cryptographically secure random token generation
      
      UX RISK: Error messages expose too much system detail
      Mitigation: Generic user-facing errors, detailed logs for debugging
  
  security_requirements:
    description: "Map security needs to OWASP Top 10 categories"
    owasp_categories:
      A01_broken_access_control:
        - "Authentication required?"
        - "Authorization checks present?"
        - "Role-based access control (RBAC) implemented?"
      A02_cryptographic_failures:
        - "Sensitive data encrypted at rest?"
        - "HTTPS enforced for data in transit?"
        - "No hardcoded secrets or API keys?"
      A03_injection:
        - "Parameterized queries used?"
        - "Input validation on all user inputs?"
        - "Output encoding to prevent XSS?"
      A04_insecure_design:
        - "Threat modeling completed?"
        - "Security requirements defined early?"
        - "Secure defaults configured?"
      A05_security_misconfiguration:
        - "Default passwords changed?"
        - "Unnecessary features disabled?"
        - "Error messages don't expose system details?"
      A06_vulnerable_components:
        - "Dependencies scanned for CVEs?"
        - "All packages up to date?"
        - "SBOM (Software Bill of Materials) documented?"
      A07_identification_auth_failures:
        - "Multi-factor authentication available?"
        - "Account lockout after failed attempts?"
        - "Session timeout configured?"
      A08_software_data_integrity:
        - "Code signing implemented?"
        - "CI/CD pipeline secured?"
        - "Dependencies verified via checksums?"
      A09_security_logging_monitoring:
        - "Security events logged?"
        - "Logs monitored for anomalies?"
        - "Incident response plan defined?"
      A10_server_side_request_forgery:
        - "User-supplied URLs validated?"
        - "Network access restricted?"
        - "Allowlist for external requests?"
    auto_suggest:
      # Auto-map feature keywords to relevant OWASP categories
      authentication: ["A01", "A02", "A07"]
      api: ["A03", "A01", "A05"]
      data_storage: ["A02", "A08", "A09"]
      file_upload: ["A03", "A08", "A10"]
      payment: ["A02", "A04", "A09"]
  
  definition_of_done:
    description: "Complete checklist before declaring work done"
    checklist:
      code:
        - "Code implements all acceptance criteria"
        - "Code follows clean code principles (no unused code, clear naming)"
        - "Complexity within thresholds (see quality tiers)"
        - "No commented-out code (except TODO/FIXME with tickets)"
      tests:
        - "All tests passing (unit + integration)"
        - "Test coverage meets quality tier target (80-90%)"
        - "Edge cases covered with tests"
        - "Security test cases included"
      documentation:
        - "API endpoints documented (if applicable)"
        - "Configuration changes documented"
        - "README updated with new features"
        - "Inline code comments for complex logic"
      security:
        - "No HIGH/CRITICAL security vulnerabilities (bandit, pip-audit)"
        - "No hardcoded secrets (detect-secrets scan passed)"
        - "OWASP checklist reviewed"
      quality_gates:
        - "Pre-commit hooks passing (clean code gates)"
        - "CI/CD pipeline green"
        - "Code review approved"
        - "Performance validated (no regressions)"

# DoR Validation Workflow
validation_workflow:
  step_1_scan_for_ambiguity:
    action: "Analyze user request for vague terms, missing context, undefined references"
    output: "List of ambiguous items with specific challenges"
    example: |
      ‚ùå Ambiguity Detected (Line 3): "improve performance"
      Challenge: Improve by how much? What specific metric?
      Suggestion: Define target metric (e.g., page load <1s, query time <100ms)
  
  step_2_prompt_self_audit:
    action: "Ask 5+ clarifying questions from self_audit_prompts"
    output: "List of questions requiring user answers"
    example: |
      üìù Self-Audit Questions:
      1. What happens if the input is empty?
      2. Who can access this feature?
      3. What data is at risk if compromised?
      4. How do we test this?
      5. What dependencies does this have?
  
  step_3_check_mandatory_sections:
    action: "Verify all mandatory sections are addressed"
    output: "Checklist with ‚úÖ complete or ‚ùå missing"
    example: |
      ‚òê Acceptance Criteria (GIVEN-WHEN-THEN format)
      ‚òê Risk Analysis (Technical, Security, UX)
      ‚òê Security Requirements (OWASP categories)
      ‚òê Definition of Done (Code, Tests, Docs, Security)
  
  step_4_dor_approval:
    action: "User reviews and approves DoR, or iterates on clarifications"
    output: "DoR status: APPROVED or INCOMPLETE"
    example: |
      ‚úÖ DoR APPROVED
      All ambiguities resolved, self-audit complete, mandatory sections present.
      Ready to hand off to Development Executor.

# Exit Criteria (before moving to development)
exit_criteria:
  planning_complete:
    - "DoR checklist approved"
    - "Acceptance criteria defined (testable, measurable)"
    - "Risk analysis completed (technical, security, UX)"
    - "Security requirements mapped to OWASP categories"
    - "Definition of Done agreed upon"
  
  user_approval:
    - "User reviews planning artifacts"
    - "User approves plan OR requests clarifications"
    - "No blocking ambiguities remain"
  
  handoff_ready:
    - "If implementation requested: Hand off to Development Executor"
    - "If planning-only: Save artifacts, close planning session"
    - "All planning artifacts stored in cortex-brain/documents/planning/features/"

# Integration with Response Templates
response_template_enhancements:
  work_planner_success:
    add_section: "self_audit_questions"
    content: |
      üìù Self-Audit Questions (answer before proceeding):
      {{self_audit_prompts}}
      
      Once answered, I'll generate the complete plan.
  
  planning_dor_incomplete:
    add_section: "line_level_feedback"
    content: |
      ‚ö†Ô∏è DoR Incomplete - Ambiguities Detected:
      
      {{ambiguity_list_with_line_numbers}}
      
      Please clarify these items before proceeding.
  
  planning_security_review:
    new_template: true
    content: |
      üîí Security Review (OWASP Category Checklist):
      
      Based on your feature ({{feature_type}}), these OWASP categories apply:
      {{auto_suggested_owasp_categories}}
      
      Review checklist:
      {{owasp_checklist_for_categories}}
      
      Any HIGH/CRITICAL concerns? Address before development.

# Quality Assurance
qa_notes:
  - "DoR enforcement happens BEFORE Development Executor is invoked"
  - "No development logic (TDD, refactoring) in planning phase"
  - "Self-audit prompts should be conversational, not accusatory"
  - "OWASP auto-suggest helps users without security expertise"
  - "Line-level feedback pinpoints exact location of ambiguity"
