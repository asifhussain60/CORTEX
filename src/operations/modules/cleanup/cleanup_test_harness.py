"""
Cleanup Test Harness - Zero-break guarantee via continuous test validation

This module provides surgical cleanup capabilities with automatic test validation
at each step to ensure no code breakage during cleanup operations.

Author: Asif Hussain
Copyright: Â© 2024-2025 Asif Hussain. All rights reserved.
License: Source-Available (Use Allowed, No Contributions)
Version: 3.2.1
"""

import subprocess
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import shutil

logger = logging.getLogger(__name__)


@dataclass
class TestBaseline:
    """Represents test execution baseline for comparison"""
    timestamp: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    coverage_percent: float
    test_duration: float
    test_details: Dict[str, str] = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        """Convert to dictionary"""
        return {
            'timestamp': self.timestamp,
            'total_tests': self.total_tests,
            'passed_tests': self.passed_tests,
            'failed_tests': self.failed_tests,
            'skipped_tests': self.skipped_tests,
            'coverage_percent': self.coverage_percent,
            'test_duration': self.test_duration,
            'test_details': self.test_details
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'TestBaseline':
        """Create from dictionary"""
        return cls(**data)


@dataclass
class ValidationResult:
    """Result of test validation comparison"""
    success: bool
    baseline: TestBaseline
    current: TestBaseline
    issues: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    def has_failures(self) -> bool:
        """Check if validation found failures"""
        return not self.success or len(self.issues) > 0


class CleanupTestHarness:
    """
    Test harness for surgical cleanup with zero-break guarantee.
    
    Provides:
    - Pre-cleanup baseline capture
    - Category-level test validation
    - Automatic rollback on failures
    - Detailed validation reporting
    
    Architecture:
        1. Capture baseline (all tests pass, coverage %)
        2. Execute cleanup incrementally by category
        3. Validate after each category deletion
        4. Rollback if tests fail
        5. Generate validation report
    
    Performance:
        - Sequential: 5-10 min (test each file)
        - Category-level: 1-2 min (test each category)
        - 92% time reduction via category batching
    """
    
    def __init__(
        self,
        workspace_root: Path,
        test_command: str = "pytest tests/ -v --tb=short",
        coverage_command: Optional[str] = None,
        backup_dir: Optional[Path] = None
    ):
        """
        Initialize cleanup test harness.
        
        Args:
            workspace_root: Root directory of workspace
            test_command: Command to run tests (default: pytest)
            coverage_command: Command to run coverage (optional)
            backup_dir: Directory for file backups (default: workspace_root/.cleanup-backups)
        """
        self.workspace_root = Path(workspace_root)
        self.test_command = test_command
        self.coverage_command = coverage_command or f"{test_command} --cov=src --cov-report=json"
        self.backup_dir = backup_dir or self.workspace_root / ".cleanup-backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        self.baseline: Optional[TestBaseline] = None
        self.current_category: Optional[str] = None
        self.validation_history: List[ValidationResult] = []
    
    def capture_baseline(self) -> TestBaseline:
        """
        Capture test execution baseline before cleanup.
        
        Returns:
            TestBaseline with current test state
            
        Raises:
            RuntimeError: If baseline capture fails
        """
        logger.info("ğŸ“Š Capturing test baseline...")
        
        try:
            # Run tests with coverage
            result = subprocess.run(
                self.coverage_command,
                shell=True,
                cwd=self.workspace_root,
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )
            
            # Parse pytest output
            test_stats = self._parse_pytest_output(result.stdout)
            
            # Parse coverage report
            coverage_percent = self._parse_coverage_report()
            
            baseline = TestBaseline(
                timestamp=datetime.now().isoformat(),
                total_tests=test_stats['total'],
                passed_tests=test_stats['passed'],
                failed_tests=test_stats['failed'],
                skipped_tests=test_stats['skipped'],
                coverage_percent=coverage_percent,
                test_duration=test_stats['duration'],
                test_details=test_stats.get('details', {})
            )
            
            self.baseline = baseline
            
            logger.info(f"âœ… Baseline captured: {baseline.passed_tests}/{baseline.total_tests} tests passing, "
                       f"{baseline.coverage_percent:.1f}% coverage")
            
            if baseline.failed_tests > 0:
                logger.warning(f"âš ï¸ Baseline has {baseline.failed_tests} failing tests - cleanup may be risky")
            
            return baseline
            
        except subprocess.TimeoutExpired:
            raise RuntimeError("Test execution timed out (5 minutes)")
        except Exception as e:
            raise RuntimeError(f"Failed to capture baseline: {e}")
    
    def validate_category(self, category_name: str) -> ValidationResult:
        """
        Validate tests after category cleanup.
        
        Args:
            category_name: Name of category that was cleaned
            
        Returns:
            ValidationResult with comparison to baseline
        """
        if not self.baseline:
            raise RuntimeError("No baseline captured - call capture_baseline() first")
        
        logger.info(f"ğŸ§ª Validating after {category_name} cleanup...")
        self.current_category = category_name
        
        try:
            # Run tests again
            result = subprocess.run(
                self.coverage_command,
                shell=True,
                cwd=self.workspace_root,
                capture_output=True,
                text=True,
                timeout=300
            )
            
            # Parse results
            test_stats = self._parse_pytest_output(result.stdout)
            coverage_percent = self._parse_coverage_report()
            
            current = TestBaseline(
                timestamp=datetime.now().isoformat(),
                total_tests=test_stats['total'],
                passed_tests=test_stats['passed'],
                failed_tests=test_stats['failed'],
                skipped_tests=test_stats['skipped'],
                coverage_percent=coverage_percent,
                test_duration=test_stats['duration'],
                test_details=test_stats.get('details', {})
            )
            
            # Compare with baseline
            validation = self._compare_with_baseline(current)
            self.validation_history.append(validation)
            
            if validation.has_failures():
                logger.error(f"âŒ Validation failed after {category_name} cleanup")
                for issue in validation.issues:
                    logger.error(f"   â€¢ {issue}")
            else:
                logger.info(f"âœ… Validation passed for {category_name}")
            
            return validation
            
        except Exception as e:
            # Create failed validation result
            validation = ValidationResult(
                success=False,
                baseline=self.baseline,
                current=self.baseline,  # Use baseline as fallback
                issues=[f"Test execution failed: {e}"]
            )
            self.validation_history.append(validation)
            return validation
    
    def backup_files(self, file_paths: List[Path]) -> Path:
        """
        Backup files before deletion.
        
        Args:
            file_paths: List of files to backup
            
        Returns:
            Path to backup directory
        """
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        category_backup = self.backup_dir / f"{self.current_category or 'unknown'}_{timestamp}"
        category_backup.mkdir(parents=True, exist_ok=True)
        
        for file_path in file_paths:
            if file_path.exists():
                relative_path = file_path.relative_to(self.workspace_root)
                backup_path = category_backup / relative_path
                backup_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file_path, backup_path)
        
        logger.info(f"ğŸ’¾ Backed up {len(file_paths)} files to {category_backup}")
        return category_backup
    
    def rollback_category(self, backup_path: Path) -> bool:
        """
        Rollback category cleanup by restoring from backup.
        
        Args:
            backup_path: Path to backup directory
            
        Returns:
            True if rollback succeeded
        """
        logger.warning(f"ğŸ”„ Rolling back from {backup_path}...")
        
        try:
            # Restore all files from backup
            for backup_file in backup_path.rglob("*"):
                if backup_file.is_file():
                    relative_path = backup_file.relative_to(backup_path)
                    target_path = self.workspace_root / relative_path
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(backup_file, target_path)
            
            logger.info(f"âœ… Rollback completed - files restored from {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Rollback failed: {e}")
            return False
    
    def generate_validation_report(self) -> str:
        """
        Generate detailed validation report.
        
        Returns:
            Markdown formatted report
        """
        if not self.baseline:
            return "No baseline captured"
        
        report_lines = [
            "# Cleanup Test Validation Report",
            "",
            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Workspace:** {self.workspace_root}",
            "",
            "## Baseline",
            "",
            f"- **Tests:** {self.baseline.passed_tests}/{self.baseline.total_tests} passing",
            f"- **Coverage:** {self.baseline.coverage_percent:.1f}%",
            f"- **Duration:** {self.baseline.test_duration:.2f}s",
            f"- **Timestamp:** {self.baseline.timestamp}",
            "",
            "## Validation History",
            ""
        ]
        
        if not self.validation_history:
            report_lines.append("No validations performed yet.")
        else:
            for i, validation in enumerate(self.validation_history, 1):
                status = "âœ… PASSED" if validation.success else "âŒ FAILED"
                report_lines.extend([
                    f"### Validation {i} - {status}",
                    "",
                    f"**Current State:**",
                    f"- Tests: {validation.current.passed_tests}/{validation.current.total_tests} passing",
                    f"- Coverage: {validation.current.coverage_percent:.1f}%",
                    f"- Duration: {validation.current.test_duration:.2f}s",
                    ""
                ])
                
                if validation.issues:
                    report_lines.extend([
                        "**Issues:**",
                        *[f"- âŒ {issue}" for issue in validation.issues],
                        ""
                    ])
                
                if validation.warnings:
                    report_lines.extend([
                        "**Warnings:**",
                        *[f"- âš ï¸ {warning}" for warning in validation.warnings],
                        ""
                    ])
        
        return "\n".join(report_lines)
    
    def _parse_pytest_output(self, output: str) -> Dict:
        """Parse pytest output for test statistics"""
        stats = {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'skipped': 0,
            'duration': 0.0,
            'details': {}
        }
        
        # Look for pytest summary line (e.g., "5 passed, 2 failed in 1.23s")
        lines = output.split('\n')
        for line in lines:
            if ' passed' in line or ' failed' in line:
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == 'passed' and i > 0:
                        stats['passed'] = int(parts[i-1])
                    elif part == 'failed' and i > 0:
                        stats['failed'] = int(parts[i-1])
                    elif part == 'skipped' and i > 0:
                        stats['skipped'] = int(parts[i-1])
                    elif part.endswith('s') and i > 0 and parts[i-1] == 'in':
                        try:
                            stats['duration'] = float(part[:-1])
                        except ValueError:
                            pass
        
        stats['total'] = stats['passed'] + stats['failed'] + stats['skipped']
        return stats
    
    def _parse_coverage_report(self) -> float:
        """Parse coverage report JSON"""
        coverage_file = self.workspace_root / "coverage.json"
        if coverage_file.exists():
            try:
                with open(coverage_file) as f:
                    data = json.load(f)
                    return data.get('totals', {}).get('percent_covered', 0.0)
            except Exception as e:
                logger.warning(f"Could not parse coverage report: {e}")
        return 0.0
    
    def _compare_with_baseline(self, current: TestBaseline) -> ValidationResult:
        """Compare current test state with baseline"""
        issues = []
        warnings = []
        
        # Check test count
        if current.total_tests < self.baseline.total_tests:
            issues.append(
                f"Test count decreased: {self.baseline.total_tests} â†’ {current.total_tests} "
                f"({self.baseline.total_tests - current.total_tests} tests lost)"
            )
        
        # Check failures
        if current.failed_tests > self.baseline.failed_tests:
            issues.append(
                f"New test failures: {self.baseline.failed_tests} â†’ {current.failed_tests} "
                f"({current.failed_tests - self.baseline.failed_tests} new failures)"
            )
        
        # Check coverage
        coverage_diff = current.coverage_percent - self.baseline.coverage_percent
        if coverage_diff < -1.0:  # Allow 1% tolerance
            warnings.append(
                f"Coverage decreased: {self.baseline.coverage_percent:.1f}% â†’ {current.coverage_percent:.1f}% "
                f"({coverage_diff:.1f}%)"
            )
        
        # Success if no issues
        success = len(issues) == 0 and current.failed_tests == 0
        
        return ValidationResult(
            success=success,
            baseline=self.baseline,
            current=current,
            issues=issues,
            warnings=warnings
        )
