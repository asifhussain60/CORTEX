"""
Policy Test Generator - Pytest Generation from Policy Rules

Purpose: Automatically generate pytest test cases from policy documents.
         Enables continuous compliance validation in CI/CD pipelines.

Features:
- Generate pytest test functions from policy rules
- Add @pytest.mark.compliance decorator for filtering
- Support parametrized tests for multi-file validation
- Auto-generate fixtures for codebase and policy loading
- Create runnable test files with proper imports

Author: Asif Hussain
Copyright: Â© 2024-2025 Asif Hussain. All rights reserved.
License: Source-Available (Use Allowed, No Contributions)
Repository: https://github.com/asifhussain60/CORTEX
"""

import os
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

try:
    from .policy_analyzer import PolicyDocument, PolicyRule, PolicyLevel, PolicyCategory
    from .compliance_validator import ComplianceValidator
except ImportError:
    # For standalone execution
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    from src.policy.policy_analyzer import PolicyDocument, PolicyRule, PolicyLevel, PolicyCategory
    from src.policy.compliance_validator import ComplianceValidator


class PolicyTestGenerator:
    """
    Generate pytest tests from policy documents.
    
    Creates executable pytest test files that validate code compliance
    against policy rules. Tests can be run in CI/CD pipelines for
    continuous compliance monitoring.
    
    Generated tests include:
    - @pytest.mark.compliance marker for filtering
    - Parametrized tests for multiple files/rules
    - Custom fixtures for policy and codebase loading
    - Detailed assertion messages with remediation hints
    """
    
    def __init__(self):
        """Initialize test generator"""
        self.validator = ComplianceValidator()
    
    def generate_test_file(
        self,
        policy_doc: PolicyDocument,
        output_path: str,
        codebase_path: str = ".",
        test_class_name: str = "TestPolicyCompliance"
    ) -> str:
        """
        Generate a complete pytest test file from policy document.
        
        Args:
            policy_doc: Parsed policy document
            output_path: Path where test file will be written
            codebase_path: Path to codebase being tested
            test_class_name: Name for the test class
        
        Returns:
            Path to generated test file
        """
        print(f"\nðŸ§ª Generating Pytest Tests from Policy")
        print(f"Policy: {policy_doc.title or 'Untitled'}")
        print(f"Output: {output_path}")
        print(f"Rules: {len(policy_doc.rules)}\n")
        
        # Build test file content
        content = self._build_test_file_content(
            policy_doc,
            codebase_path,
            test_class_name
        )
        
        # Write to file
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… Generated {len(policy_doc.rules)} test functions")
        print(f"ðŸ“ Test file: {output_path}\n")
        
        return str(output_path)
    
    def _build_test_file_content(
        self,
        policy_doc: PolicyDocument,
        codebase_path: str,
        test_class_name: str
    ) -> str:
        """Build complete test file content"""
        
        # File header
        header = self._generate_header(policy_doc)
        
        # Imports
        imports = self._generate_imports()
        
        # Fixtures
        fixtures = self._generate_fixtures(policy_doc, codebase_path)
        
        # Test class
        test_class = self._generate_test_class(policy_doc, test_class_name)
        
        # Combine all sections
        content = f"{header}\n\n{imports}\n\n{fixtures}\n\n{test_class}\n"
        
        return content
    
    def _generate_header(self, policy_doc: PolicyDocument) -> str:
        """Generate file header with metadata"""
        return f'''"""
Compliance Tests - Auto-generated from Policy

Policy: {policy_doc.title or 'Untitled'}
Version: {policy_doc.version or 'N/A'}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Rules: {len(policy_doc.rules)}

This file was automatically generated by CORTEX PolicyTestGenerator.
DO NOT EDIT MANUALLY - regenerate from policy document instead.

Run tests:
    pytest {Path().name} -v
    pytest {Path().name} -v -m compliance
    pytest {Path().name} -v -m "compliance and security"

Author: CORTEX Policy System
"""'''
    
    def _generate_imports(self) -> str:
        """Generate import statements"""
        return '''import pytest
import sys
from pathlib import Path

# Add CORTEX to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.policy import PolicyAnalyzer, ComplianceValidator, PolicyLevel'''
    
    def _generate_fixtures(self, policy_doc: PolicyDocument, codebase_path: str) -> str:
        """Generate pytest fixtures"""
        return f'''@pytest.fixture(scope="module")
def policy_document():
    """Load and parse policy document"""
    analyzer = PolicyAnalyzer()
    return analyzer.analyze_file("{policy_doc.file_path}")


@pytest.fixture(scope="module")
def codebase_path():
    """Path to codebase being tested"""
    return "{codebase_path}"


@pytest.fixture(scope="module")
def validator():
    """Compliance validator instance"""
    return ComplianceValidator()


@pytest.fixture(scope="module")
def compliance_report(policy_document, codebase_path, validator):
    """Full compliance report (cached for all tests)"""
    return validator.validate(policy_document, codebase_path)'''
    
    def _generate_test_class(self, policy_doc: PolicyDocument, class_name: str) -> str:
        """Generate test class with all test methods"""
        
        # Class header
        class_header = f'''class {class_name}:
    """
    Compliance tests for: {policy_doc.title or 'Untitled Policy'}
    
    Tests are organized by policy category and severity.
    Each test validates one policy rule against the codebase.
    """
'''
        
        # Generate test methods for each rule
        test_methods = []
        for rule in policy_doc.rules:
            test_method = self._generate_test_method(rule)
            test_methods.append(test_method)
        
        # Summary test
        summary_test = self._generate_summary_test()
        
        # Combine class
        class_content = class_header + "\n".join(test_methods) + "\n" + summary_test
        
        return class_content
    
    def _generate_test_method(self, rule: PolicyRule) -> str:
        """Generate a test method for a specific rule"""
        
        # Sanitize rule ID for function name
        func_name = f"test_{rule.id.lower().replace('-', '_')}"
        
        # Determine severity marker
        severity = self._get_severity_marker(rule.level)
        
        # Generate test function
        test_func = f'''
    @pytest.mark.compliance
    @pytest.mark.{rule.category.value}
    @pytest.mark.{severity}
    def {func_name}(self, policy_document, compliance_report):
        """
        {rule.text}
        
        Level: {rule.level.value}
        Category: {rule.category.value}
        """
        # Find violations for this rule
        violations = [
            v for v in compliance_report.violations
            if v.rule_id == "{rule.id}"
        ]
        
        # Build detailed error message
        if violations:
            error_msg = f"Policy violation: {rule.text}\\n\\n"
            error_msg += f"Found {{len(violations)}} violation(s):\\n"
            
            for i, violation in enumerate(violations[:5], 1):  # Show first 5
                error_msg += f"\\n{{i}}. {{violation.file_path}}"
                if violation.line_number:
                    error_msg += f":{{violation.line_number}}"
                error_msg += f"\\n   {{violation.violation_details}}"
            
            if len(violations) > 5:
                error_msg += f"\\n\\n... and {{len(violations) - 5}} more violations"
            
            # Add remediation hints
            actions = [
                a for a in compliance_report.remediation_actions
                if a.rule_id == "{rule.id}"
            ]
            if actions:
                error_msg += "\\n\\nRecommended Actions:"
                for action in actions[:3]:  # Show first 3 actions
                    error_msg += f"\\n  â€¢ {{action.description}}"
                    if action.estimated_effort:
                        error_msg += f" ({{action.estimated_effort}})"
            
            pytest.fail(error_msg)
        
        # Test passes - no violations
        assert len(violations) == 0, "No violations expected"
'''
        
        return test_func
    
    def _generate_summary_test(self) -> str:
        """Generate summary test that shows overall compliance"""
        return '''
    @pytest.mark.compliance
    @pytest.mark.summary
    def test_overall_compliance_score(self, compliance_report):
        """
        Overall compliance score must meet minimum threshold.
        
        This test validates that the codebase achieves at least
        70% compliance with the policy document.
        """
        min_score = 70.0
        actual_score = compliance_report.compliance_score
        
        if actual_score < min_score:
            error_msg = f"Compliance score too low: {actual_score}% < {min_score}%\\n\\n"
            error_msg += f"Summary:\\n"
            error_msg += f"  Total Violations: {len(compliance_report.violations)}\\n"
            error_msg += f"  Critical: {compliance_report.summary.get('by_severity', {}).get('critical', 0)}\\n"
            error_msg += f"  High: {compliance_report.summary.get('by_severity', {}).get('high', 0)}\\n"
            error_msg += f"  Medium: {compliance_report.summary.get('by_severity', {}).get('medium', 0)}\\n"
            error_msg += f"  Low: {compliance_report.summary.get('by_severity', {}).get('low', 0)}\\n\\n"
            error_msg += f"Estimated Effort: {compliance_report.summary.get('estimated_effort_hours', 0)} hours\\n"
            
            pytest.fail(error_msg)
        
        assert actual_score >= min_score, f"Compliance score: {actual_score}%"


    @pytest.mark.compliance
    @pytest.mark.summary
    def test_no_critical_violations(self, compliance_report):
        """
        No critical (MUST/MUST NOT) violations allowed.
        
        Critical violations represent mandatory requirements that
        MUST be satisfied for compliance.
        """
        critical_violations = [
            v for v in compliance_report.violations
            if v.severity == "critical"
        ]
        
        if critical_violations:
            error_msg = f"Found {len(critical_violations)} critical violation(s):\\n\\n"
            
            for i, v in enumerate(critical_violations[:10], 1):
                error_msg += f"{i}. {v.file_path}"
                if v.line_number:
                    error_msg += f":{v.line_number}"
                error_msg += f"\\n   Rule: {v.rule_text}\\n"
                error_msg += f"   Issue: {v.violation_details}\\n\\n"
            
            if len(critical_violations) > 10:
                error_msg += f"... and {len(critical_violations) - 10} more critical violations\\n"
            
            pytest.fail(error_msg)
        
        assert len(critical_violations) == 0, "No critical violations allowed"
'''
    
    def _get_severity_marker(self, level: PolicyLevel) -> str:
        """Get pytest marker for severity level"""
        if level in [PolicyLevel.MUST, PolicyLevel.MUST_NOT]:
            return "critical"
        elif level == PolicyLevel.SHOULD:
            return "high"
        elif level == PolicyLevel.SHOULD_NOT:
            return "medium"
        else:
            return "low"
    
    def generate_conftest(self, output_dir: str) -> str:
        """
        Generate conftest.py for custom pytest configuration.
        
        Args:
            output_dir: Directory where conftest.py will be written
        
        Returns:
            Path to generated conftest.py
        """
        conftest_content = '''"""
Pytest Configuration for Compliance Tests

Defines custom markers and configuration for policy compliance testing.
"""

import pytest


def pytest_configure(config):
    """Register custom markers"""
    config.addinivalue_line(
        "markers", "compliance: mark test as a compliance validation test"
    )
    config.addinivalue_line(
        "markers", "critical: mark test as validating a critical (MUST) requirement"
    )
    config.addinivalue_line(
        "markers", "high: mark test as validating a high priority (SHOULD) requirement"
    )
    config.addinivalue_line(
        "markers", "medium: mark test as validating a medium priority requirement"
    )
    config.addinivalue_line(
        "markers", "low: mark test as validating a low priority requirement"
    )
    config.addinivalue_line(
        "markers", "summary: mark test as a summary/overview test"
    )
    
    # Category markers
    config.addinivalue_line(
        "markers", "security: mark test as validating security requirements"
    )
    config.addinivalue_line(
        "markers", "quality: mark test as validating quality requirements"
    )
    config.addinivalue_line(
        "markers", "performance: mark test as validating performance requirements"
    )
    config.addinivalue_line(
        "markers", "testing: mark test as validating testing requirements"
    )
    config.addinivalue_line(
        "markers", "documentation: mark test as validating documentation requirements"
    )
    config.addinivalue_line(
        "markers", "architecture: mark test as validating architecture requirements"
    )


def pytest_collection_modifyitems(config, items):
    """Modify test collection"""
    # Sort tests: critical first, then high, medium, low, summary last
    priority_order = {"critical": 1, "high": 2, "medium": 3, "low": 4, "summary": 5}
    
    def get_priority(item):
        for marker, priority in priority_order.items():
            if marker in [m.name for m in item.iter_markers()]:
                return priority
        return 6  # Unknown
    
    items.sort(key=get_priority)
'''
        
        output_path = Path(output_dir) / "conftest.py"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(conftest_content)
        
        return str(output_path)


def main():
    """Test policy test generator"""
    try:
        from .policy_analyzer import PolicyAnalyzer
    except ImportError:
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent.parent))
        from src.policy.policy_analyzer import PolicyAnalyzer
    
    # Create sample policy
    sample_policy = """# Code Quality Policy
Version: 1.0
Date: 2025-11-26

## Security Requirements

- Passwords MUST NOT be stored in plain text.
- All user input MUST be validated and sanitized.
- API keys SHOULD NOT be hardcoded in source files.

## Testing Requirements

- Test coverage MUST be greater than 80%.
- Unit tests SHOULD run in less than 5 seconds.

## Documentation Requirements

- Public functions SHOULD have docstrings.
- Classes MUST have docstrings.
"""
    
    # Write to temp file
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
        f.write(sample_policy)
        policy_path = f.name
    
    try:
        # Parse policy
        analyzer = PolicyAnalyzer()
        policy_doc = analyzer.analyze_file(policy_path)
        
        # Generate test file
        generator = PolicyTestGenerator()
        
        # Output to temp directory
        output_dir = Path(tempfile.gettempdir()) / "cortex_policy_tests"
        output_dir.mkdir(exist_ok=True)
        
        test_file = generator.generate_test_file(
            policy_doc,
            output_path=str(output_dir / "test_compliance.py"),
            codebase_path="."
        )
        
        # Generate conftest
        conftest = generator.generate_conftest(str(output_dir))
        
        print(f"âœ… Test Generation Complete!")
        print(f"\nGenerated Files:")
        print(f"  â€¢ {test_file}")
        print(f"  â€¢ {conftest}")
        print(f"\nRun tests with:")
        print(f"  cd {output_dir}")
        print(f"  pytest -v")
        print(f"  pytest -v -m compliance")
        print(f"  pytest -v -m critical")
        print(f"  pytest -v -m security")
    
    finally:
        os.unlink(policy_path)


if __name__ == "__main__":
    main()
