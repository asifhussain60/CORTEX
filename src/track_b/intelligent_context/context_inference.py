"""
CORTEX 3.0 Track B: Context Inference Engine
============================================

Advanced context inference system that builds understanding of development
context from multiple sources and provides intelligent insights.

Key Features:
- Multi-source context fusion
- Semantic understanding of development activity
- Intent prediction and recommendation
- Contextual memory and learning
- Integration with CORTEX brain tiers

Author: Asif Hussain
Copyright: Â© 2024-2025 Asif Hussain. All rights reserved.
License: Proprietary - See LICENSE file for terms
"""

import logging
import json
from collections import defaultdict, deque, Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, field
from enum import Enum


class ContextType(Enum):
    """Types of development context."""
    CURRENT_TASK = "current_task"
    PROJECT_STATE = "project_state"
    DEVELOPMENT_FOCUS = "development_focus"
    WORKFLOW_STATE = "workflow_state"
    TECHNICAL_CONTEXT = "technical_context"
    TEMPORAL_CONTEXT = "temporal_context"


class ConfidenceLevel(Enum):
    """Confidence levels for context inference."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


@dataclass
class ContextSignal:
    """Individual signal contributing to context inference."""
    source: str  # 'file_change', 'git_commit', 'terminal_command'
    signal_type: str
    content: str
    timestamp: datetime
    weight: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class InferredContext:
    """Represents inferred development context."""
    context_id: str
    context_type: ContextType
    confidence: ConfidenceLevel
    title: str
    description: str
    evidence_signals: List[ContextSignal]
    inferred_at: datetime
    expires_at: Optional[datetime]
    related_files: List[Path]
    related_concepts: List[str]
    predictions: List[Dict[str, Any]]
    recommendations: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass 
class DevelopmentSession:
    """Represents a development session context."""
    session_id: str
    start_time: datetime
    end_time: Optional[datetime]
    primary_focus: str
    files_worked_on: Set[Path]
    commands_executed: List[str]
    commits_made: List[Dict[str, Any]]
    context_switches: int
    productivity_score: float


class ContextInference:
    """
    Context inference engine for CORTEX Track B
    
    Analyzes multiple streams of development activity to infer
    current context, predict next actions, and provide recommendations.
    """
    
    def __init__(self, workspace_path: Path):
        self.workspace_path = workspace_path
        self.logger = logging.getLogger("cortex.track_b.context_inference")
        
        # Context storage
        self.active_contexts: Dict[str, InferredContext] = {}
        self.context_history: List[InferredContext] = []
        
        # Signal processing
        self.signal_queue: deque = deque(maxlen=1000)
        self.signal_processors = self._initialize_signal_processors()
        
        # Session tracking
        self.current_session: Optional[DevelopmentSession] = None
        self.session_history: List[DevelopmentSession] = []
        
        # Context patterns and learning
        self.context_patterns: Dict[str, Dict[str, Any]] = {}
        self.concept_graph: Dict[str, Set[str]] = defaultdict(set)
        
        # Configuration
        self.signal_weights = self._initialize_signal_weights()
        self.context_timeout = timedelta(minutes=30)
        self.session_timeout = timedelta(minutes=15)
    
    def _initialize_signal_processors(self) -> Dict[str, callable]:
        """Initialize signal processing functions."""
        return {
            'file_change': self._process_file_change_signal,
            'git_operation': self._process_git_signal,
            'terminal_command': self._process_terminal_signal,
        }
    
    def _initialize_signal_weights(self) -> Dict[str, float]:
        """Initialize weights for different signal types."""
        return {
            'file_change': 1.0,
            'git_commit': 2.0,
            'terminal_build': 1.5,
            'terminal_test': 1.5,
            'terminal_debug': 2.0,
            'file_create': 1.2,
            'file_delete': 1.2,
            'import_change': 0.8,
            'function_add': 1.3,
            'function_modify': 1.1,
            'class_add': 1.4,
        }
    
    def add_signal(self, signal: ContextSignal):
        """Add a new context signal for processing."""
        try:
            self.signal_queue.append(signal)
            
            # Process signal immediately
            self._process_signal(signal)
            
            # Update session tracking
            self._update_current_session(signal)
            
            # Trigger context inference
            self._infer_current_context()
            
        except Exception as e:
            self.logger.error(f"Error adding context signal: {e}")
    
    def _process_signal(self, signal: ContextSignal):
        """Process a context signal and extract meaning."""
        try:
            processor = self.signal_processors.get(signal.source)
            if processor:
                processor(signal)
            else:
                self.logger.warning(f"No processor for signal source: {signal.source}")
        except Exception as e:
            self.logger.error(f"Error processing signal: {e}")
    
    def _process_file_change_signal(self, signal: ContextSignal):
        """Process file change signals."""
        try:
            # Extract file information
            file_path = signal.metadata.get('file_path')
            change_type = signal.metadata.get('change_type', 'modified')
            
            if not file_path:
                return
            
            file_path = Path(file_path)
            
            # Update concept graph
            self._update_concept_graph_from_file(file_path, signal)
            
            # Determine signal importance
            if change_type in ['created', 'deleted']:
                signal.weight *= 1.2
            
            # Check for architectural significance
            if self._is_architecturally_significant_file(file_path):
                signal.weight *= 1.5
                
            # Update signal metadata
            signal.metadata.update({
                'file_extension': file_path.suffix,
                'file_name': file_path.name,
                'relative_path': str(file_path.relative_to(self.workspace_path))
            })
            
        except Exception as e:
            self.logger.error(f"Error processing file change signal: {e}")
    
    def _process_git_signal(self, signal: ContextSignal):
        """Process git operation signals."""
        try:
            operation = signal.metadata.get('event_type', 'unknown')
            
            if operation == 'commit':
                # Analyze commit message for intent
                message = signal.metadata.get('message', '')
                intent = self._analyze_commit_intent(message)
                signal.metadata['inferred_intent'] = intent
                
                # Weight based on commit size
                files_changed = len(signal.metadata.get('files_changed', []))
                if files_changed > 10:
                    signal.weight *= 1.3  # Large commits are significant
                elif files_changed == 1:
                    signal.weight *= 0.9  # Small commits are less significant
            
            elif operation in ['merge', 'rebase']:
                signal.weight *= 2.0  # Integration operations are very significant
                
        except Exception as e:
            self.logger.error(f"Error processing git signal: {e}")
    
    def _process_terminal_signal(self, signal: ContextSignal):
        """Process terminal command signals."""
        try:
            command = signal.content
            command_type = signal.metadata.get('command_type', 'other')
            
            # Analyze command intent
            intent = self._analyze_command_intent(command, command_type)
            signal.metadata['inferred_intent'] = intent
            
            # Weight based on command type
            type_weights = {
                'build': 1.5,
                'test': 1.5,
                'git': 1.2,
                'file': 0.8,
                'package': 1.3,
                'other': 1.0
            }
            
            signal.weight *= type_weights.get(command_type, 1.0)
            
            # Check for error conditions
            exit_code = signal.metadata.get('exit_code')
            if exit_code and exit_code != 0:
                signal.weight *= 1.4  # Failed commands are more significant
                signal.metadata['is_error'] = True
                
        except Exception as e:
            self.logger.error(f"Error processing terminal signal: {e}")
    
    def _update_concept_graph_from_file(self, file_path: Path, signal: ContextSignal):
        """Update concept graph based on file changes."""
        try:
            # Extract concepts from file path
            concepts = set()
            
            # File name concepts
            file_stem = file_path.stem.lower()
            concepts.add(file_stem)
            
            # Directory concepts
            for part in file_path.parts[:-1]:  # Exclude filename
                if part not in ['.', '..', 'src', 'lib', 'tests']:
                    concepts.add(part.lower())
            
            # Programming language concept
            if file_path.suffix:
                concepts.add(f"language:{file_path.suffix[1:]}")
            
            # Connect concepts in the graph
            for concept in concepts:
                for other_concept in concepts:
                    if concept != other_concept:
                        self.concept_graph[concept].add(other_concept)
                        
        except Exception as e:
            self.logger.error(f"Error updating concept graph: {e}")
    
    def _analyze_commit_intent(self, commit_message: str) -> str:
        """Analyze commit message to infer intent."""
        message_lower = commit_message.lower()
        
        intent_patterns = {
            'feature': ['add', 'implement', 'create', 'new', 'feature'],
            'fix': ['fix', 'bug', 'issue', 'resolve', 'correct'],
            'refactor': ['refactor', 'restructure', 'reorganize', 'clean'],
            'update': ['update', 'upgrade', 'modify', 'change'],
            'documentation': ['doc', 'readme', 'comment', 'documentation'],
            'test': ['test', 'testing', 'spec', 'coverage'],
            'performance': ['performance', 'optimize', 'speed', 'efficient'],
            'security': ['security', 'auth', 'permission', 'encrypt']
        }
        
        for intent, keywords in intent_patterns.items():
            if any(keyword in message_lower for keyword in keywords):
                return intent
        
        return 'maintenance'
    
    def _analyze_command_intent(self, command: str, command_type: str) -> str:
        """Analyze command to infer developer intent."""
        command_lower = command.lower()
        
        if command_type == 'test':
            if 'debug' in command_lower or 'verbose' in command_lower:
                return 'debugging'
            elif 'coverage' in command_lower:
                return 'quality_assurance'
            else:
                return 'validation'
        elif command_type == 'build':
            if 'clean' in command_lower:
                return 'maintenance'
            elif 'release' in command_lower or 'prod' in command_lower:
                return 'deployment'
            else:
                return 'development'
        elif command_type == 'git':
            return 'version_control'
        elif command_type == 'package':
            return 'dependency_management'
        
        return 'exploration'
    
    def _is_architecturally_significant_file(self, file_path: Path) -> bool:
        """Check if file is architecturally significant."""
        significant_files = {
            'package.json', 'requirements.txt', 'Cargo.toml', 'pom.xml',
            'Dockerfile', 'docker-compose.yml', 'Makefile', 'CMakeLists.txt',
            'main.py', 'index.js', 'app.py', 'server.py'
        }
        
        significant_dirs = {'src', 'lib', 'core', 'api', 'services'}
        
        if file_path.name in significant_files:
            return True
        
        if any(part in significant_dirs for part in file_path.parts):
            return True
        
        return False
    
    def _update_current_session(self, signal: ContextSignal):
        """Update current development session."""
        try:
            current_time = signal.timestamp
            
            # Check if we need to start a new session
            if (self.current_session is None or 
                current_time - self.current_session.start_time > self.session_timeout):
                
                # End previous session if it exists
                if self.current_session:
                    self.current_session.end_time = current_time
                    self.session_history.append(self.current_session)
                
                # Start new session
                self.current_session = DevelopmentSession(
                    session_id=f"session_{current_time.strftime('%Y%m%d_%H%M%S')}",
                    start_time=current_time,
                    end_time=None,
                    primary_focus="unknown",
                    files_worked_on=set(),
                    commands_executed=[],
                    commits_made=[],
                    context_switches=0,
                    productivity_score=0.0
                )
            
            # Update current session
            if signal.source == 'file_change':
                file_path = signal.metadata.get('file_path')
                if file_path:
                    self.current_session.files_worked_on.add(Path(file_path))
            
            elif signal.source == 'terminal_command':
                self.current_session.commands_executed.append(signal.content)
            
            elif signal.source == 'git_operation' and signal.metadata.get('event_type') == 'commit':
                self.current_session.commits_made.append(signal.metadata)
            
            # Update primary focus based on activity
            self._update_session_focus()
            
        except Exception as e:
            self.logger.error(f"Error updating current session: {e}")
    
    def _update_session_focus(self):
        """Update the primary focus of current session."""
        if not self.current_session:
            return
        
        try:
            # Analyze recent activity to determine focus
            recent_signals = list(self.signal_queue)[-20:]  # Last 20 signals
            
            # Count activity types
            activity_counts = defaultdict(int)
            
            for signal in recent_signals:
                if signal.source == 'file_change':
                    file_path = signal.metadata.get('file_path', '')
                    if 'test' in file_path.lower():
                        activity_counts['testing'] += 1
                    elif file_path.endswith(('.py', '.js', '.java', '.cpp')):
                        activity_counts['coding'] += 1
                    elif file_path.endswith(('.md', '.txt', '.rst')):
                        activity_counts['documentation'] += 1
                
                elif signal.source == 'terminal_command':
                    command_type = signal.metadata.get('command_type', 'other')
                    if command_type == 'test':
                        activity_counts['testing'] += 2
                    elif command_type == 'build':
                        activity_counts['building'] += 2
                    elif command_type == 'git':
                        activity_counts['version_control'] += 1
            
            # Determine primary focus
            if activity_counts:
                primary_focus = max(activity_counts.items(), key=lambda x: x[1])[0]
                self.current_session.primary_focus = primary_focus
                
        except Exception as e:
            self.logger.error(f"Error updating session focus: {e}")
    
    def _infer_current_context(self):
        """Infer current development context from recent signals."""
        try:
            if not self.signal_queue:
                return
            
            # Analyze recent signals (last 10)
            recent_signals = list(self.signal_queue)[-10:]
            
            # Infer different types of context
            contexts = []
            
            # Current task context
            task_context = self._infer_current_task(recent_signals)
            if task_context:
                contexts.append(task_context)
            
            # Development focus context
            focus_context = self._infer_development_focus(recent_signals)
            if focus_context:
                contexts.append(focus_context)
            
            # Workflow state context
            workflow_context = self._infer_workflow_state(recent_signals)
            if workflow_context:
                contexts.append(workflow_context)
            
            # Update active contexts
            for context in contexts:
                self.active_contexts[context.context_id] = context
            
            # Clean up expired contexts
            self._cleanup_expired_contexts()
            
        except Exception as e:
            self.logger.error(f"Error inferring current context: {e}")
    
    def _infer_current_task(self, signals: List[ContextSignal]) -> Optional[InferredContext]:
        """Infer current task from recent activity."""
        try:
            if not signals:
                return None
            
            # Analyze file patterns and changes
            file_signals = [s for s in signals if s.source == 'file_change']
            if not file_signals:
                return None
            
            # Group by file type and changes
            files_by_type = defaultdict(list)
            for signal in file_signals:
                file_path = signal.metadata.get('file_path', '')
                if file_path:
                    ext = Path(file_path).suffix
                    files_by_type[ext].append(signal)
            
            # Determine task type
            task_type = "development"
            confidence = ConfidenceLevel.MEDIUM
            
            if '.py' in files_by_type or '.js' in files_by_type:
                if any('test' in s.metadata.get('file_path', '') for s in file_signals):
                    task_type = "testing"
                else:
                    task_type = "coding"
            elif '.md' in files_by_type or '.txt' in files_by_type:
                task_type = "documentation"
            
            # Create context
            context_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # Get related files
            related_files = []
            for signal in file_signals:
                file_path = signal.metadata.get('file_path')
                if file_path:
                    related_files.append(Path(file_path))
            
            # Generate predictions
            predictions = self._generate_task_predictions(task_type, related_files)
            
            # Generate recommendations
            recommendations = self._generate_task_recommendations(task_type, signals)
            
            return InferredContext(
                context_id=context_id,
                context_type=ContextType.CURRENT_TASK,
                confidence=confidence,
                title=f"Current Task: {task_type.title()}",
                description=f"Working on {task_type} task involving {len(related_files)} files",
                evidence_signals=signals[-3:],  # Last 3 signals as evidence
                inferred_at=datetime.now(),
                expires_at=datetime.now() + self.context_timeout,
                related_files=related_files,
                related_concepts=[task_type],
                predictions=predictions,
                recommendations=recommendations,
                metadata={'task_type': task_type, 'file_count': len(related_files)}
            )
            
        except Exception as e:
            self.logger.error(f"Error inferring current task: {e}")
            return None
    
    def _infer_development_focus(self, signals: List[ContextSignal]) -> Optional[InferredContext]:
        """Infer development focus area."""
        try:
            # Analyze concepts from signals
            concepts = set()
            for signal in signals:
                file_path = signal.metadata.get('file_path', '')
                if file_path:
                    path_parts = Path(file_path).parts
                    concepts.update(part.lower() for part in path_parts if part not in ['.', '..'])
            
            if not concepts:
                return None
            
            # Find most common concepts
            concept_counts = Counter()
            for concept in concepts:
                if concept in self.concept_graph:
                    concept_counts[concept] += len(self.concept_graph[concept])
                else:
                    concept_counts[concept] += 1
            
            if not concept_counts:
                return None
            
            primary_concept = concept_counts.most_common(1)[0][0]
            
            context_id = f"focus_{primary_concept}"
            
            return InferredContext(
                context_id=context_id,
                context_type=ContextType.DEVELOPMENT_FOCUS,
                confidence=ConfidenceLevel.MEDIUM,
                title=f"Development Focus: {primary_concept.title()}",
                description=f"Current development activity centered around {primary_concept}",
                evidence_signals=signals,
                inferred_at=datetime.now(),
                expires_at=datetime.now() + self.context_timeout,
                related_files=[],
                related_concepts=list(concepts),
                predictions=[],
                recommendations=[],
                metadata={'primary_concept': primary_concept, 'concept_strength': concept_counts[primary_concept]}
            )
            
        except Exception as e:
            self.logger.error(f"Error inferring development focus: {e}")
            return None
    
    def _infer_workflow_state(self, signals: List[ContextSignal]) -> Optional[InferredContext]:
        """Infer current workflow state."""
        try:
            if not signals:
                return None
            
            # Analyze recent terminal commands and git operations
            terminal_signals = [s for s in signals if s.source == 'terminal_command']
            git_signals = [s for s in signals if s.source == 'git_operation']
            
            workflow_state = "development"
            confidence = ConfidenceLevel.LOW
            
            # Determine workflow state from commands
            if terminal_signals:
                last_command = terminal_signals[-1]
                command_type = last_command.metadata.get('command_type', 'other')
                
                if command_type == 'test':
                    workflow_state = "testing"
                    confidence = ConfidenceLevel.HIGH
                elif command_type == 'build':
                    workflow_state = "building"
                    confidence = ConfidenceLevel.HIGH
                
            # Check git operations
            if git_signals:
                last_git = git_signals[-1]
                operation = last_git.metadata.get('event_type', 'unknown')
                
                if operation == 'commit':
                    workflow_state = "committing"
                    confidence = ConfidenceLevel.HIGH
                elif operation in ['merge', 'rebase']:
                    workflow_state = "integrating"
                    confidence = ConfidenceLevel.HIGH
            
            context_id = f"workflow_{workflow_state}"
            
            return InferredContext(
                context_id=context_id,
                context_type=ContextType.WORKFLOW_STATE,
                confidence=confidence,
                title=f"Workflow State: {workflow_state.title()}",
                description=f"Currently in {workflow_state} phase of development workflow",
                evidence_signals=terminal_signals + git_signals,
                inferred_at=datetime.now(),
                expires_at=datetime.now() + timedelta(minutes=10),  # Workflow state expires quickly
                related_files=[],
                related_concepts=[workflow_state],
                predictions=[],
                recommendations=[],
                metadata={'workflow_state': workflow_state}
            )
            
        except Exception as e:
            self.logger.error(f"Error inferring workflow state: {e}")
            return None
    
    def _generate_task_predictions(self, task_type: str, related_files: List[Path]) -> List[Dict[str, Any]]:
        """Generate predictions for current task."""
        predictions = []
        
        try:
            if task_type == "coding":
                predictions.extend([
                    {
                        'type': 'next_action',
                        'description': 'Likely to run tests after code changes',
                        'probability': 0.7,
                        'suggested_action': 'Run test suite'
                    },
                    {
                        'type': 'workflow',
                        'description': 'May need to commit changes soon',
                        'probability': 0.6,
                        'suggested_action': 'Review changes for commit'
                    }
                ])
            
            elif task_type == "testing":
                predictions.extend([
                    {
                        'type': 'next_action',
                        'description': 'Likely to fix failing tests',
                        'probability': 0.8,
                        'suggested_action': 'Investigate test failures'
                    }
                ])
                
        except Exception as e:
            self.logger.error(f"Error generating task predictions: {e}")
        
        return predictions
    
    def _generate_task_recommendations(self, task_type: str, signals: List[ContextSignal]) -> List[str]:
        """Generate recommendations for current task."""
        recommendations = []
        
        try:
            if task_type == "coding":
                recommendations.extend([
                    "Consider writing tests for new functionality",
                    "Check code style and documentation",
                    "Run static analysis tools"
                ])
            
            elif task_type == "testing":
                recommendations.extend([
                    "Ensure test coverage is adequate",
                    "Consider edge cases and error conditions"
                ])
            
            elif task_type == "documentation":
                recommendations.extend([
                    "Update relevant code examples",
                    "Check for outdated information"
                ])
                
        except Exception as e:
            self.logger.error(f"Error generating task recommendations: {e}")
        
        return recommendations
    
    def _cleanup_expired_contexts(self):
        """Remove expired contexts."""
        try:
            current_time = datetime.now()
            
            expired_contexts = [
                context_id for context_id, context in self.active_contexts.items()
                if context.expires_at and context.expires_at < current_time
            ]
            
            for context_id in expired_contexts:
                context = self.active_contexts.pop(context_id)
                self.context_history.append(context)
            
            # Limit history size
            if len(self.context_history) > 100:
                self.context_history = self.context_history[-100:]
                
        except Exception as e:
            self.logger.error(f"Error cleaning up expired contexts: {e}")
    
    def get_current_context(self) -> Optional[InferredContext]:
        """Get the most relevant current context."""
        if not self.active_contexts:
            return None
        
        # Return context with highest confidence
        return max(
            self.active_contexts.values(),
            key=lambda ctx: (ctx.confidence.value, ctx.inferred_at)
        )
    
    def get_context_summary(self) -> Dict[str, Any]:
        """Get summary of current context state."""
        current_context = self.get_current_context()
        
        return {
            'current_context': {
                'title': current_context.title if current_context else None,
                'type': current_context.context_type.value if current_context else None,
                'confidence': current_context.confidence.value if current_context else None
            } if current_context else None,
            'active_contexts': len(self.active_contexts),
            'current_session': {
                'focus': self.current_session.primary_focus if self.current_session else None,
                'files_worked_on': len(self.current_session.files_worked_on) if self.current_session else 0,
                'duration_minutes': int((datetime.now() - self.current_session.start_time).total_seconds() / 60) if self.current_session else 0
            },
            'signals_processed': len(self.signal_queue),
            'concept_graph_size': len(self.concept_graph)
        }
    
    async def integrate_with_cortex_brain(self) -> Dict[str, Any]:
        """Enhanced Intelligence: Integrate Track B insights with CORTEX brain tiers."""
        integration_result = {
            'tier1_integration': 'not_implemented',
            'tier2_integration': 'not_implemented', 
            'tier3_integration': 'not_implemented',
            'insights_shared': 0,
            'patterns_learned': 0,
            'recommendations_generated': []
        }
        
        try:
            self.logger.info("Integrating Track B context with CORTEX brain...")
            
            # Tier 1: Working Memory Integration
            integration_result['tier1_integration'] = await self._integrate_with_tier1()
            
            # Tier 2: Knowledge Graph Integration  
            integration_result['tier2_integration'] = await self._integrate_with_tier2()
            
            # Tier 3: Context Intelligence Integration
            integration_result['tier3_integration'] = await self._integrate_with_tier3()
            
            # Generate cross-tier insights
            cross_tier_insights = self._generate_cross_tier_insights()
            integration_result['insights_shared'] = len(cross_tier_insights)
            
            # Learn from integration
            learned_patterns = self._learn_integration_patterns()
            integration_result['patterns_learned'] = len(learned_patterns)
            
            # Generate strategic recommendations
            integration_result['recommendations_generated'] = self._generate_integration_recommendations(
                cross_tier_insights, learned_patterns
            )
            
            self.logger.info(f"Brain integration complete: {integration_result['insights_shared']} insights shared")
            
        except Exception as e:
            self.logger.error(f"Error integrating with CORTEX brain: {e}")
            integration_result['integration_error'] = str(e)
        
        return integration_result
    
    async def _integrate_with_tier1(self) -> str:
        """Integrate context insights with Tier 1 (Working Memory)."""
        try:
            # Prepare conversation context data from Track B
            conversation_context = {
                'current_context': self.get_current_context(),
                'active_session': self.current_session,
                'recent_signals': list(self.signal_queue)[-10:] if self.signal_queue else [],
                'development_focus': self._extract_development_focus(),
                'predicted_next_actions': self._predict_immediate_actions()
            }
            
            # Format for Tier 1 storage
            tier1_data = {
                'source': 'track_b_context_inference',
                'timestamp': datetime.now().isoformat(),
                'context_type': 'development_session',
                'data': conversation_context,
                'entities': self._extract_entities_for_tier1(),
                'confidence': self._calculate_tier1_confidence()
            }
            
            # Note: Actual Tier 1 storage would require CORTEX brain module
            # For Phase 2, we're demonstrating the integration pattern
            self.logger.debug(f"Prepared Tier 1 integration data: {len(tier1_data['entities'])} entities")
            
            return 'ready_for_integration'
            
        except Exception as e:
            self.logger.error(f"Error integrating with Tier 1: {e}")
            return 'integration_error'
    
    async def _integrate_with_tier2(self) -> str:
        """Integrate learned patterns with Tier 2 (Knowledge Graph)."""
        try:
            # Extract patterns learned from Track B analysis
            learned_patterns = {
                'workflow_patterns': self._extract_workflow_patterns(),
                'file_relationship_patterns': self._extract_file_patterns(),
                'development_cycle_patterns': self._extract_cycle_patterns(),
                'productivity_patterns': self._extract_productivity_patterns()
            }
            
            # Format for Tier 2 knowledge graph
            tier2_patterns = []
            for pattern_type, patterns in learned_patterns.items():
                for pattern in patterns:
                    tier2_pattern = {
                        'pattern_id': f"track_b_{pattern_type}_{pattern['id']}",
                        'pattern_type': pattern_type,
                        'source': 'track_b_intelligent_context',
                        'confidence': pattern['confidence'],
                        'usage_count': pattern.get('usage_count', 1),
                        'context': {
                            'learned_from': 'execution_channel_analysis',
                            'workspace': str(self.workspace_path),
                            'pattern_data': pattern['data']
                        },
                        'relationships': pattern.get('relationships', [])
                    }
                    tier2_patterns.append(tier2_pattern)
            
            # Note: Actual Tier 2 storage would require CORTEX brain module
            self.logger.debug(f"Prepared Tier 2 integration: {len(tier2_patterns)} patterns")
            
            return 'ready_for_integration'
            
        except Exception as e:
            self.logger.error(f"Error integrating with Tier 2: {e}")
            return 'integration_error'
    
    async def _integrate_with_tier3(self) -> str:
        """Integrate development metrics with Tier 3 (Context Intelligence)."""
        try:
            # Prepare Track B metrics for Tier 3
            tier3_metrics = {
                'development_velocity': self._calculate_development_velocity(),
                'context_switching_frequency': self._calculate_context_switches(),
                'file_hotspots_detected': self._identify_file_hotspots_for_tier3(),
                'productivity_indicators': self._extract_productivity_indicators(),
                'session_patterns': self._extract_session_patterns(),
                'workflow_efficiency': self._calculate_workflow_efficiency_for_tier3()
            }
            
            # Format for Tier 3 storage
            tier3_data = {
                'source': 'track_b_execution_channel',
                'measurement_period': {
                    'start': self.current_session.start_time.isoformat() if self.current_session else None,
                    'end': datetime.now().isoformat()
                },
                'metrics': tier3_metrics,
                'insights': {
                    'performance_trends': self._analyze_performance_trends(),
                    'efficiency_recommendations': self._generate_efficiency_recommendations(),
                    'predictive_insights': self._generate_predictive_insights()
                }
            }
            
            # Note: Actual Tier 3 storage would require CORTEX brain module
            self.logger.debug(f"Prepared Tier 3 integration: {len(tier3_metrics)} metric categories")
            
            return 'ready_for_integration'
            
        except Exception as e:
            self.logger.error(f"Error integrating with Tier 3: {e}")
            return 'integration_error'
    
    def _extract_development_focus(self) -> Dict[str, Any]:
        """Extract current development focus from context analysis."""
        focus = {
            'primary_language': 'unknown',
            'primary_framework': 'unknown', 
            'current_task_type': 'unknown',
            'focus_areas': []
        }
        
        if self.current_session:
            # Analyze files worked on
            file_extensions = {}
            for file_path in self.current_session.files_worked_on:
                ext = file_path.suffix.lower()
                file_extensions[ext] = file_extensions.get(ext, 0) + 1
            
            # Determine primary language
            if file_extensions:
                primary_ext = max(file_extensions.items(), key=lambda x: x[1])[0]
                language_map = {
                    '.py': 'python', '.js': 'javascript', '.ts': 'typescript',
                    '.java': 'java', '.cpp': 'cpp', '.rs': 'rust', '.go': 'go'
                }
                focus['primary_language'] = language_map.get(primary_ext, 'other')
            
            # Analyze primary focus
            focus['current_task_type'] = self.current_session.primary_focus
            focus['focus_areas'] = list(self.current_session.context_tags)
        
        return focus
    
    def _extract_entities_for_tier1(self) -> List[Dict[str, Any]]:
        """Extract entities for Tier 1 conversation context."""
        entities = []
        
        # Extract from current context
        current_context = self.get_current_context()
        if current_context:
            for file_path in current_context.related_files:
                entities.append({
                    'type': 'file',
                    'name': file_path.name,
                    'path': str(file_path),
                    'relevance': 'high'
                })
            
            for concept in current_context.related_concepts:
                entities.append({
                    'type': 'concept',
                    'name': concept,
                    'relevance': 'medium'
                })
        
        # Extract from active session
        if self.current_session:
            for file_path in self.current_session.files_worked_on:
                entities.append({
                    'type': 'file',
                    'name': file_path.name,
                    'path': str(file_path),
                    'relevance': 'session_active'
                })
        
        return entities
    
    def _extract_workflow_patterns(self) -> List[Dict[str, Any]]:
        """Extract workflow patterns for Tier 2 knowledge graph."""
        patterns = []
        
        # Analyze signal sequences for workflow patterns
        if len(self.signal_queue) > 5:
            signal_sequence = list(self.signal_queue)
            
            # Look for common sequences (e.g., edit -> test -> commit)
            for i in range(len(signal_sequence) - 2):
                sequence = signal_sequence[i:i+3]
                pattern = {
                    'id': f"workflow_seq_{i}",
                    'data': {
                        'sequence': [s.signal_type for s in sequence],
                        'files_involved': [s.metadata.get('file_path', '') for s in sequence],
                        'timestamp_pattern': [(s.timestamp.hour, s.timestamp.minute) for s in sequence]
                    },
                    'confidence': 0.7,
                    'usage_count': 1,
                    'relationships': []
                }
                patterns.append(pattern)
        
        return patterns[:10]  # Limit to top 10 patterns
    
    def _extract_file_patterns(self) -> List[Dict[str, Any]]:
        """Extract file relationship patterns for Tier 2."""
        patterns = []
        
        # Analyze co-occurrence of files in contexts
        file_co_occurrence = defaultdict(lambda: defaultdict(int))
        
        for context in self.context_history[-20:]:  # Last 20 contexts
            files = context.related_files
            for i, file1 in enumerate(files):
                for file2 in files[i+1:]:
                    file_co_occurrence[str(file1)][str(file2)] += 1
        
        # Convert to patterns
        for file1, related_files in file_co_occurrence.items():
            if related_files:
                most_related = max(related_files.items(), key=lambda x: x[1])
                pattern = {
                    'id': f"file_rel_{hash(file1)}",
                    'data': {
                        'primary_file': file1,
                        'related_file': most_related[0], 
                        'co_occurrence_count': most_related[1]
                    },
                    'confidence': min(most_related[1] / 10.0, 1.0),
                    'usage_count': most_related[1],
                    'relationships': ['file_dependency']
                }
                patterns.append(pattern)
        
        return patterns[:5]  # Top 5 file patterns
    
    def _generate_cross_tier_insights(self) -> List[Dict[str, Any]]:
        """Generate insights that span multiple CORTEX brain tiers."""
        insights = []
        
        # Insight: Development pattern correlation
        insights.append({
            'type': 'pattern_correlation',
            'title': 'Development Workflow Optimization',
            'description': 'Track B identifies optimal workflow sequences for current project',
            'tier1_component': 'Recent conversation patterns about workflow efficiency',
            'tier2_component': 'Learned workflow templates from past successful sequences',
            'tier3_component': 'Git commit velocity and file change patterns',
            'confidence': 0.8,
            'actionable': True
        })
        
        # Insight: Context switching impact
        insights.append({
            'type': 'productivity_impact',
            'title': 'Context Switching Analysis',
            'description': 'Correlation between context changes and productivity metrics',
            'tier1_component': 'Conversation topic transitions',
            'tier2_component': 'Learned patterns about productive vs disruptive context switches',
            'tier3_component': 'File hotspot analysis and session duration metrics',
            'confidence': 0.7,
            'actionable': True
        })
        
        return insights
    
    def _learn_integration_patterns(self) -> List[Dict[str, Any]]:
        """Learn patterns from Track B and CORTEX brain integration."""
        learned_patterns = []
        
        # Pattern: Optimal integration timing
        learned_patterns.append({
            'pattern_type': 'integration_timing',
            'description': 'Best times to sync Track B insights with brain tiers',
            'data': {
                'optimal_frequency': 'every_15_minutes',
                'trigger_conditions': ['context_change', 'session_end', 'significant_signal'],
                'integration_cost': 'low',
                'value_gained': 'high'
            },
            'confidence': 0.8
        })
        
        return learned_patterns
    
    def _generate_integration_recommendations(self, insights: List[Dict], patterns: List[Dict]) -> List[str]:
        """Generate recommendations based on brain integration analysis."""
        recommendations = []
        
        if insights:
            recommendations.append("Enable automatic Track B -> CORTEX brain sync for enhanced context awareness")
        
        if patterns:
            recommendations.append("Implement learned integration patterns to optimize sync timing")
        
        if self.current_session and len(self.current_session.files_worked_on) > 10:
            recommendations.append("Consider session break - high file activity detected")
        
        return recommendations